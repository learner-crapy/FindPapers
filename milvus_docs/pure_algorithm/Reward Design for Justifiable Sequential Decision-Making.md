## Published as a conference paper at ICLR 2024
**REWARD DESIGN FOR JUSTIFIABLE SEQUENTIAL**
`{asukovic, gradanovic}@mpi-sws.org`
**ABSTRACT**
DECISION -MAKING Aleksa Sukovic1, 2Goran Radanovic1 Max Planck Institute for Software Systems1Saarland University2 Equipping agents with the capacity to justify made decisions using supporting evidence represents a cornerstone of accountable decision-making. Furthermore, ensuring that justifications are in line with human expectations and societal norms is vital, especially in high-stakes situations such as healthcare. In this work, we propose the use of a debate-based reward model for reinforcement learning agents, where the outcome of a zero-sum debate game quantifies the justifiability of a de cision in a particular state. This reward model is then used to train a justifiable pol icy, whose decisions can be more easily corroborated with supporting evidence. In the debate game, two argumentative agents take turns providing supporting ev idence for two competing decisions. Given the proposed evidence, a proxy of a human judge evaluates which decision is better justified. We demonstrate the po tential of our approach in learning policies for prescribing and justifying treatment decisions of septic patients. We show that augmenting the reward with the feed back signal generated by the debate-based reward model yields policies highly favored by the judge when compared to the policy obtained solely from the envi ronment rewards, while hardly sacrificing any performance. Moreover, in terms of the overall performance and justifiability of trained policies, the debate-based feedback is comparable to the feedback obtained from an ideal judge proxy that evaluates decisions using the full information encoded in the state. This sug gests that the debate game outputs key information contained in states that is most relevant for evaluating decisions, which in turn substantiates the practicality of combining our approach with human-in-the-loop evaluations. Lastly, we show case that agents trained via multi-agent debate learn to propose evidence that is resilient to refutations and closely aligns with human preferences.
* * *
## _1 I NTRODUCTION_
Reinforcement learning (RL) has been achieving impressive successes in a wide range of complex
* * *
domains. However, specifying a reward function which incentivizes RL agents to exhibit a desired behavior remains difficult (Leike et al., 2018). Prior work proposes several approaches that address these difficulties (Kwon et al., 2023; Bahdanau et al., 2018; Jothimurugan et al., 2019), including those based on learning from pairwise preferences (Christiano et al., 2017). However, such reward models are not informative enough for agents to learn how to substantiate their decisions with sup porting evidence – a key property needed for accountable decision-making (Bovens, 2007). Hence, we ask the following question: How can we design rewards that incentivize agents to carry out a given task, but through decisions that can be justified? To answer this questions, we consider a setting in which an RL agent acts as a principal, influencing the state of the world, while a human agent acts as a verifier responsible for validating the justifia bility of the decisions taken by the RL agent, based on the evidence provided. This scenario mirrors common real-world situations where accountability is critical, including healthcare scenarios where doctors are tasked with scrutinizing the validity of automated decisions. We recognize three impor tant properties that the provided evidence should satisfy. First, it should reflect the underlying state of the world, i.e., include the key information based on which an action was taken. A naive solution is to provide the full state as evidence. However, this discards the fact that the human, as a suboptimal decision maker, may not easily reason about the taken decision because the state might be too large 1arXiv:2402.15826v1 [cs.LG] 24 Feb 2024
* * *
## Published as a conference paper at ICLR 2024
## _Figure 1: To obtain a debate reward rd_
tin the state st, two argumentative agents A1andA2take
* * *
turns proposing supporting evidence (depicted as triangles) for two decisions, up to a predefined limit (here, 3evidence each). Then, a positive debate reward is issued whenever a proxy of a judge Jconsiders action at, taken by the justifiable policy πJ, better justified than action aB ttaken by the baseline policy πB. This reward is then mixed with the environment reward re tvia debate coefficient λ, yielding the final reward rtused to train the justifiable agent. or otherwise incomprehensible. This brings us to the second property: the provided evidence should also be concise and only reflect the most relevant information. The third property builds on the second one: given that the provided evidence contains only partial information about the state, this information should not be easy to refute. More specifically, additional information about the un derlying state, e.g., additional evidence, should not change the human’s judgment. Therefore, the overall challenge is to design a framework which can enable such justifications through a reward model that incentivizes both performance and justifiability. To tackle this challenge, we consider a framework which modifies the environment rewards by mix ing them with rewards coming from a debate-based reward model (see Figure 1). Each debate-based reward is defined through the outcome of a two-player zero-sum debate game. More specifically, we let two argumentative agents debate by taking turns providing supporting evidence contingent on the current state, each corroborating a decision made by one of two competing policies. Based on the proposed set of evidence, the human then states their preference over these two decisions, and this preference defines the debate reward. In this setup, one decision comes from a baseline policy, while the other comes from a justifiable policy that we optimize. Our approach builds upon the work of Irving et al. (2018), but we consider sequential decision-making problems and utilize debate to quantify a decision’s justifiability. To this end, we recognize two main technical challenges. First, learning a proxy of a human judge that evaluates decisions solely based on the proposed evidence, with comparable performance to methods requiring full state exposure. Second, learning a represen tation of argumentative strategies that are solutions to different instances of the debate game. These two components are needed to enable efficient learning of the justifiable policy. Contributions. Our contributions are as follows. (i) We formalize the problem of justifiable decision-making, modeling debate as an extensive-form game (Sec. 3). (ii) We provide a method for learning a proxy of a human judge that evaluates a decision’s justifiability using proposed evidence (Sec. 4.2). (iii) We propose an approach to learning contextualized argumentative strategies that constitute approximate solutions of the debate games (Sec. 4.3). (iv) We conduct extensive empir ical evaluation of our approach on a real-world problem of treating sepsis, testing the performance and justifiability of policies trained through our framework (Sec. 5.2), as well as the effectiveness and robustness of argumentative agents (Sec. 5.3, Sec. 5.4, and Sec. 5.5)1.
* * *
## _2 R ELATED WORK_
Debate. Irving et al. (2018) first outlined theoretical and practical implications of debate as a method
* * *
for training aligned agents. Debate has also been used to improve factuality of large language models (Du et al., 2023), reason in knowledge graphs (Hildebrandt et al., 2020), and aid in Q&A; tasks (Perez et al., 2019). We build on this line of work by bringing the utility of debate in sequential decision making problems, where it is used as an approach of quantifying justifiability of made decisions. In addition, by leveraging learning from pairwise preferences as in Christiano et al. (2017), our 1Our code is publicly available at github.com/aleksa-sukovic/iclr2024-reward-design-for-justifiable-rl. 2
* * *
## Published as a conference paper at ICLR 2024
framework enables encoding of human judgments in a form of the preferred evidence that renders a decision justified. Reward Design. Previous work proposes several approaches that address difficulties of reward design, based on natural language (Kwon et al., 2023; Bahdanau et al., 2018), rule-based methods (Jothimurugan et al., 2019) and preferences (Biyik & Sadigh, 2018; Christiano et al., 2017). Further more, there is a line of work considering interpretable reward design, including reward sparsification (Devidze et al., 2021) and reward decomposition (Juozapaitis et al., 2019; Bica et al., 2020). Dif ferently, we define a reward model as an outcome of a zero-sum debate game, which in itself is interpretable, and learn contextualized policies that solve it. Most similar to our methodology is learning from pairwise preferences as in Christiano et al. (2017), only we assume comparisons are made over justifying evidence, without exposure to the underlying state or trajectory. Explainable AI and Other Related Work. Our approach is most similar to the attribution-based techniques for explaining the inner workings of models. In such approaches, contributions of input features are quantified numerically (Lundberg & Lee, 2017; Ribeiro et al., 2016) or visually repre sented as heatmaps (Selvaraju et al., 2017; Mundhenk et al., 2019). This line of research has received a significant attention, also in the context of explaining decisions of RL agents (Ragodos et al., 2022; Bastani et al., 2018). However, one limitation of these explanations is the inability to further align them with human preferences (Hadfield, 2021; Brundage et al., 2020). In contrast, our approach additionally enables human-specified justifications, facilitating the incorporation of preferences into their generation. There is also a line of work that examines approaches for adaptation of agent’s recommendations (actions) to a baseline human policy in an expert-in-loop setup (Grand-Clément & Pauphilet, 2022; Faros et al., 2023). Differently, we consider a problem of learning to take actions that can be justified, where the agent itself acts as a primary decision-maker in the environment.
* * *
## _3 F ORMAL SETUP_
We consider a sequential decision-making problem, where an agent interacts with an environment
* * *
over a series of time-steps, modeled by a discrete-time Markov Decision Processes (MDP). The episode begins by sampling a starting state from the initial state distribution ρ. At each time-step t, an agent observes the current state st∈ S, takes an action at∈ A and receives an environment reward re(st, at). The environment then transitions to a successor state st+1with a probability specified by the transition function T(st+1|st, at). 3.1 A GENTS We consider two kinds of agents that operate in the defined MDP: baseline andjustifiable agent. Baseline Agent. Thebaseline agent aims to maximize the expected discounted value of the envi ronment’s return given as R=PT−1 t=0γtre(st, at), where γ∈[0,1]is a user-specified discount factor. Its optimal action-value function, defined as Q∗(s, a) = max πE[R|st=s, at=a, π], is the maximum expected discounted return obtained after taking action ain state s. We will refer to a deterministic policy that maximizes the expected value of Ras the baseline policy πB, which satisfies πB(s)∈argmaxaQ∗(s, a).2 Justifiable Agent. While the baseline agent learns to solve the environment, its decisions may not be easy to justify when evaluated by a human. To design an agent that accounts for the justifiability of its decisions, we consider a reward defined as a weighted combination of the environment reward reand the debate reward rd, which encapsulates human judgment of justifiability and is specified in the next subsection. The expected return is then defined as: RJ=XT−1 t=0γt (1−λ)·re(st, at) +λ·rd(st, at, aB t) where λ∈[0,1]is adebate coefficient, balancing between environment and debate rewards, and aB tis the action of the baseline agent in state st. The value of λ= 0.0corresponds to the return of the baseline policy, whereas for the value λ= 1.0we obtain a setup similar to Christiano et al. (2017), where the agent relies only on the debate reward model. We refer to an agent that maxi mizes the expected value of RJas the justifiable agent, denote its optimal action-value function as 2In practice, we require the baseline policy to be well-performing, but not necessarily optimal. 3
* * *
## Published as a conference paper at ICLR 2024
Q∗ J(s, a) = max πE[RJ|st=s, at=a, π]and its deterministic policy πJ(s)∈argmaxaQ∗ J(s, a) as the justifiable policy. 3.2 R EWARD MODELING VIA DEBATE Our objective is to learn a reward model, denoted as rd(st, at, aB t), which quantifies the justifiability of a decision. To this end, we introduce a reward model based on a debate game. In this model, rd(st, at, aB t)represents the value of a debate game induced by a tuple (st, at, aB t). In more details, for a given tuple (st, at, aB t), the induced debate game is formulated as a two-player zero-sum extensive-form game (Shoham & Leyton-Brown, 2008), in which the first player argues for taking the decision atin state st, while the second player argues for taking the baseline decision aB tinst. Debate Game. WithNwe denote a set of nodes in a finite, rooted game tree. The action space is represented by a finite set of evidence E, and each node n∈ N consists of evidence proposed thus far in the game n={e} ⊆ E. Additionally, the edges to successors of each node define actions (evidence) {e:e∈ E \ n}available to the acting player, where we disallow evidence repetition. The debate game is a perfect-information game: at all times, the players have no ambiguity about the evidence proposed up until the current point and have a complete knowledge about the state of the game. The game proceeds as players take turns: in turn l, player i=lmod 2+1 proposes evidence ei l/2.3The total number of turns Lis assumed to be even and significantly smaller than the evidence set, i.e., L≪ |E|. After the last turn, a terminal node nL= (e1 1, e2 1,..., e1 L/2, e2 L/2) ={enL}is evaluated. The players’ utilities are u1(nL) =−u2(nL) =U(at, aB t,{enL}), withUdefined as: U(at, aB t,{e}) =  +1,J(at,{e})>J(aB t,{e}) 0, J(at,{e}) =J(aB t,{e}) −1, otherwise Here,Jis a model of a human judge that, for a given decision aand evidence {e}, outputs a numerical value J(a,{e})∈Rquantifying how justifiable ais under evidence {e}. Strategies and Solution Concept. A player’s strategy σi:N → E outputs available evidence σi(n)∈ E \ nin a given node nandΣiis the set of all strategies of the player i. Based on the utility function, we additionally define G({σ1, σ2}, st, at, aB t)as the payoff (utility) of the first player, conditioned on both players following the strategy profile {σ1, σ2}. Then, a set of the best responses of the first (resp. second) player to its opponent strategy σ2(resp. σ1) is defined as b1(σ2) = arg max σ1∈Σ1G({σ1, σ2}, st, at, aB t)(resp. b2(σ1) = arg min σ2∈Σ2G({σ1, σ2}, st, at, aB t)). A strategy profile ¯σ={¯σ1,¯σ2}is a pure-strategy Nash equilibrium if ¯σi∈bi(¯σ−i). Because the debate game is a perfect-information extensive-form game, a pure-strategy Nash equilibrium exists (Shoham & Leyton-Brown, 2008), and due to its zero-sum structure, it can be obtained by solving the following max-min optimization problem: max σ1∈Σ1minσ2∈Σ2G({σ1, σ2}, st, at, aB t). We refer to G({¯σ1,¯σ2}, st, at, aB t)as the value of the game4and define rd(st, at, aB t)to be equal to it, i.e.,rd(st, at, aB t) =α· G({¯σ1,¯σ2}, st, at, aB t), where α >0is a scaling coefficient.
* * *
## _4 L EARNING FRAMEWORK_
To effectively use a debate game outcome during training of justifiable policies, it is necessary to
* * *
devise a model of a human judge J(Sec. 4.2) that encapsulates justifiability judgment and addi tionally learn argumentative policies that approximate a Nash equilibrium and are able to generalize across different instances of the debate game. With ˆrd(st, at, aB t)we denote an approximation of rd(st, at, aB t), obtained by running the argumentative policies from Sec. 4.3in the debate game induced by (st, at, aB t). In all our experiments, we set α= 5. 3In our implementation of the debate game, we randomly chose which player has the first turn, i.e., i= (l+τ) mod 2 + 1, where τ∼ U({0,1}). This only affects the order of the evidence in nL. 4For a two-player zero-sum game, the value of the game (or payoff) is unique (von Neumann & Morgen stern, 1947). 4
* * *
## Published as a conference paper at ICLR 2024
4.1 P REFERENCE DATASET We assume the human judgments are collected in a preference dataset Dof tuples (st, a0, a1, p), where stis a state, a0̸=a1are two decisions and p∈ {0,1}indicates which of the two decision is more justified in a particular state. The value of p= 0 (resp. p= 1) indicates that a0(resp. a1) is more preferred. 4.2 J UDGE MODEL Because asking for human feedback is expensive, we aim to learn a judge model from the dataset D of preferences that can be used to evaluate the outcome of debate games. The judge, parametrized with learnable parameter θ∈Rd1, is defined as a scalar reward function Jθ(a,{e})∈Rquantifying how justifiable a decision ais, given evidence {e}. Similar to Christiano et al. (2017), we addition ally assume that justifiability preference for decision a0over decision a1follows the Bradley-Terry model (Bradley & Terry, 1952): P(a0≻a1,{e}) =expJθ(a0,{e}) expJθ(a0,{e}) + exp Jθ(a1,{e}). Here, we require the judge to quantify the level of justifiability given all evidence at once. Note the lack of dependency on the state st: the judge evaluates only proposed evidence, whereas the argumentative agents are in charge of providing those evidence, contingent on the state. We optimize the parameters by minimizing the cross-entropy loss between preference-predictions and labels from the dataset. See App. C.1 for more details. 4.3 A RGUMENTATIVE AGENT Our overarching goal is to learn a generalizable argumentative policy that is able to solve any de bate game, conditioned on its defining tuple (st, at, aB t). This is a difficult feat, as the evidence set available to the agent is contingent on the state st. To this end, we can treat the debate game as an instance of a contextualized extensive form game (Sessa et al., 2020), where we consider the tuple (st, at, aB t)as a context z∈ Z. In a general case, the justifiable agent πJobserves the state stand takes action atwhich, paired with an action aB tthe baseline agent πBwould have taken, sets the debate game context z={st, at, aB t}. In the specific case of offline reinforcement learning we consider here, the contexts are sampled i.i.d. from the static dataset throughout train ing of both, argumentative and justifiable agents. Therefore, given a sample from the preference dataset (st, a0, a1, p)∼ D, we set the context to z= (st, ap, a1−p). Player i’s contextual strategy, parametrized with learnable parameter ϕi∈Rd2, is defined as σc ϕi:Z → Σi, mapping a con text to the strategy of the player for the induced debate game5. To learn parameters ϕi, we solve: maxϕ1∈Rd2minϕ2∈Rd2Ez∼D[G({σc ϕ1(z), σc ϕ2(z)}, z)]. 4.4 M ETHOD Judge. The judge Jθis parametrized by weights θ∈Rd1of a neural network with two fully connected layers of size 256, using parametric relu (He et al., 2015) activation and batch normal ization (Ioffe & Szegedy, 2015). The network receives a vector xinR|E|, where only the values of evidence {e}are shown, while the rest are set to zero. In addition, a binary mask of the same dimen sion, wherein all elements corresponding to the evidence {e}are assigned a value of one, while the remaining elements are set to zero, as well as a one-hot encoded action aare passed. The learning is done for a total of 100epochs using batches of 64comparisons sampled from the preference dataset D, Adam optimizer and a learning rate of 5e-4. See App. C.1 for more details. Argumentative Agent. We represent parameters ϕ∈Rd2of the argumentative agent σc ϕ(·|z), as weights of a neural network composed of 2fully-connected layers of size 512with leaky-relu activation function (Maas et al., 2013) with slope of 1e-2. The network takes as input a 44-dim state, a one-hot encoded decision for which the agent argues, as well as a binary mask of currently proposed evidence. The evidence (action) space of the policy is discrete and has 44choices, each corresponding to exactly one state feature. To train the agent, we use PPO (Schulman et al., 2017) 5Equivalently, we will also denote the contextual strategy of player iasσc ϕi(·|z) :N → E forz∈ Z. 5
* * *
## Published as a conference paper at ICLR 2024
λ=0.0 λ=0.25 λ=0.50 λ=0.75 λ=1.0 clinicianλ=0.0 λ=0.25 λ=0.50 λ=0.75 λ=1.0 clinician 0100200300400500 Iterations0.300.400.500.600.700.800.901.00Performance0100200300400500 Iterations0.300.400.500.600.700.800.901.00Performance (a) WIS evaluation λ=0.25 λ=0.50 λ=0.75 λ=1.0 Policy Variant0.500.600.700.800.901.00Preference Over Baselineλ=0.25 λ=0.50 λ=0.75 λ=1.0 Policy Variant0.500.600.700.800.901.00Preference Over Baseline (b) Preferred to baseline −0.08 −0.04 0.00 0.04 0.08 Excess 1e40.000.100.200.300.400.50Mortality −0.08 −0.04 0.00 0.04 0.08 Excess 1e40.000.100.200.300.400.50Mortality (c) IV dose excess −0.50−0.25 0.000.250.50 Excess0.000.100.200.300.400.50Mortality −0.50−0.25 0.000.250.50 Excess0.000.100.200.300.400.50Mortality (d) VC dose excess
* * *
## _Figure 2: Evaluation of justifiable policies. For (b)-(d), the confidence intervals represent ±2stan-_
dard errors of the mean over 5random seeds. (a) Policy performance as measured by WIS evaluation
* * *
on a held-out test set with ±1terminal rewards for every patient discharge or death. The mean and standard deviation are reported over 5random seeds. (b) Percent of times judge preferred decisions of justifiable policies (i.e., λ > 0.0) compared to those of the baseline policy (i.e., λ= 0.0). (c) (d) Observed patient mortality (y-axis) against variations in IV/VC treatment doses prescribed by clinicians compared to the recommendations of learned policies (x-axis). and examine two optimization strategies, namely self-play andmaxmin. We train the self-play debate by letting the agent argue with a copy of itself6, updating that copy every 100k steps, and repeating the procedure for 500generations. For maxmin debate, we use 2different set of weights, ϕ1and ϕ2, to represent the agents’ policies. The first agent i.e., ϕ1is trained for 4k steps, followed by training of the second agent i.e., ϕ2for100k steps, repeating this procedure for 500generations. This approach of overfitting the second agent to the current version of the first ensured learning of defense strategies against a very strong adversary. See App. C.2 for more details. Justifiable Agent. To learn a justifiable policy πJ(s)∈arg maxaQ∗ J(s, a), we consider model-free reinforcement learning methods based on Q-learning. Our approach builds on top of Raghu et al. (2017) and is based on a variant of Deep-Q networks (Mnih et al., 2015), specifically double-deep Q-network with the dueling architecture (Wang et al., 2016; Van Hasselt et al., 2016). The final network consists of 2fully-connected layers of size 128using leaky-relu activation function with slope 1e-2. The learning is done in batches of 256 ( s, a, r, s′)tuples sampled from a Prioritized Experience Replay buffer (Schaul et al., 2015) using a learning rate of 1e-4, for a total of 25k iterations, evaluating the policy every 50iterations on a held-out test set. When incorporating signal from the debate, i.e., ˆrd, we augment the reward from the replay buffer as described in Sec. 3.1. Modifying just the observed reward, without changing the state dynamics, has been shown to be sufficient to induce learning of an alternative policy (Ma et al., 2019). See App. C.3 for more details, including additional loss terms and a list of all hyperparameters.
* * *
## _5 E XPERIMENTS_
In our experiments, we aim to empirically evaluate the properties of debate as a method for reward
* * *
specification. To this end, we perform quantitative and qualitative evaluation of justifiable policies (Sec. 5.2), analyze the effect of pairwise comparison only over proposed evidence on performance and justifiability (Sec. 5.3), examine the necessity of multi-agent debate in learning to provide evidence that is resilient to refutations (Sec. 5.4), and compare efficiency in providing supporting evidence for a decision of argumentative policies and SHAP feature-attribution method (Sec. 5.5). 5.1 E NVIRONMENTAL SETUP Sepsis. Data for our cohort were obtained following steps outlined in Komorowski et al. (2018), utilizing MIMIC-III v1.4 database (Johnson et al., 2016). We focus our analysis on patients that fulfill Sepsi-3 criteria (Singer et al., 2016), 18,585 in total. The patient vector consists of 40contin uous and 4discrete features, and the action space consists of 5×5discrete choices of intravenous 6In terms of learnable parameters, this implies ϕ2:=ϕ1. 6
* * *
## Published as a conference paper at ICLR 2024
(IV) fluids and vasopressors (VC). As in Raghu et al. (2017), the environment rewards are clinically guided. We set the environment reward reto±15for terminal states resulting in patient survival or death, respectively, and shape the intermediate rewards based on the SOFA score (measure of organ failure). See App. B for more details. Preference Dataset. To make a comprehensive evaluation possible, we define a synthetic ground truth preference by making an assumption that a human judge always prefers a treatment prescribed by the clinician. Therefore, clinician’s actions are the justified actions in our experiments. More formally, we bootstrap the dataset of preferences Dby matching every pair (st, at)from the cohort with an alternative action ar∼ U(A), ar̸=atsampled uniform-random from the action space, initializing the preference variable pto point to the true action at, as taken by the clinician (see App. D.5 for alternative dataset bootstrapping methods). The number of evidence is fixed to 6(∼13.6% of the full state) (see App. D.1 for results with L= 4 evidence). The dataset is split into chunks of 70%, 15%, 15% used for training, validation, and testing respectively. We report all our results on a held-out test set, not seen by any of the agents nor the judge. When training a judge model, we additionally augment a tuple (st, a0, a1, p)with an evidence set {e}. To generate it, we sample a predetermined number of state features uniform-random i.e., {e} ∼ U (st), from the state, which is inspired by Irving et al. (2018) and ensures evidence is contingent on the state (Sec. 3.2)7. On the test set, the judge is able to correctly recover the underlying preference from the dataset with a relatively low accuracy of 65%. Baselines. When comparing effectiveness of treatment policies, we consider two baselines. First, we consider the observed reward of the clinician from the dataset (depicted as a gray horizontal line in plots). Second, the baseline policy (Sec. 3.1) serves as an indicator of the optimal treatment policy. To demonstrate the robustness of multi-agent debate, we introduce an isolated argumentative agent. This agent aims to find an evidence set {e}that maximizes J(ap,{e}), for a given ap. To achieve this, we solve a search problem akin to the debate game by applying reinforcement learning (see App. C for more details). Lastly, we use SHAP when comparing effectiveness of debate to a feature-importance approach in providing supporting evidence for a decision. 5.2 E XPERIMENT 1: E FFECTIVENESS OF TASKS POLICIES To examine the potential of specifying the reward as an adversarial game and its effect on the quality of learned behaviors, we train several policies by varying the debate coefficient λ. Quantitative Evaluation. In Plot 2a, we evaluate the performance of different justifiable policies on a held-out test set during the course of training using weighted importance sampling (WIS) (Sutton & Barto, 2018)8. Likewise, in Plot 2b, we show the judge’s preference over decisions made by justifiable policies (i.e., λ > 0.0), compared to the baseline policy (i.e., λ= 0.0), when the two were different9. The observed inherent trade-off between performance and justifiability suggests that tuning the debate coefficient λis important in practice, and we further elaborate on this in Sec. 6. Qualitative Evaluation. In addition to quantitative evaluation, we perform qualitative analysis similar to Raghu et al. (2017). Plots 2c and 2d showcase correlation between observed mortality and difference between the optimal doses suggested by policies, and the actual doses prescribed by the clinicians. For all trained policies, the lowest mortality is observed when the difference is near zero, thus further showcasing their potential validity. It is also encouraging to see that the policy trained solely with debate rewards (i.e., λ= 1.0) remains quantitatively and qualitatively competitive in addition to being highly favored by the judge, even though it relies only on debate rewards. 5.3 E XPERIMENT 2: D EBATE -BASED FEEDBACK VS. STATE -BASED FEEDBACK Because the judge is evaluating justifiability using only proposed evidence, a natural question to ask is how much does this affect the performance and alignment of trained policies. To provide an answer, we train a new judge that evaluates decisions based on the full state, namely J′(at, st). We then use this judge instead of ˆrdto train a new justifiable policy using state-based feedback, one 7We discuss different approaches for defining a set of evidence in Sec. 6. 8The behavior policy used in WIS was derived via behavior cloning, further described in App. C.3. 9See Plot 6a in App. D for a detailed breakdown of actions proposed by justifiable policies. 7
* * *
## Published as a conference paper at ICLR 2024
0 100 200 300 400 500 Iterations0.300.400.500.600.700.800.901.00Performance debate-based feedback state-based feedback clinician0 100 200 300 400 500 Iterations0.300.400.500.600.700.800.901.00Performance debate-based feedback state-based feedback clinician (a)λ= 0.25 0 100 200 300 400 500 Iterations0.300.400.500.600.700.800.901.00Performance debate-based feedback state-based feedback clinician0 100 200 300 400 500 Iterations0.300.400.500.600.700.800.901.00Performance debate-based feedback state-based feedback clinician (b)λ= 0.50 0 100 200 300 400 500 Iterations0.300.400.500.600.700.800.901.00Performance debate-based feedback state-based feedback clinician0 100 200 300 400 500 Iterations0.300.400.500.600.700.800.901.00Performance debate-based feedback state-based feedback clinician (c)λ= 0.75 0 100 200 300 400 500 Iterations0.300.400.500.600.700.800.901.00Performance debate-based feedback state-based feedback clinician0 100 200 300 400 500 Iterations0.300.400.500.600.700.800.901.00Performance debate-based feedback state-based feedback clinician (d)λ= 1.0
* * *
## _Figure 3: Performance of policies trained with state-based feedback compared to debate-based feed-_
back, as measured by the weighted importance sampling evaluation on a held-out test set with ±1
* * *
terminal rewards for every patient discharge or death. The mean and standard deviation are reported over5random seeds. for each λ. Given a sample (st, a0, a1, p)∼ D, we consider a policy πaligned to the ground-truth preference if Q∗ π(s, ap)≥Q∗ π(s, a1−p). The results are shown in Plot 4a for various policies trained with state- and debate-based feedback. We see that, even though the debate-based approach uses only ∼13.6%of the full state, the achieved level of alignment remains similar. Likewise, in Plots 3a-3d we observe a matching trend when it comes to performance of the policies, although here we additionally note that the difference seems to increase in favor of policies trained with the state-based feedback as we increase the debate coefficient λ. These results seem promising, as they indicate one can expect to obtain a competitive level of alignment and performance, while requiring the judge to elicit preference over relatively small number of evidence. 5.4 E XPERIMENT 3: E FFECTIVENESS OF ARGUMENTATIVE POLICIES Preference Recovery Rate. We recall the judge’s accuracy in correctly predicting the preferred ac tionapfrom the preference dataset was 65%. To evaluate the effectiveness of argumentative policies, for each sample (st, a0, a1, p)∼ D we measure the judge’s accuracy in predicting the more justified action ap, when evidence {e}is provided by one of the argumentative policies. In Plot 4b (green) we show the judge’s accuracy when different argumentative agents propose L= 6required evidence for action ap, averaged across 1000 different debate games. The judge’s accuracy is boosted from 65% to a near 90%, demonstrating that agents can significantly amplify the capabilities of an otherwise limited judge. Robust Argumentation. A good supporting evidence is both convincing and not easily refutable by counterarguments. To test the robustness of the proposed evidence, we train 3adversarial confuser agents10, each targeting one of the three (frozen) argumentative agents. The goal of the confuser is to convince the judge of the opposing action a1−p. For L= 6, to obtain evidence {e}, the agent (maxmin or self-play) and its corresponding confuser take turns and propose 3evidence each. The isolated agent is trained to first propose L= 3 evidence, followed by the confuser proposing the remainder (see App. C.2.1 for more details and App. D.3 for additional results). Plot 4b (red) shows judge’s accuracy in this setting for 1000 different debate games. We observe that the isolated argumentative agent (Sec. 5.1) is not resilient to refutations, enabling the confuser to bring the judge’s accuracy down to 38%, effectively convincing it of the opposite of its preference. Differently, both maxmin and self-play agents managed to keep the judge’s accuracy to about 85%. 5.5 E XPERIMENT 4: C OMPARISON TO SHAP- BASED EXPLANATIONS A widely used approach to analyze black-box machine learning models is through feature-attribution techniques. We aim to demonstrate that these explanations may not necessarily be as effective when used as supporting evidence. We focus on the SHAP framework (Lundberg & Lee, 2017), specifi cally in providing justifications for decisions of various justifiable policies (Sec. 5.2). To justify a 10Confuser agents use the same architecture as argumentative agents, further described in App. C. 8
* * *
## Published as a conference paper at ICLR 2024
λ=0.25 λ=0.50 λ=0.75 λ=1.000.000.200.400.600.801.00Frac. Of Aligned Decisionsλ=0.25 λ=0.50 λ=0.75 λ=1.000.000.200.400.600.801.00Frac. Of Aligned Decisions (a) State/debate feedback isolated maxmin self-play0.000.200.400.600.801.00Judge Accuracyisolated maxmin self-play0.000.200.400.600.801.00Judge Accuracy (b) Preference recovery lablablablablablablablab0.650.700.750.800.850.900.951.00Judge Accuracylablablablablablablablab0.650.700.750.800.850.900.951.00Judge Accuracy (c) Comparison to SHAP state-based feedback debate-based feedback without confuser with confuser isolated debate (maxmin) debate (self-play)shap (λ=0.0) shap (λ=0.25) shap (λ=0.50) shap (λ=0.75) shap (λ=1.0)state-based feedback debate-based feedback without confuser with confuser isolated debate (maxmin) debate (self-play)shap (λ=0.0) shap (λ=0.25) shap (λ=0.50) shap (λ=0.75) shap (λ=1.0)
* * *
## _Figure 4: (a) Fraction of aligned decisions of policies trained with debate- and state-based feedback._
Confidence intervals (CI) represent ±2standard errors of the mean over 5random seeds. (b) Ac
* * *
curacy of the judge in predicting the preferred action, with and without the confuser agent, with CI representing ±2standard errors of the mean estimate. (c) Effectiveness of SHAP-based explana tions when used to justify a decision, as measured by the judge’s accuracy, with CI representing ±2 standard errors of the mean estimate. decision using SHAP, we select the top 6state features, as ranked by their Shapely values. For ar gumentative models, we either run the debate between a0anda1(for maxmin and self-play agents) or propose 6evidence in isolation (for isolated agent). In Plot 4c, we show the judge’s accuracy against different approaches of proposing the evidence {e}, across 1000 comparisons (st, a0, a1, p) sampled uniform-random from the test set. The SHAP-based evidence do improve the accuracy to about 70%, but nevertheless fall short compared to the argumentative agents.
* * *
## _6 D ISCUSSION_
In this work, we proposed use of a debate-based reward model as a general method of specifying
* * *
desired behavior and necessary evidence to justify it. In this section, we take a step back and touch upon a couple of aspects that are relevant to future research based on this work. Eliciting Preferences. The success of debate depends on the human’s capability of judging its out come, a process which may be affected by one’s beliefs and biases. Extra care must be taken when collecting large datasets of preferences, as belief bias11is known to alter the results of judgment in human evaluations (Anderson & Hartzler, 2014) and is amplified in time-limited situations (Evans & Curtis-Holmes, 2005), which human annotators frequently encounter. Furthermore, in our exper iments, we assumed existence of a single preference (clinician’s decision). However, preferences collected from multiple raters will undoubtedly yield a higher variability in this respect. Motivated by positive results seen in this work, future research could undertake further studies that examine the effectiveness of eliciting preferences over partial state visibility through use of the debate as an amplification method. Defining Arguments is Difficult. We have focused on debate assuming a well-defined argument space. While state features are one possible and easy choice, finding clear and expressive arguments presents a challenge, impacting the applicability of debate. In the context of RL, one potential alternative is considering previous trajectories in support of the current decision. In domains involv ing text generation, evidence could be defined as sentences, paragraphs, or references supporting a claim. A potentially interesting research direction is to examine the utility of a debate in a domain of human-ai collaboration, specifically in sequential decision-making tasks. Practical Considerations. We would like to emphasize that when deploying our framework in practice, it is important to account for the context in which the system is being employed. The performance-justifiability trade-off from our experiments suggests that a special care ought to be given to selecting hyperparameters, in particular, those that weight the importance of the environ ment rewards and debate-based rewards. In practice, this means that a reward designer has to assess the potentially differing objectives encoded in these values as well as the accuracy of the proxy judge model, prior to the training process. 11Abelief bias represents a tendency to judge the strength of arguments based on the plausibility of the conclusion, instead based on how strongly they support the conclusion. 9
* * *
## Published as a conference paper at ICLR 2024
ETHICS STATEMENT We acknowledge a potential misuse of the debate framework for malevolent purposes, such as decep tion. In the advent of AI systems that surpass human performance in many tasks, novel approaches must be developed which enable defense against malicious agents. While recent results indicate that debate is useful in thwarting malicious use of AI systems, further research is paramount in ensuring one can detect and defend against nefarious purposes. Moreover, the reliance on human judgments introduces the possibility of capturing their biases. Subsequently, designed reward function can in centivize argumentative agents to amplify those biases or learn to leverage them to achieve their objective. This is particularly important for practical considerations (Sec. 6), where a reward de signer is tasked with assessing and handling the performance-justifiability trade-off. It is important to note that our results do not hint at solutions for dealing with these challenges; they only demon strate that the trade-off exists. It is therefore of great importance that future research investigates novel algorithmic approaches and methods to tackle such challenges. ACKNOWLEDGMENTS This research was, in part, funded by the Deutsche Forschungsgemeinschaft (DFG, German Re search Foundation) – project number 467367360. We thank the anonymous reviewers for their valuable comments and suggestions. REFERENCES Richard B Anderson and Beth M Hartzler. Belief bias in the perception of sample size adequacy. Thinking & Reasoning, 20(3):297–314, 2014. Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Arian Hosseini, Pushmeet Kohli, and Edward Grefenstette. Learning to understand goal specifications by modelling reward. arXiv preprint arXiv:1806.01946, 2018. Osbert Bastani, Yewen Pu, and Armando Solar-Lezama. Verifiable reinforcement learning via policy extraction. Advances in Neural Information Processing Systems, 31, 2018. Ioana Bica, Daniel Jarrett, Alihan Hüyük, and Mihaela van der Schaar. Learning" what-if" explana tions for sequential decision-making. arXiv preprint arXiv:2007.13531, 2020. Erdem Biyik and Dorsa Sadigh. Batch active preference-based learning of reward functions. In Conference on robot learning, pp. 519–528. PMLR, 2018. Mark Bovens. Analysing and assessing accountability: A conceptual framework 1. European law journal, 13(4):447–468, 2007. Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. The method of paired comparisons. Biometrika, 39(3/4):324–345, 1952. Miles Brundage, Shahar Avin, Jasmine Wang, Haydn Belfield, Gretchen Krueger, Gillian Hadfield, Heidy Khlaaf, Jingying Yang, Helen Toner, Ruth Fong, et al. Toward trustworthy ai development: mechanisms for supporting verifiable claims. arXiv preprint arXiv:2004.07213, 2020. Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems, 30, 2017. Jonathan Cohen, Jean-Louis Vincent, Neill K J Adhikari, Flavia R Machado, Derek C Angus, Thierry Calandra, Katia Jaton, Stefano Giulieri, Julie Delaloye, Steven Opal, Kevin Tracey, Tom van der Poll, and Eric Pelfrene. Sepsis: a roadmap for future research. Lancet Infect Dis, 15(5): 581–614, April 2015. R Phillip Dellinger, Jean M Carlet, Henry Masur, Herwig Gerlach, Thierry Calandra, Jonathan Cohen, Juan Gea-Banacloche, Didier Keh, John C Marshall, Margaret M Parker, Graham Ramsay, Janice L Zimmerman, Jean-Louis Vincent, Mitchell M Levy, and Surviving Sepsis Campaign 10
* * *
## Published as a conference paper at ICLR 2024
Management Guidelines Committee. Surviving sepsis campaign guidelines for management of severe sepsis and septic shock. Crit Care Med, 32(3):858–873, March 2004. Rati Devidze, Goran Radanovic, Parameswaran Kamalaruban, and Adish Singla. Explicable reward design for reinforcement learning agents. Advances in Neural Information Processing Systems, 34:20118–20131, 2021. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improv ing factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023. Jonathan St BT Evans and Jodie Curtis-Holmes. Rapid responding increases belief bias: Evidence for the dual-process theory of reasoning. Thinking & Reasoning, 11(4):382–389, 2005. Ioannis Faros, Aditya Dave, and Andreas A Malikopoulos. A q-learning approach for adherence aware recommendations. arXiv preprint arXiv:2309.06519, 2023. Julien Grand-Clément and Jean Pauphilet. The best decisions are not the best advice: Making adherence-aware recommendations. arXiv preprint arXiv:2209.01874, 2022. Gillian K Hadfield. Explanation and justification: Ai decision-making, law, and the rights of citizens. University of Toronto/Schwartz Reisman Institute for Technology and Society, 18, 2021. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026–1034, 2015. Marcel Hildebrandt, Jorge Andres Quintero Serna, Yunpu Ma, Martin Ringsquandl, Mitchell Joblin, and V olker Tresp. Reasoning on knowledge graphs with debate dynamics. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 4123–4131, 2020. Yong Huang, Rui Cao, and Amir Rahmani. Reinforcement learning for sepsis treatment: A continu ous action space solution. In Machine Learning for Healthcare Conference, pp. 631–647. PMLR, 2022. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448–456. pmlr, 2015. Geoffrey Irving, Paul Christiano, and Dario Amodei. Ai safety via debate. arXiv preprint arXiv:1805.00899, 2018. Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. Scientific data, 3(1):1–9, 2016. Kishor Jothimurugan, Rajeev Alur, and Osbert Bastani. A composable specification language for reinforcement learning tasks. Advances in Neural Information Processing Systems, 32, 2019. Z Juozapaitis, A Koul, A Fern, M Erwig, and F Doshi-Velez. Explainable reinforcement learning via reward decomposition, ijcai. In Proceedings at the International Joint Conference on Artificial Intelligence. A Workshop on Explainable Artificial Intelligence., 2019. Matthieu Komorowski, Leo A Celi, Omar Badawi, Anthony C Gordon, and A Aldo Faisal. The arti ficial intelligence clinician learns optimal treatment strategies for sepsis in intensive care. Nature medicine, 24(11):1716–1720, 2018. Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language models. arXiv preprint arXiv:2303.00001, 2023. Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scal able agent alignment via reward modeling: A research direction. arxiv 2018. arXiv preprint arXiv:1811.07871, 2018. 11
* * *
## Published as a conference paper at ICLR 2024
Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in Neural Information Processing Systems, 30, 2017. Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Jerry Zhu. Policy poisoning in batch reinforcement learning and control. Advances in Neural Information Processing Systems, 32, 2019. Andrew L Maas, Awni Y Hannun, Andrew Y Ng, et al. Rectifier nonlinearities improve neural network acoustic models. In International Conference on Machine Learning, volume 30, pp. 3. Atlanta, GA, 2013. V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015. T Nathan Mundhenk, Barry Y Chen, and Gerald Friedland. Efficient saliency maps for explainable ai.arXiv preprint arXiv:1911.11293, 2019. Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q&a; models. arXiv preprint arXiv:1909.05863, 2019. Aniruddh Raghu, Matthieu Komorowski, Imran Ahmed, Leo Celi, Peter Szolovits, and Marzyeh Ghassemi. Deep reinforcement learning for sepsis treatment. arXiv preprint arXiv:1711.09602, 2017. Ronilo Ragodos, Tong Wang, Qihang Lin, and Xun Zhou. Protox: Explaining a reinforcement learning agent via prototyping. Advances in Neural Information Processing Systems, 35:27239– 27252, 2022. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. "why should i trust you?" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135–1144, 2016. Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local ization. In Proceedings of the IEEE international conference on computer vision, pp. 618–626, 2017. Pier Giuseppe Sessa, Ilija Bogunovic, Andreas Krause, and Maryam Kamgarpour. Contextual games: Multi-agent learning with side information. Advances in Neural Information Process ing Systems, 33:21912–21922, 2020. Yoav Shoham and Kevin Leyton-Brown. Multiagent systems: Algorithmic, game-theoretic, and logical foundations. Cambridge University Press, 2008. Mervyn Singer, Clifford S Deutschman, Christopher Warren Seymour, Manu Shankar-Hari, Djillali Annane, Michael Bauer, Rinaldo Bellomo, Gordon R Bernard, Jean-Daniel Chiche, Craig M Coopersmith, et al. The third international consensus definitions for sepsis and septic shock (sepsis-3). Jama, 315(8):801–810, 2016. Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, pp. 2094– –2100, 2016. J. von Neumann and O. Morgenstern. Theory of Games and Economic Behavior. Princeton Univer sity Press, 1947. 12
* * *
## Published as a conference paper at ICLR 2024
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, pp. 1995–2003. PMLR, 2016. A L IST OF APPENDICES In this section, we provide a brief description of the content provided in the appendices of the paper. • Appendix B provides details about the patient dataset and the defined environment: o B.1 provides a general background on sepsis; o B.2 provides details about the patient cohort; o B.3 provides details about the environment’s action space; o B.4 provides details about the environment’s reward structure. • Appendix C provides details about the models: o C.1 provides details about the judge model; o C.2 provides details about the argumentative agents and their confusers; o C.3 provides details about justifiable and baseline agents. • Appendix D provides additional results: o D.1 provides results involving debates with L= 4evidence; o D.2 provides further analysis of justifiable agent’s actions; o D.3 provides additional results pertaining to the isolated agent; o D.4 provides results for agents using an alternative definition of the utility function; o D.5 provides analysis of alternative methods for the preference dataset definition. B D ATASET B.1 S EPSIS Sepsis is a life-threatening condition, defined as severe infection leading to an acute organ dysfunc tion, which in turn can cause a cascade of changes that damage multiple organ systems (Singer et al., 2016). It is also one of the leading causes of patient mortality (Cohen et al., 2015). Apart from administration of antibiotics and control of the infection source, a critical challenge in man agement of sepsis lies in the administration of intravenous fluids (IV) and vasopressors (VC). While international efforts attempt to provide a general guidance (Dellinger et al., 2004), clinicians are nevertheless tasked in devising individualized treatments based on specificities of patients. A first step towards automated management of septic patients was done in a seminal work of Ko morowski et al. (2018). The problem of treating sepsis was tackled by applying reinforcement learning to devise optimal treatment strategies for prescribing doses of intravenous fluids and vaso pressors. The authors discretized the possible doses, which resulted in an action-space consisting of 25 distinct choices. The patient data was obtained from the MIMIC-III dataset (Johnson et al., 2016), and the authors extracted a subset of 48 patient features, discretized into 4h time windows. The pre liminary results indicated that a learned policy was both quantitatively and qualitatively desirable. Since then, several other works extend and improve upon on this line of research. For example, Raghu et al. (2017) proposed a continuous state-space approach based on deep reinforcement learn ing, on which we build on in this work. Likewise, Huang et al. (2022) proposed an approach that is based on continuous action-space, thus enabling more granular control of prescribed doses. B.2 P ATIENT COHORT To assemble our patient cohort, we utilize the MIMIC-III v1.4 database (Johnson et al., 2016), focusing our analysis on patients that fulfill the Sepsis-3 criteria (Singer et al., 2016). Similar as in Komorowski et al. (2018), the sepsis is defined as a suspected existence of infection (indicated by the prescribed antibiotics) paired with a mild evidence of organ dysfunction (indicated by the 13
* * *
## Published as a conference paper at ICLR 2024
## _Table 1: List of features comprising the patient vector. We select 40 time-varying continuous fea-_
tures and 4 demographic discrete features. Continuous features
* * *
SOFA Calcium Shock index Urine output 4h Urine output total Cumulated balance Glasgow coma scale Heart rate Systolic blood pressure Mean blood pressure Diastolic blood pressure Total input fluids Respiratory rate Temperature FiO2fraction of inspired oxygen Potassium Sodium Chloride Glucose Magnesium SIRS Hemoglobin White blood cells count Platelets count Partial Thromboplastin Time PHAcidity PaO2 PaCO2 Base excess Bicarbonate Lactate PaO2/FiO2 Ratio Oxygen saturation BUN Creatinine SGOT SGPT Total bilirubin International normalized ratio Prothrombin time Discrete features Age Gender Weight Mechanical ventilation SOFA score ≥2). To extract, preprocess and impute the data we utilize the pipeline described in Komorowski et al. (2018), a process which resulted in 18,585 different patients that comprised our cohort. The cohort is split into three chunks of sizes 70%, 15%, 15% representing the train, validation, and test splits. The observed mortality of the entire cohort was slightly above 6%, and the splits were selected to approximately preserve this ratio. The course of a patient treatment is represented as a trajectory consisting of state-action pairs, terminating upon patient discharge or death. The average trajectory length was 13, with 2 being the smallest and 20 being the largest. The state is a 44 dimensional vector, comprised of 40 time-varying continuous and 4 demographic discrete features, shown in Table 1. Patients’ data were discretized using 4h time steps, where variables with multiple measurements within this window were averaged (e.g., heart rate) or summed (e.g., urine output) as appropriate. B.3 A CTION SPACE Argumentative Policies. The evidence (action) space of the argumentative policies is defined by the total number of state features, 44in our case, as listed in Table 1. To prevent the agent from repeating already proposed arguments, we additionally employ action masking, setting the log probability of already presented arguments to negative infinity. Baseline and Justifiable Policy. To devise an action space of policies treating sepsis, we follow previous work (Komorowski et al., 2018; Raghu et al., 2017) and focus on managing the total volume of intravenous fluids (IV) and maximum dose of vasopressors (VC) administered to the patient over a4h discretization window. The dose of each treatment is represented as one of four possible non null choices derived from observed doses divided into four quartiles. We additionally define another choice, designated as an option ’no drug given’. The combination of these produced 25possible discrete actions, 5per each treatment, comprising the action space of the policy. 14
* * *
## Published as a conference paper at ICLR 2024
B.4 R EWARDS Sepsis. To define intermediate and terminal rewards for treating septic patients, following the work of Komorowski et al. (2018), we defined the primary outcome of the treatment via 90-day patient mortality. Therefore, the agent’s objective is to optimize for patient survival. To this end, we issue terminal environment rewards of ±15for every patient discharge and death, respectively. To stabi lize the training of a deep-rl policy, we also issue intermediate rewards that are clinically guided, as in Raghu et al. (2017). These rewards are comprised of fairly reliable indicators of the patient’s overall health, namely the SOFA score (measure of organ failure) and a patient’s lactate levels (mea sure of cell-hypoxia, which is usually higher in septic patients). The rewards for intermediate time steps are then shaped as follows: r(st, st+1) =C0 1(sSOFA t+1=sSOFA t&sSOFA; t+1>0) +C2(sSOFA t+1−sSOFA t) +C2tanh(sLactate t+1−sLactate t) where C0,C1andC2are tunable parameters which we set to C0=−0.025,C1=−0.125and C2=−2, following previous work of Raghu et al. (2017). Rewards defined in this way penalize both, the high SOFA scores and lactate values, as well as increases in these quantities. C M ODELS C.1 J UDGE We defined a judge as a scalar reward function Jθ(a,{e})∈Rthat quantifies the level of support a decision ahas by the set of evidences {e}. The judge is parametrized by weights θ∈Rd1of a neural network with two hidden layers of size 256, using parametric relu (He et al., 2015) activation and batch normalization (Ioffe & Szegedy, 2015). The network takes as input values of proposed evidence {e}, a binary mask wherein all elements corresponding to the evidence {e}are assigned a value of one, while the remaining elements are set to zero, as well as a one-hot encoded action. The addition of a binary mask empirically led to a more stable learning. During training, we augment a tuple (st, a0, a1, p)∼ D with an evidence set of state features sampled uniform-random from the statesti.e.,{e} ∼ U, anew for each training batch. To learn the parameter θ, we minimize the cross-entropy loss between preference-predictions and labels from the dataset: min θ∈Rd−E(st,{e},a0,a1,p)∼D[p·logP(a1≻a0,{e}) + (1 −p)·logP(a0≻a1,{e})] The learning is done for a total of 100 epochs using a batch size of 64, Adam optimizer and a learning rate of 5e-4. C.2 A RGUMENTATION All argumentative models utilize the same network architecture comprised of 2hidden layers of size512with a leaky-relu activation function with slope of 1e-2. The network input consists of a 44 dimensional patient state vector, the decision for which the agent is arguing, as well as a binary mask of arguments proposed thus far. The action space of the agent is represented by all 44state features (see App. B.3 for more details). For training, we use PPO (Schulman et al., 2017), running the procedure for 1M steps using the Adam optimizer with a learning rate 5e-4and a batch size of 128. The discount factor was empirically tuned and set to 0.9. The full list of hyperparameters and their considered tuning ranges is given in Table 2. During training, the agent’s policy is stochastic: the evidence is sampled from a categorical distribution defined by its logits. To obtain a deterministic policy used in evaluations, we perform an argmax operator over obtained logits in a particular state. C.2.1 I SOLATED AGENT When investigating robustness of the multi-agent debate, we examine two different setups involving an isolated agent baseline: precommit (reported in the main paper, Plot 4b) and adaptive (reported in the App. D.3, Plot 6b). The former represents an easier case for the agent, but lacks the debate structure. The latter uses a full debate setup as described in Sec. 3.2, but evaluates the isolated agent in a setup slightly different from one it was trained in. Precommit. In this case, an isolated agent is trained to propose evidence {e}of size L/2that for a given apmaximizes J′(ap,{e}), where J′is a new judge trained to evaluate L/2evidence. When 15
* * *
## Published as a conference paper at ICLR 2024
## _Table 2: Hyperparameters used for argumentative agents. Unless otherwise indicated, all agents_
utilize the same parameters. Common parameters
* * *
Parameter name Parameter value Tuning range Hidden dim 512 [256, 512] Learning rate 5e-4 loguniform[1e-5:1] Entropy coefficient 1e-2 loguniform[0.00000001:0.1] Clip range 0.1 [0.1, 0.2, 0.3, 0.4] Discount 0.9 [0.8, 0.9, 0.95, 0.99] GAE lambda 0.7 [0.7, 0.8, 0.9, 0.92, 0.95, 0.98, 0.99, 1.0] VF weight 0.5 [0.3, 0.5, 0.65, 0.75] Max grad norm 0.1 [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 5] Normalize rewards true [true, false] Ortho init true [true, false] Confuser parameters Hidden dim 256 [256, 512] Entropy coefficient 3e-4 loguniform[0.00000001:0.1] Clip range 0.4 [0.1, 0.2, 0.3, 0.4] VF weight 0.65 [0.3, 0.5, 0.65, 0.75] Max grad norm 2 [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 5] evaluating robustness (Plot 4b), the agent first proposes L/2evidence, followed by a confuser agent which proposes the remainder. Adaptive. In this case, an isolated agent is trained to propose all Levidence {e}that maximize J(ap,{e}), for a given ap. When evaluating robustness (Plot 6b), the isolated agent and its associ ated confuser take turns and propose a total of L/2evidence each. C.2.2 D EBATE AGENTS For multi-agent scenarios, both maxmin andself-play agents use the architecture described in the beginning of Sec. C.2, but each modify the underlying optimization pipeline. To train a self-play debate agent, we let the agent argue with a (frozen) copy of itself, updating it every 100k steps. This procedure is then repeated for a fixed number of 500generations. To train the maxmin debate agent, we parametrize the agent’s opponent with a different set of weights ϕ2∈Rd2. The procedure starts by training the main agent for 4k steps, followed by training of its opponent for 100k steps. Like in the previous case, the procedure is repeated for 500generations. This approach is based on the bi-level optimization and allows for overfitting the opponent to the current version of the main agent, which ensures learning of defense strategies against a very strong adversary. C.2.3 C ONFUSER AGENTS The evaluation of robustness we presented in Section 5.4 required learning three separate adver saries (one for each argumentative agent), explicitly tasked in providing counterarguments that will confuse the judge. The architecture of these confuser agents is mostly the same as that of the argu mentative agents, shown in Table 2. For a sample (st, a0, a1, p)∼ D, the confuser agent is rewarded positively, whenever the judge is convinced of the alternative (non-preferred) action. 16
* * *
## Published as a conference paper at ICLR 2024
## _Table 3: Hyperparameters used for baseline and justifiable policies. Unless otherwise indicated, all_
policies use the same parameters. If a parameter does not specify a tuning range, its value has either
* * *
been selected based on previous work (e.g., buffer αandβ) or tuning was not necessary (e.g., debate coefficient λ). Baseline policy Parameter name Parameter value Tuning Range Hidden dim 128 [64, 128, 256] Learning rate 1e-4 loguniform[1e-5:1e-1] Batch size 256 [128, 256, 512, 1024] Polyak update 1e-3 [1e-1, 1e-2, 1e-3, 1e-4] ReLu slope 1e-2 [1e-1, 1e-2, 1e-3] Discount 0.99 Num. estimation step 6 [1, 3, 6, 10, 15] Terminal reward ±15 Debate scaling coefficient α ±5 Debate coefficient λ 0.0 Buffer α 0.6 Buffer β 0.9 λ∈[0.25, 0.5, 0.75] Num. estimation step 3 λ= 1.0 Num. estimation step 1 C.3 B ASELINE AND JUSTIFIABLE AGENTS Our approach for learning treatment policies for septic patients is based on continuous state-space models and builds on Raghu et al. (2017). The policy is based on a variant of Deep-Q networks (Mnih et al., 2015), specifically double-deep (Van Hasselt et al., 2016) Q-network, also employing the dueling architecture (Wang et al., 2016), where the estimated action-value function for a pair (s, a)is split into separate value andadvantage streams. The final network consists of 2fully connected layers of size 128using leaky-relu activation functions with slope 1e-2. The learning is done in batches of 256 ( s, a, r, s′)tuples sampled from a Prioritized experience replay buffer (Schaul et al., 2015) with a learning rate of 1e-4. Instead of periodically updating the target net work, we leverage Polyak averaging with an update coefficient τset to 1e-3. The full list of used hyperparameters is given in Table 3. Similar to Raghu et al. (2017), we augment the standard Q network loss. First, we added a regularization term that penalizes Q-values outside the allowed threshold Qthresh=±20. In addition, we clip the target network outputs to ±20, which empirically proved to stabilize the learning. The final loss function we used is given by: L(Θ) = E (Qdouble-target −Q(s, a; Θ))2 +β·max(|Q(s, a; Θ)| −Qthresh,0) Qdouble-target =r+γ·Q(s′,arg maxa′Q(s′, a′; Θ); Θ′) Where βis a user-specified coefficient we set to β= 5.0in all our experiments following Raghu et al. (2017). The learning is done for a total of 25k iterations, evaluating the policy every 50 iterations using weighted importance sampling (WIS) (Sutton & Barto, 2018) on a held-out test set. Behavioral Policy. The calculation of the importance sampling ratio requires access to a so-called behavioral policy that generated offline samples. To obtain it, we train a behavior-cloning (BC) clinician policy to take in a state stfrom the patient cohort and predict the action attaken by the human clinician, minimizing the cross-entropy loss over the training dataset. The network consists of two fully-connected layers of size 64. We run the training with the Adam optimizer, using a learning rate of 1e-3and a weight decay set to 1e-1for a total of 100epochs with a batch size of 64. 17
* * *
## Published as a conference paper at ICLR 2024
D A DDITIONAL RESULTS D.1 S HORTER DEBATES In the main text, we have seen positive results involving debate using 6arguments, which amounts to∼13% of the entire state. In this section, we want to further examine the effectiveness of debate when limiting the number of evidence to L= 4, amounting to 9% of the entire state. The first question that arises when further limiting the amount of state visibility during debate is the impact it has on the quality of learned task policies. In Plots 5a-5d, we evaluate the performance of different justifiable policies on a held-out test set during the course of training using weighted importance sampling, for the case of 4and6evidence limit. While the reduced number of evidence exposes the judge to only 9% of the entire state, we can see that the achieved performance is com parable to the case where L= 6. Furthermore, in Plot 5f, we also confirm that trained policies are significantly more preferred to the baseline policy, a trend equivalent to the one we saw in the main text. Apart from these quantitative evaluations, in Plots 5g and 5h, we show the qualitative analysis of patient mortality from Sec. 5.2. We confirm that the lowest mortality is observed when clinician prescribed doses recommended by justifiable policies, thus further signaling potential validity of policies trained with rewards stemming from debates with L= 4evidence. To evaluate the effectiveness of argumentative agents, we repeat the evaluation from Sec. 5.4. When exposed to 4randomly selected evidence, the judge trained via the procedure outlined in App. C.1 achieves accuracy of ∼59%. Plot 5e shows the judge’s accuracy when different argumentative agents propose L= 4evidence12, averaged over 1000 samples (st, a1, a2, p)∼ D. Without the confuser, all three agents achieve performance similar to the case of L= 6 evidence, boosting the judge’s accuracy to almost 90%. In the setup involving an adversary, agents propose a total of 2evidence each before the judge evaluates the outcome. The observed trend is also similar to the one from the main text (Sec. 5.4). D.2 P REFERENCE BREAKDOWN To gain a better understanding about the behavior of learned justifiable policies, we further examine actions they propose. In particular, Plot 6a shows the percent of times an action from the justifiable policy was preferred to that of the baseline policy (JP), percent of times when the action of the baseline policy was preferred to the action of the justifiable policy (BP), and percent of time when the two were equally preferred (EP)13. We see that the number of actions proposed by the justifiable agent and the baseline agent that are equally preferred decreases as the parameter λis increased: for λ= 0.25two agents chose the equally justifiable action about 49% of the times, for λ= 0.50this number drops to 40%, whereas for λ= 0.75andλ= 1.0this number is 36% and35% respectively. Out of the remaining actions, the ones from justifiable policies were increasingly more preferred, as the parameter λwas increased. D.3 I SOLATED AGENT SETUP In App. C.2.1, we described two setups which we consider when evaluating robustness of the isolated agent, namely precommit andadaptive. In Plot 6b, we show the accuracy of the judge in predicting the preferred action when evidence was proposed by an isolated agent trained with one of these approaches for 1000 different debate games. Without the confuser, both approaches amplify the capabilities of the judge, performing roughly equivalent. When faced with a confuser agent, the precommit approach performs better, since in the adaptive case, the agent is evaluated in a debate-like setup, which was not accounted for during training. In both cases, however, we observe that the isolated agent is not robust to an adversary. D.4 A LTERNATIVE UTILITY FUNCTIONS In Sec. 3.2, we have defined the utility function Uto output binary values {−1,0,+1}based on the justifiability rewards obtained from the judge. One might ask if there are alternative ways of 12Like in the main text, we use the precommit setup for the isolated agent (see App. C.2.1 for more details). 13In our experiments, the judge deemed two actions equally justifiable whenever they were the same. 18
* * *
## Published as a conference paper at ICLR 2024
0 100 200 300 400 500 Iterations0.300.400.500.600.700.800.901.00Performance 6 evidence 4 evidence clinician0 100 200 300 400 500 Iterations0.300.400.500.600.700.800.901.00Performance 6 evidence 4 evidence clinician (a)λ= 0.25 0 100 200 300 400 500 Iterations0.300.400.500.600.700.800.901.00Performance 6 evidence 4 evidence clinician0 100 200 300 400 500 Iterations0.300.400.500.600.700.800.901.00Performance 6 evidence 4 evidence clinician (b)λ= 0.5 0 100 200 300 400 500 Iterations0.300.400.500.600.700.800.901.00Performance 6 evidence 4 evidence clinician0 100 200 300 400 500 Iterations0.300.400.500.600.700.800.901.00Performance 6 evidence 4 evidence clinician (c)λ= 0.75 0 100 200 300 400 500 Iterations0.300.400.500.600.700.800.901.00Performance 6 evidence 4 evidence clinician0 100 200 300 400 500 Iterations0.300.400.500.600.700.800.901.00Performance 6 evidence 4 evidence clinician (d)λ= 1.0 isolated maxmin self-play Argumentative Agent0.000.200.400.600.801.00Judge Accuracywithout confuser with confuserisolated maxmin self-play Argumentative Agent0.000.200.400.600.801.00Judge Accuracywithout confuser with confuser (e) Convincing success λ=0.25 λ=0.50 λ=0.75 λ=1.0 Policy Variant0.500.600.700.800.901.00Preference Over Baselineλ=0.25 λ=0.50 λ=0.75 λ=1.0λ=0.25 λ=0.50 λ=0.75 λ=1.0 Policy Variant0.500.600.700.800.901.00Preference Over Baselineλ=0.25 λ=0.50 λ=0.75 λ=1.0 (f) Preferred to baseline −800 −400 0400 800 Excess0.000.100.200.300.400.50Mortality λ=0.0 λ=0.25 λ=0.50 λ=0.75 λ=1.0−800 −400 0400 800 Excess0.000.100.200.300.400.50Mortality λ=0.0 λ=0.25 λ=0.50 λ=0.75 λ=1.0 (g) IV dose excess −0.50−0.25 0.000.250.50 Excess0.000.100.200.300.400.50Mortality λ=0.0 λ=0.25 λ=0.50 λ=0.75 λ=1.0−0.50−0.25 0.000.250.50 Excess0.000.100.200.300.400.50Mortality λ=0.0 λ=0.25 λ=0.50 λ=0.75 λ=1.0 (h) VC dose excess
* * *
## _Figure 5: Quantitative and qualitative evaluation of policies trained using debate-based rewards_
limited to L= 4evidence. For (f)-(h), the confidence intervals represent ±2standard errors of the
* * *
mean over 5random seeds. (a)-(d) Performance of policies trained with L= 4andL= 6evidence, as measured by WIS evaluation on a held-out test set with ±1terminal rewards for every patient discharge or death. The mean and standard deviation are reported over 5random seeds. (e) Accuracy of the judge in predicting the preferred action using 4proposed evidence, with and without the confuser agent. The CI represent ±2standard errors of the mean estimate. (f) Percent of times judge preferred decisions of justifiable policies (i.e., λ > 0.0) compared to those of the baseline policy (i.e.,λ= 0.0). (g) (h) Observed patient mortality (y-axis) against variations in IV/VC treatment doses prescribed by clinicians compared to the recommendations of learned policies (x-axis). BP EP JP Preference0.000.100.200.300.400.50Percent of T otal Actionsλ=0.25 λ=0.50 λ=0.75 λ=1.0BP EP JP Preference0.000.100.200.300.400.50Percent of T otal Actionsλ=0.25 λ=0.50 λ=0.75 λ=1.0 (a) Preference breakdown precommit adaptive Agent Type0.000.200.400.600.801.00Judge Accuracy without confuser with confuserprecommit adaptive Agent Type0.000.200.400.600.801.00Judge Accuracy without confuser with confuser (b) Isolated agent setup MM MM-diff SP SP-diff Argumentative Agent0.000.200.400.600.801.00Judge Accuracy without confuser with confuserMM MM-diff SP SP-diff Argumentative Agent0.000.200.400.600.801.00Judge Accuracy without confuser with confuser (c) Convincing success offset random exhaustive Dataset Generation Method0.000.200.400.600.801.00Judge Accuracyoffset random exhaustive Dataset Generation Method0.000.200.400.600.801.00Judge Accuracy (d) Preference datasets
* * *
## _Figure 6: (a) Percent of times when actions of the justifiable policy were more preferred to those of_
the baseline policy (JP), less preferred (BP) and equally preferred (EP). (b) Accuracy of the judge
* * *
in predicting the preferred action, with and without the confuser agent when evidence was proposed by two isolated agents trained in precommit andadaptive setup. The CI represents ±2standard errors of the mean estimate. (c) Accuracy of the judge in predicting the preferred action, when evidence was proposed by maxmin (MM) and self-play (SP) agents trained using the utility function proposed in Sec. 3.2 compared to agents trained using the utility function defined as a difference in predicted rewards proposed in App D.4, MM-diff and SP-diff respectively. The CI represents ±2 standard errors of the mean estimate. (d) Accuracy of the judge in predicting the preferred action with evidence sampled uniform-random, trained on datasets generated with different methods. 19
* * *
## Published as a conference paper at ICLR 2024
defining Uthat preserve the zero-sum structure and potentially positively influence the learning. In this section, we consider one particular alternative that defines the utility function based on the difference between predicted rewards, which might provide a more informative learning signal. In particular, we set u1(nL) =−u2(nL) =U(at, aB t,{enL}), with Udefined as U(at, aB t,{e}) = J(at,{e})− J(aB t,{e}). We rerun the experiments from Sec. 5.4 for maxmin and self-play agents and show preference recovery success and robustness results in Plot 6c. We can see that the two approaches perform similarly in situations which do not involve a confuser agent. However, it seems that defining the utility using the difference in judge’s rewards leads to slightly lower scores when debate agents are faced with an adversary. D.5 A LTERNATIVE PREFERENCE DATASET In Sec. 5.1 we construct a preference dataset by matching every pair (st, at)from the cohort with an alternative action ar∼ U(A), ar̸=atsampled uniform-random from the action space. In this section, we examine additional strategies one could take when constructing such a synthetic dataset. In particular, we consider the random strategy we just described, paired with two alternative ones, namely exhaustive andoffset. The exhaustive strategy pairs atwith all possible alternative actions, 24in total. The offset strategy pairs atwith an alternative action that is in its neighborhood. To define a neighborhood, we recall that there are a total of 5choices for both, vasopressors (VC) and intravenous fluids (VC). Therefore, we can write at= 5∗IV+VC, where IV,VC∈ {0,1,2,3,4}. To obtain an alternative action, we consider changing IV and VC by an offset sampled uniform random from a set {−1,0,1}, for both IV and VC. We then train a new judge for each of the datasets and show its accuracy in Plot 6d. The exhaustive variant represents the most informative, but also unrealistically large, dataset. The random variant represents somewhat of a “middle ground” in terms of dataset difficulty. Lastly, the offset variant represents the most difficult case, as differences between two actions are more nuanced. However, while the achieved accuracies reflect the difficulty of the corresponding dataset, the capabilities of a trained judge model are roughly the same. 20
* * *
