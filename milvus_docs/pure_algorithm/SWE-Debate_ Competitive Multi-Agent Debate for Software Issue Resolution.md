## SWE-Debate: Competitive Multi-Agent Debate for Software Issue
Resolution Han Li† Shanghai Jiao Tong University China lihan0421@sjtu.edu.cnYuling Shi† Shanghai Jiao Tong University China yuling.shi@sjtu.edu.cnShaoxin Lin Huawei China 2120200411@mail.nankai.edu.cn Xiaodong Gu‡ Shanghai Jiao Tong University China xiaodong.gu@sjtu.edu.cnHeng Lian Xidian University China lianheng23@163.comXin Wang Huawei China betterwangx@foxmail.com Yantao Jia Huawei China jamaths.h@163.comTao Huang Huawei China 276255565@qq.comQianxiang Wang Huawei China wangqianxiang@huawei.com ABSTRACT Issue resolution has made remarkable progress thanks to the ad vanced reasoning capabilities of large language models (LLMs). Recently, agent-based frameworks such as SWE-agent have further advanced this progress by enabling autonomous, tool-using agents to tackle complex software engineering tasks. While existing agent based issue resolution approaches are primarily based on agents’ independent explorations, they often get stuck in local solutions and fail to identify issue patterns that span across different parts of the codebase. To address this limitation, we propose SWE-Debate, a competitive multi-agent debate framework that encourages diverse reasoning paths and achieves more consolidated issue localization. SWE-Debate first creates multiple fault propagation traces as local ization proposals by traversing a code dependency graph. Then, it organizes a three-round debate among specialized agents, each em bodying distinct reasoning perspectives along the fault propagation trace. This structured competition enables agents to collaboratively converge on a consolidated fix plan. Finally, this consolidated fix plan is integrated into an MCTS-based code modification agent for patch generation. Experiments on the SWE-bench benchmark show that SWE-Debate achieves new state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin1.
* * *
## _1 INTRODUCTION_
Automated repository-level issue resolution has emerged as a criti
* * *
_†Equal contribution._
cal challenge in software engineering. The task aims to automat ically localize and fix the defective code snippets, based on re ported issues. In software development, developers spend a major ity of their debugging efforts in understanding code and making changes [ 15,52]. Meanwhile, automated tools often struggle with the same challenge [ 4,33,39]. Inadequate code understanding leads ‡Xiaodong Gu is the corresponding author. 1Our code and data are available at https://github.com/YerbaPage/SWE-Debateto incomplete fixes, introduces new bugs, and significantly extends development cycles [5, 7]. The key challenge in effective issue resolution is fault localiza tion, namely, identifying the code snippets triggering the specific issue [ 43]. Unlike conventional code retrieval, fault localization re quires a deeper connection between natural language issue descrip tions and programming language structures. This process requires reasoning over the structural and semantic properties of code, of ten across complex dependency graphs [ 6,14,18], and entails a comprehensive understanding of software architecture as well as strategic decision-making. The emergence of LLMs has significantly advanced this area by leveraging code understanding and reasoning capabilities [ 35, 42,44]. More recently, agent-based methods [ 1,3,38,46,52] have emerged, simulating autonomous agents capable of tool use and high-level decision-making. These approaches use iterative explo ration and planning to enable systematic codebase traversal, repre senting a shift toward structured and interactive issue resolution processes [22, 43]. While agent-based approaches have shown notable progress on standard benchmarks such as SWE-bench [ 15], they mainly rely on agents’ independent exploration, that is, the agents individually understand code repository and propose their modification plans. As a result, they often get stuck in local solutions and fail to identify issue patterns that span across large, complex codebases [ 5,7]. This fundamental limitation stems from a core challenge we term limited observation scope [32,48]: when multiple code locations appear rel evant to the issue description, correct resolution often depends on a deep understanding of code structure and component relationships. However, independent exploring agents lack the diverse analyti cal perspectives needed to systematically compare and rank these competing alternatives. This limitation becomes more pronounced when agents are faced with multiple plausible fix strategies or mod ification points, each with different implications for maintainability, compatibility, and architectural soundness [6, 14, 18, 52]. Without sufficient reasoning capacity to evaluate these trade-offs holisti cally, independent exploring agents often fail to directly identifyarXiv:2507.23348v1 [cs.SE] 31 Jul 2025
* * *
Conference’17, July 2017, Washington, DC, USA Li et al. the correct fix location and strategy, relying instead on repeated trial-and-error that is both inefficient and error-prone [1, 10]. To address these challenges, we propose SWE-Debate, a com petitive multi-agent debate framework that promotes diverse rea soning paths and achieves more consolidated fault localization. SWE-Debate reframes issue resolution through graph-guided local ization and structured debate mechanisms. The framework operates through a three-stage pipeline. First, it creates multiple fault prop agation traces as localization proposals by dependency analysis across the codebase. Specifically, a static dependency graph is built to represent relationships among code entities—such as function calls, class inheritance, module imports, and variable references. Using language model-based semantic matching, SWE-Debate iden tifies entities that are most relevant to the issue description, which serve as high-confidence entry points for chain construction. SWE Debate traverses the graph from each entry point, yielding a set of candidate localization chains. Each chain captures a potential fault propagation path, reflecting different structural viewpoints, i.e., al ternative code organization contexts in which the issue may appear, such as along a call hierarchy, inheritance structure, or shared data flow. Next, the algorithm creates a consensus fix plan through a structured three-round debate process. In the first round, multiple agents engage in competitive ranking to select the most promising fault propagation trace. Based on the selected trace, agents inde pendently propose candidate modification plans based on different reasoning perspectives, then engage in competitive refinement to defend their proposals while critiquing alternatives. A discrimina tor selects the most promising plan, synthesizing insights from the debate to produce a coherent and actionable modification plan. In the final stage, the modification plan is used to initialize a Monte Carlo Tree Search (MCTS) framework [1] for patch generation. Our experimental evaluation on the SWE-Bench-Verified dataset systematically compares SWE-Debate against state-of-the-art base lines. SWE-Debate achieves new state-of-the-art results under open source agent frameworks and outperforms baseline methods by a large margin. Ablation studies show that the multiple chain gen eration mechanism provides the largest contribution to overall performance, validating our hypothesis that the fault propagation traces proposal enables more accurate fault localization and issue resolution. Our main contributions include: •A novel method to generate multiple candidate fault propaga tion traces. The method captures diverse potential fault prop agation paths through code dependencies and structural rela tionships. •A competitive multi-agent debate framework for precise fault localization through diverse reasoning perspectives and struc tured argumentation. •Extensive experiments show that our competitive debate paradigms achieve 6.7% improvement in issue resolution rate and 5.1% im provement in fault localization accuracy.
* * *
## _2 MOTIVATION_
Repository-level issue resolution reveals the fundamental limited
* * *
observation scope that agentic approaches face in complex softwareengineering scenarios [ 5,7]. While individual agents can success fully handle straightforward localization tasks, they systematically fail when multiple code locations appear relevant to issue descrip tions, requiring comprehensive architectural understanding and careful evaluation of competing modification plans [ 32,48]. The core challenge lies in the inherent perspective limitations that pre vent single agents from accurately assessing the trade-offs between multiple viable solutions [2, 9]. The Individual Exploration Problem As illustrated in Fig ure 1, single-agent individual exploitation approaches rely on indi vidual exploration where agents independently understand code repositories and propose modification plans without systematic evaluation of alternative approaches. Consider the Django-11999 issue where users cannot override get_FOO_display() methods in Django 2.2+. The isolated agent performs semantic search with query "get_FOO_display impl" and immediately focuses on django/ db/models/base.py, specifically the _get_FIELD_display method. This individual exploration path appears reasonable from a single perspective but represents a fundamental misunderstanding of the issue’s root cause. The individual exploration approach exemplifies the core limited observation scope problem [ 6,14]: the agent cannot systemati cally evaluate whether the base method implementation or the field registration mechanism contains the actual fault source [ 32], lacks the diverse reasoning perspectives necessary to compare run time workarounds against structural solutions [ 18,52], and cannot effectively analyze the architectural trade-offs between different modification plans [ 1,10]. This individual exploration limitation prevents the agent from reconsidering its fundamental localization strategy when initial approaches fail, leading to inefficient trial-and error cycles that characterize single-agent individual exploration methods. Multi-Agent Debate Resolution The correct resolution re quires recognizing that method override failures stem from the field registration process in Field.contribute_to_class, where Django unconditionally overwrites user-defined methods during class construction. This structural insight emerges through multi agent debate where different agents examine alternative code re gions and systematically defend their localization strategies against competing interpretations, moving beyond individual exploration to structured competitive analysis [2, 9]. In the Django-11999 case, multi-agent debate directly addresses the limited observation scope: Agent A advocates for modifying the base _get_FIELD_display method while Agent B argues for intervention in Field.contribute_to_class, forcing systematic comparison of these structurally different approaches. Through structured debate, different agents defend competing modifica tion philosophies—runtime flexibility versus source-level preven tion—revealing architectural trade-offs and maintainability impli cations that are invisible to individual exploration. When initial approaches fail, competitive pressure prevents agents from aban doning promising directions and instead drives systematic analysis of why specific strategies succeed or fail, transforming individual exploration into collaborative reasoning that enables comprehen sive evaluation of the architectural soundness of different solutions. Through this debate process, agents discover that the contribute_ to_class approach provides superior design properties: it prevents
* * *
SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference’17, July 2017, Washington, DC, USA Individual Exploration***stage 1***instruction: Enhance the _get_FIELD_display method in django/db/models/base.py to support custom display methods by checking for a user-defined get_FOO_display method (where FOO is the field name) before falling back to the default display logic.context: django/db/models/base.py:Model._get_FIELD_display, lines 939-944 Competitive Debate***stage 1***instruction: Modify the contribute_to_classmethod in db/models/fields/__init__.pyto check for existing get_FIELD_displaymethod before setting the default implementationcontext: db/models/fields/__init__.py:Field.contribute_to_class, lines 765-767Debate Localization Chains Wrong place! Correct place!Localization Chain Modification Plan Modification Plan Cannot override get_FOO_display() in Django 2.2+.DescriptionI cannot override the get_FIELD_displayfunction on models since version 2.2. It works in version 2.1.Example:class FooBar(models.Model):foo_bar= models.CharField(_(“foo”), choices=[(1, 'foo'), (2, 'bar')])def __str__(self):return self.get_foo_bar_display() # This returns 'foo' or 'bar' in 2.2, but 'something' in 2.1def get_foo_bar_display(self):return "something"What I expect is that I should be able to override this function. ---a/django/db/models/fields/__init__.py+++ b/django/db/models/fields/__init__.py@@ -763,8 +763,12 @@def contribute_to_class(self, cls, name, private_only=False)…-setattr(cls, 'get_%s_display' % self.name,-partialmethod(cls._get_FIELD_display, field=self))+ if not hasattr(cls, 'get_%s_display' % self.name):+ setattr(cls,+ 'get_%s_display' % self.name,+ partialmethod(cls._get_FIELD_display, field=self),+) Golden Patch：Issue django-11999：Individual Exploration***stage 1***instruction: Enhance the _get_FIELD_display method in django/db/models/base.py to support custom display methods by checking for a user-defined get_FOO_display method (where FOO is the field name) before falling back to the default display logic.context: django/db/models/base.py:Model._get_FIELD_display, lines 939-944 Competitive Debate***stage 1***instruction: Modify the contribute_to_classmethod in db/models/fields/__init__.pyto check for existing get_FIELD_displaymethod before setting the default implementationcontext: db/models/fields/__init__.py:Field.contribute_to_class, lines 765-767Debate Localization Chains Wrong place! Correct place!Localization Chain Modification Plan Modification Plan Cannot override get_FOO_display() in Django 2.2+.DescriptionI cannot override the get_FIELD_displayfunction on models since version 2.2. It works in version 2.1.Example:class FooBar(models.Model):foo_bar= models.CharField(_(“foo”), choices=[(1, 'foo'), (2, 'bar')])def __str__(self):return self.get_foo_bar_display() # This returns 'foo' or 'bar' in 2.2, but 'something' in 2.1def get_foo_bar_display(self):return "something"What I expect is that I should be able to override this function. ---a/django/db/models/fields/__init__.py+++ b/django/db/models/fields/__init__.py@@ -763,8 +763,12 @@def contribute_to_class(self, cls, name, private_only=False)…-setattr(cls, 'get_%s_display' % self.name,-partialmethod(cls._get_FIELD_display, field=self))+ if not hasattr(cls, 'get_%s_display' % self.name):+ setattr(cls,+ 'get_%s_display' % self.name,+ partialmethod(cls._get_FIELD_display, field=self),+) Golden Patch：Issue django-11999：
* * *
## _Figure 1: Motivating example of multi-agent debate._
method overwriting at the source rather than attempting runtime
* * *
workarounds, requires minimal code changes at lines 765-767 in django/db/models/fields/__init__.py with a simple existence check using if not hasattr(cls,method_name), and maintains backward compatibility while enabling user method preservation. This debate dynamic creates productive analytical tension where agents must defend their approaches against alternatives [ 19,50], transforming issue resolution from individual exploration to struc tured multi-perspective reasoning that enables precise architectural understanding and modification plans that individual agents cannot achieve [31, 53].
* * *
## _3 METHODOLOGY_
3.1 Problem Formulation
* * *
Given an issue description 𝑝and a codebase state represented as a set of code entities 𝑉={𝑣1,...,𝑣𝑚}, an agent must identify the subset of entities to be modified 𝑉𝑚𝑜𝑑⊂𝑉through an explo ration chain{(𝑎1,𝑜1),...,(𝑎𝑛,𝑜𝑛)}, where𝑎𝑡and𝑜𝑡denote the action and observation at time step 𝑡. Current approaches, however, face two fundamental limitations. First, their exploration is ineffi cient as it overlooks structural relationships between code entities (e.g., classes, methods). Second, they struggle with modification disambiguation when multiple locations seem relevant but require different reasoning perspectives for a correct evaluation. Our approach addresses both limitations through competitive reasoning on a code dependency graph 𝐺=(𝑉,𝐸), where𝑉is the set of code entities and 𝐸represents their structural relationships. We construct localization chains 𝐶=(𝑣1,...,𝑣𝑘)composed of code entities𝑣𝑖∈𝑉that trace fault propagation paths. Subsequently, weemploy a multi-agent debate to resolve modification disambiguation and determine the most effective fix plan. Our framework operates on the principle that accurate fault localization requires diverse structural viewpoints combined with rigorous evaluation mechanisms, as illustrated in Figure 2. To achieve this, we employ a dual-stage competitive debate architecture. The first stage (Section 3.2) tackles the structural exploration problem by efficiently identifying potential fault propagation paths through graph traversal. The second stage (Section 3.3) performs compet itive debate on chain selection and modification disambiguation among multiple agents. Finally, the selected modification plan is integrated into an MCTS-based agentic framework for patch gener ation (Section 3.4). 3.2 Fault Propagation Traces Proposal Since issue descriptions rarely pinpoint exact modification loca tions, agents need to identify the fault propagation trace for issue resolution. To enable competitive fault localization, our algorithm generates multiple candidate fault propagation traces as localiza tion proposals, followed by a competitive multi-agent debate to select the most promising trace. A fault propagation trace refers to a structured chain of code entities (e.g., classes, methods, func tions, variables) that reflect how defects may propagate through the codebase. Dependency Graph Construction. The algorithm begins by building a static dependency graph 𝐺=(𝑉,𝐸), where the node set𝑉represents code entities and the edge set 𝐸captures their de pendency relationships including function calls, class inheritance, module imports, and variable references. This graph serves as the structural backbone for tracing potential fault propagation traces, allowing agents to explore the codebase in a systematic way.
* * *
Conference’17, July 2017, Washington, DC, USA Li et al. InitialEnitity1：get_foo_bar_displayInitialEnitityk-1:FooBarInitialEnitityk：get_FIELD_display Debate Round1Round2Round3 Modification PlanDebate ***stage 1***instruction: Modify the contribute_to_classmethod in db/models/fields/__init__.py to check for existing get_FIELD_displaymethod before setting the default implementationcontext: db/models/fields/__init__.py:Field.contribute_to_class, lines 765-767***stage 2***…… Extract Initial EntitiesFault Propagation Traces Proposal Localization Chain SelectionSynthesize Modification Plan Entity ContainInheritImportInvoke Chain 1 Chain 2 Chain 3 Chain N Editor Patch Environment MCTS Issue django-11999：Cannot override get_FOO_display() in Django 2.2+.DescriptionI cannot override the get_FIELD_displayfunction on models since version 2.2. It works in version 2.1.Example:class FooBar(models.Model):foo_bar= models.CharField(_("foo"), choices=[(1, 'foo'), (2, 'bar')])def __str__(self):return self.get_foo_bar_display() # This returns 'foo' or 'bar' in 2.2, but 'something' in 2.1def get_foo_bar_display(self):return "something"What I expect is that I should be able to override this function. Best Chain InitialEnitity1：get_foo_bar_displayInitialEnitityk-1:FooBarInitialEnitityk：get_FIELD_display Debate Round1Round2Round3 Modification PlanDebate ***stage 1***instruction: Modify the contribute_to_classmethod in db/models/fields/__init__.py to check for existing get_FIELD_displaymethod before setting the default implementationcontext: db/models/fields/__init__.py:Field.contribute_to_class, lines 765-767***stage 2***…… Extract Initial EntitiesFault Propagation Traces Proposal Localization Chain SelectionSynthesize Modification Plan Entity ContainInheritImportInvoke Chain 1 Chain 2 Chain 3 Chain N Editor Patch Environment MCTS Issue django-11999：Cannot override get_FOO_display() in Django 2.2+.DescriptionI cannot override the get_FIELD_displayfunction on models since version 2.2. It works in version 2.1.Example:class FooBar(models.Model):foo_bar= models.CharField(_("foo"), choices=[(1, 'foo'), (2, 'bar')])def __str__(self):return self.get_foo_bar_display() # This returns 'foo' or 'bar' in 2.2, but 'something' in 2.1def get_foo_bar_display(self):return "something"What I expect is that I should be able to override this function. Best Chain
* * *
## _Figure 2: Overview of SWE-Debate framework._
Identifying Entry Nodes via Semantic Matching. Next, the
* * *
algorithm identifies the top- 𝐾entities𝐸𝑝=𝑒1,...,𝑒𝐾that are most relevant to the issue description through semantic matching. We employ a language model-based approach to extract structural identifiers that are explicitly referenced in the issue text from the dependency graph represented as an adjacency matrix and entity metadata. These entities encompass specific code-level structural elements such as class names, function names, parameter names, or error names (e.g., "UserSession", "Redis", "wholesale") that ap pear directly in the issue description. The extraction process priori tizes such structural identifiers while maintaining diversity through deduplication mechanisms. To ensure precision, the selection is con strained to only include entities with direct textual correspondence in the issue description, preventing the introduction of spurious en try points that could mislead subsequent chain construction. These high-confidence entities act as entry points for subsequent chain construction. Chain Construction via Graph Traversal. To capture diverse propagation patterns, we systematically traverse the dependency graph using a two-phase strategy for each seed entity 𝑒𝑖∈𝐸𝑝: (1) Breadth-First Expansion: We identify top- 𝑊most issue-relevant neighboring nodes based on semantic and structural relevance. We first extract the top-K entities from the issue text, which are strongly associated with the described problem. However, in many cases, the root cause of an issue is not explicitly linked to specific functions or components within the issue description. To address this, we expand the context by retrieving code snippets related to each of the top-K entities using the dependency graph. These code snippets, along with the original issue, are then fed into the LLM individually. This enriched context allows the model to identify more diverse and informative entities as potential starting points for the localiza tion chain, effectively expanding the search space and improving localization accuracy. (2) Depth-First Search: From each selectedneighbor, we perform a depth-limited traversal (maximum depth of𝐿), selecting the most promising next entity at each step. The selection is guided by a composite scoring function that considers both semantic similarity to the issue and structural importance in the dependency graph. This process results in a total of top- 𝐾 ×top-𝑊localization chains, each representing a plausible fault propagation path. These localization chains capture fault propagation patterns through dependency relationships—defects in one component af fecting dependent components via call chains, inheritance hierar chies, or data flow. This structured approach efficiently identifies propagation paths that would require extensive exploration to dis cover through search-based methods, enabling the competitive debate process described next. 3.3 Multi-Agent Debate With the localization chains identified through graph-guided anal ysis, we synthesize a consolidated fix plan through a competitive multi-agent debate. This requires evaluating competing architec tural approaches—some chains may target core system components while others suggest more localized fixes, each with different impli cations for maintainability and system robustness. Our competitive debate forces agents to propose, defend, and refine their modifi cation plans, ultimately converging on the most consolidated fix plan. Localization Chain Selection. From the𝐾×𝑊candidate chains, we form a diverse set by selecting the longest chain plus the (𝑚− 1)most distinct chains computed based on semantic embeddings. Multiple specialized agents then engage in competitive ranking, where each agent independently evaluates and ranks these 𝑚chains based on their analytical perspective. Through this localization level debate, agents must defend their chain preferences against alternatives, revealing structural insights that single-agent selection
* * *
SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference’17, July 2017, Washington, DC, USA would miss. We select the chain with the highest aggregate vote as the optimal localization path, which serves as the foundation for the subsequent modification plan debate. Modification Plan Proposal. Based on the selected localization chain from the competitive debate, we generate a comprehensive modification plan, which systematically specifies the exact code locations requiring changes, the types of modifications needed, and their implementation priorities. The transformation from localization chains to modification pro posals is implemented through a specialized prompt-driven analysis framework. Given a localization chain 𝐶={𝑒1,𝑒2,...,𝑒𝑘}and issue description𝐼, each agent applies a structured analysis prompt that guides the examination of each entity 𝑒𝑖in the chain. The prompt instructs agents to: (1) analyze code structure and functionality, (2) identify specific modification targets, (3) determine modification types (fix_bug, add_feature, refactor, optimize), (4) assess prior ity levels, and (5) provide implementation reasoning. The output is a structured JSON specification that maps each chain entity to concrete modification targets with precise location descriptions, priority rankings, and implementation strategies. We use𝑁such agents to form a pool of 𝑁diverse modification proposals. This multi-agent approach ensures comprehensive cover age of potential modification strategies as each agent contributes its unique analytical viewpoint to form a diverse pool of modification proposals. Competitive Strategy Refinement. Each agent reviews all pro posals from the independent analysis phase and engages in struc tured argumentation to defend their approach while critically evalu ating alternatives. This competitive refinement phase addresses the limitation of independent analysis by forcing agents to explicitly justify their reasoning against competing perspectives, revealing hidden assumptions and identifying potential weaknesses in initial proposals. Agents generate refined modification plans that incor porate insights from cross-agent critique while maintaining their specialized analytical focus, driving deeper understanding of the fault localization. Synthesize modification plan. Based on the refined proposals from the competitive refinement phase, a discriminator agent syn thesizes insights from all refined proposals to produce a coherent, actionable modification plan with prioritized steps and rationale. This final selection phase is essential because competitive refine ment may produce multiple valid but incompatible strategies that require unified resolution for practical implementation. The dis criminator evaluates trade-offs between competing architectural approaches, considers implementation complexity and risk factors, and generates a structured plan that guides downstream repair processes with both strategic direction and tactical specificity. This competitive process addresses the limitations of agents’ limited individual observation and exploration scope by leveraging diverse specialized perspectives and finding architecturally sound solutions through argumentative rigor. The debate produces a struc tured modification plan with strategic insights, and this high-level plan can then be leveraged to guide the subsequent repair process.3.4 Patch Generation Based on the modification plans, the final stage generates patches and fixes the issue by employing a Monte Carlo Tree Search (MCTS) framework [ 1]. The MCTS process allows the agent to systemati cally explore the codebase, refine its modification plans, and itera tively evaluate the impact of each action on the codebase [ 21]. Un like previous approaches that begin with arbitrary exploration [ 1], our MCTS process starts from the structured modification plan generated by the competitive debate framework, enabling focused exploration with pre-specified target locations and strategic rea soning. The MCTS process unfolds as a search through a tree where nodes represent states of the codebase and edges represent actions (Search for code exploration, Plan for strategic reasoning, and Edit for code modification). The initial branches of this search tree are constructed from the steps outlined in our modification plan, ensuring the exploration is grounded in the debate’s strategic insights. The agent then iteratively navigates and expands this tree. At each step, it selects an action based on a modified Upper Confi dence Bound for Trees (UCT) [ 1] criterion that balances exploiting known high-reward paths with exploring less-visited states. After an action is executed, its outcome is assessed by a value function. This function, initially informed by the rationale from our modifi cation plan, provides not only a numerical score but also a written explanation of the decision’s quality. After each edit action, the agent can also execute existing tests and create new ones to better evaluate the current state. This qualitative feedback is then propa gated back up the search tree, refining the agent’s value estimates and guiding future decisions toward a successful resolution. This process continues until the agent reaches a satisfactory resolution of the issue or reaches a predefined exploration depth 𝐷𝑚𝑎𝑥.
* * *
## _4 EXPERIMENTAL SETUP_
4.1 Research Questions
* * *
RQ1: How effective is SWE-Debate in terms of repository-level issue resolution? RQ2: How does each component of SWE-Debate contribute to its overall performance? RQ3: How is the fault localization performance of SWE-Debate compared to other baselines? RQ4: How do the depth of chains in the debate influence the per formance? 4.2 Datasets We evaluate SWE-Debate on the SWE-Bench-Verified dataset [ 26], which contains 500 verified issues from SWE-bench [ 15]. We also evaluated on SWE-bench-Lite [ 15], which contains 300 carefully selected tasks. 4.3 Baselines We compare SWE-Debate with the following baselines on issue resolution: •Agentless [43]: A non-agentic pipeline that breaks down the repair process into different phases of localization, repair, and patch validation.
* * *
Conference’17, July 2017, Washington, DC, USA Li et al. •AutoCodeRover [52]: A software engineering-oriented ap proach that combines LLMs with sophisticated code search capabilities. •SWE-Agent [46]: A custom agent-computer interface enabling LM agents to interact with repository environments through defined actions. •SWE-Search [1]: A repository issue resolution agent that uses Monte Carlo Tree Search (MCTS) to explore the solution space. •SWESynInfer [21]: An open-source LLM series trained with development-process-centric data, simulating repository anal ysis, fault localization, and patch generation via a three-stage Chain-of-Thought workflow. •OpenHands [38]: An open-source platform for building general purpose AI agents that solve software and web tasks through code, terminal, and browser interaction. Additionally, we select the following baselines to compare the performance on fault localization: •CodeActAgent [38]: An agent that interact with environments through executing file system search commands to locate faults. •LocAgent [6]: A graph-guided LLM-agent framework designed to enhance code localization through powerful multi-hop rea soning. •KGComposs [45]: A framwework ridges semantic gaps in repository-level repair by constructing a repository-aware knowl edge graph and leveraging path-guided reasoning to enhance LLM-based patch generation. 4.4 Metrics We employ the following metrics to evaluate the performance of SWE-Debate: •Pass@1: The percentage of issues that are resolved success fully within the first attempt, following the evaluation protocol established by [ 1,46]. This metric directly measures the frame work’s ability to generate correct patches without requiring multiple iterations, representing the most practical scenario for real-world deployment. •Acc@1 (File): The localization accuracy at top-1 predictions at file level, where a localization is considered successful only when all required modification points are included within the top-1 predicted locations [ 6,43]. This metric evaluates the model’s capacity to precisely and comprehensively identify all code regions that require modification, providing a fine grained assessment of fault localization performance prior to the patch generation stage. 4.5 Implementation Details We implement SWE-Debate by extending the SWE-Search [ 1] frame work with our graph-based localization and multi-agent debate components. Due to unsuccessful testbed setup, we did not utilize it in our experiments. The code dependency graph is constructed us ing static analysis tools2, and the multi-agent debate employs offical DeepSeek-V3-0324 [ 8] with different system prompts to simulate diverse reasoning perspectives. For the graph traversal parameters, 2https://github.com/python/cpython/blob/3.13/Lib/ast.pywe set𝐾=5for the number of entry points, 𝑊=4for breadth first expansion width, and 𝐿=5for maximum chain length. The multi-agent debate involves 𝑚=6chains for competitive ranking and𝑁=5specialized agents in the competitive refinement process. These parameters are set based on the a held out set in the full SWE-Bench dataset [ 15]. For baseline methods, we directly use the reported results either from the official leaderboard [ 15] or from the official paper or repository. For experiments on DeepSeek-V3-0324, we reproduce the results on representative baselines with their official repositories.
* * *
## _5 RESULTS_
We present experimental results addressing each research question,
* * *
examining the effectiveness of SWE-Debate across multiple dimen sions and providing detailed analysis of component contributions. 5.1 RQ1: Effectiveness on Issue Resolution
* * *
## _Table 1 presents the main experimental results comparing SWE-_
Debate with state-of-the-art baselines on the SWE-Bench-Verified
* * *
dataset. We observe that SWE-Debate is able to solve 207 out of 500 problems, achieving 41.4% success rate. While individual baseline approaches show varying performance across different language models, SWE-Debate demonstrates consistent superiority over ex isting methods. Specifically, when comparing with methods using the same DeepSeek-V3-0324 model, SWE-Debate achieves signifi cant improvements: 6.0% over SWE-Search, improving from 35.4% to 41.4%, and 2.6% over SWE-Agent, improving from 38.8% to 41.4%.
* * *
## _Table 1: Main effectiveness results on SWE-Bench-Verified._
Method Model Pass@1
* * *
SWE-Agent GPT-4o (2024-05-13) 23.0% Claude-3.5 Sonnet 33.6% DeepSeek-V3-0324 38.8% SWE-Search DeepSeek-V3-0324 35.4% Moatless Tools DeepSeek-V3-0324 34.6% Agentless GPT-4o (2024-05-13) 36.2% DeepSeek-V3-0324 36.6% AutoCodeRover GPT-4o (2024-05-13) 38.4% CodeAct GPT-4o (2024-05-13) 30.0% SWESynInfer Claude-3.5 Sonnet 35.4% GPT-4o (2024-05-13) 31.8% Lingma SWE-GPT 72B 30.2% OpenHands DeepSeek-V3-0324 38.8% SWE-Debate DeepSeek-V3-0324 41.4% It is important to note that SWE-Debate outperforms even the strongest baseline configurations, including OpenHands with DeepSeek V3-0324 and SWE-Agent with DeepSeek-V3-0324, both achieving 38.8%. This demonstrates that our competitive multi-agent de bate framework provides substantial benefits beyond what can be achieved through model selection alone.
* * *
SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference’17, July 2017, Washington, DC, USA Finding 1: SWE-Debate achieves 41.4% Pass@1 on issue reso lution, representing a 2.6 percentage point improvement over the strongest baseline using the same model, demonstrating the effectiveness of competitive multi-agent debate for repository level issue resolution. 5.2 RQ2: Ablation Study To understand the contribution of each component in SWE-Debate, we conduct comprehensive ablation studies by systematically re moving key components and measuring performance degradation on the SWE-Bench-Verified dataset. Table 2 shows the results of this analysis, revealing distinct contributions from different archi tectural elements.
* * *
## _Table 2: Ablation study results showing the contribution of_
different components. Method Pass@1 Δ
* * *
SWE-Debate 41.4% w/o Multiple Chain Generation 31.4% -10.0% w/o Multi-Agent Debate 37.2% -4.2% w/o Edit plan 35.4% -6.0% We observe that removing the multiple chain generation compo nent causes the most significant performance drop, with the method achieving only 31.4% Pass@1, representing a 10.0 percentage point degradation. This suggests that exploring diverse fault propagation paths through graph traversal significantly improves localization accuracy. When this component is removed, the method must rely on single-path exploration, which frequently misses critical depen dency relationships that span multiple files or modules. Similarly, removing the edit plan component results in a 6.0 percentage point performance drop, declining from 41.4% to 35.4%. This demonstrates that the structured modification plans generated through competitive debate are essential for guiding the down stream patch generation process. Without these plans, the MCTS based editing agent lacks strategic direction, leading to suboptimal exploration patterns and reduced fix accuracy. The multi-agent debate component also plays a critical role. Its removal leads to a 4.2 percentage point drop, reducing performance to 37.2%. This highlights the importance of competitive reason ing in resolving modification disambiguation. Without structured debate, the system relies on individual agent exploration, which often gets stuck in local solutions when multiple plausible fix lo cations exist. Our experiments reveal that when the localization chain contains numerous candidate files, the editing process be comes inefficient, with agents spending excessive exploration time without converging on optimal solutions. Finding 2: Multiple chain generation provides the largest contri bution to performance (+10.0%), followed by edit plan generation (+6.0%) and Multi-agent debate (+4.2%), demonstrating that each component addresses distinct limitations in repository-level is sue resolution.5.3 RQ3: Localization Performance Comparison
* * *
## _Table 3: Localization Performance on SWE-Bench-lite._
Method Model Acc@1 (File)
* * *
Agentless GPT-4o (2024-05-13) 67.15 Claude-3.5 Sonnet 72.63 SWE-Agent GPT-4o (2024-05-13) 57.30 Claude-3.5 Sonnet 77.37 DeepSeek-V3-0324 67.00 SWE-Search GPT-4o (2024-05-13) 73.36 Claude-3.5 Sonnet 72.63 CodeActAgent GPT-4o (2024-05-13) 60.95 Claude-3.5 Sonnet 76.28 LocAgent Qwen2.5-7B (FT) 70.80 Qwen2.5-32B (FT) 75.91 Claude-3.5 Sonnet 77.74 KGCompass Claude-3.5 Sonnet 76.67 SWE-Debate DeepSeek-V3-0324 81.67 (+3.93)
* * *
## _Table 3 shows the localization performance comparison across_
different methods on the SWE-Bench-Lite dataset, which we adopt
* * *
to facilitate direct comparison with existing approaches [ 1,6,20,43, 46], referecing the results from LocAgent [ 6]. SWE-Debate achieves 81.67% file-level localization accuracy, significantly outperform ing all baseline methods. When comparing with methods using the same DeepSeek-V3-0324 model, SWE-Debate demonstrates substan tial improvements: 14.67% over SWE-Agent, improving from 67.00% to 81.67%. This represents an 3.93 percentage point improvement over the strongest baseline across all model configurations, surpass ing LocAgent with Claude-3.5 Sonnet which achieves 77.74%. The improvement in localization accuracy is largely attributed to our graph-guided approach for constructing multiple fault prop agation traces. By systematically exploring code dependency re lationships and generating diverse candidate chains, SWE-Debate captures structural patterns that single-pass exploration methods frequently miss. The dramatic improvement over SWE-Agent using the same model—from 67.00% to 81.67%—particularly highlights the effectiveness of our structured reasoning approach compared to tra ditional search-based methods. Compared to baseline approaches that perform localization once, our method aggregates multiple potential paths, significantly increasing the likelihood that at least one trace contains the correct fix location. The results demonstrate that our architectural innovations pro vide benefits that transcend model capabilities. While methods using stronger language models like Claude-3.5 Sonnet generally achieve better localization performance than those using GPT 4o, SWE-Debate with DeepSeek-V3-0324 surpasses even the best Claude-3.5 Sonnet results. More importantly, the consistent supe riority over other methods using the identical DeepSeek-V3-0324 model confirms that performance gains stem from enhanced rea soning frameworks rather than model sophistication alone. Furthermore, the gap between our localization performance and that of specialized localization methods like LocAgent demonstrates
* * *
Conference’17, July 2017, Washington, DC, USA Li et al. substantial improvement, with SWE-Debate achieving 81.67% com pared to LocAgent’s 77.74%. This shows that competitive multi agent debate can enhance even domain-specific approaches. The reason is that our debate process forces agents to systematically evaluate competing localization hypotheses, preventing premature convergence on suboptimal solutions. Finding 3: SWE-Debate achieves 81.67% file-level localization accuracy, surpassing the strongest baseline by 3.93%, demon strating that graph-guided fault propagation traces combined with competitive debate enable more accurate fault localization than individual exploration approaches. 5.4 RQ4: Impact of the Chain Depth. To investigate the optimal configuration for multi-agent reason ing in software fault localization, we study the impact of chain depth on localization performance. To balance computational effi ciency with representative evaluation, we constructed a dataset of 75 instances, termed SWE-Bench-Verified-S, consisting of 50 sam ples from SWE-Bench-verified-mini3and 25 additional instances randomly selected in SWE-Bench-Verified.
* * *
## _Figure 3 shows the impact of varying chain depth in the com-_
petitive debate process on this dataset. We observe that increasing
* * *
the chain depth from 1 to 5 consistently boosts file-level localiza tion accuracy, reaching a peak Acc@1(File) of 86.7%. This suggests that deeper reasoning chains enable more effective exploration of the code graph, capturing complex dependencies and contextual signals that shallow chains often miss. These results highlight the benefits of multi-step reasoning in guiding the model toward more informed and accurate localization decisions. 1 3 5 7 Chain Depth0.650.700.750.800.850.90Acc@1(File) 70.7%72.0%86.7% 82.7%1 3 5 7 Chain Depth0.650.700.750.800.850.90Acc@1(File) 70.7%72.0%86.7% 82.7%
* * *
## _Figure 3: Impact of the Chain Depth._
However, we also find that increasing the chain depth beyond
* * *
5 leads to diminishing returns and even slight performance degra dation. This suggests a trade-off between reasoning depth and relevance. Longer chains are more likely to include information unrelated to resolving the issue, which can distract the model and reduce its ability to judge which chain is most likely to lead to a correct fix. As a result, the debate process becomes less focused, making it harder to converge on accurate localization decisions. 3https://huggingface.co/datasets/MariusHobbhahn/swe-bench-verified-miniFinding 4: A chain depth of 5 achieves the best trade-off be tween reasoning depth and relevance, yielding the highest lo calization accuracy. Further increases introduce distracting in formation that hinders effective decision-making during the debate. 5.5 Case Study To further verify the effectiveness of SWE-debate in actual use, we analyzed a case in SWE-bench. In this case, we investigate an issue in SymPy related to the incorrect evaluation of powers ap plied to TensorProduct expressions. Specifically, expressions like TensorProduct(1,Pauli(3))*TensorProduct(1,Pauli(3)) fail to simplify into TensorProduct(1,1) or1, even though Pauli(3)**2 =1. This failure arises because neither the expand(tensorproduct =True) method nor the tensor_product_simp function are equipped to handle exponentiation of TensorProduct objects. The issue is non-trivial as it requires coordinated reasoning over symbolic power expressions and tensor algebra simplification. Our method first generates localization chains based on graph based reasoning over symbolic dependencies, and then applies a Multi-Agent Debate process to guide the repair. In the first stage, the agent constructs multiple candidate localization chains. Among the generated candidates, Chain 2 demonstrates high accuracy by precisely capturing all necessary modules involved in the failure, such as tensor_product_simp_Mul,tensor_product_simp, and TensorProduct.eval_expand_tensorproduct. This chain provides a comprehensive view of the symbolic rewriting pipeline for tensor expressions, effectively guiding the system to the correct set of files and even narrowing down the specific functions requiring modification. Based on this chain, our debate process formulates a multi stage plan. It proposes a four-step approach: first, modifying the simplification logic in tensor_product_simp ; second, updating tensor_product_simp_Mul to support powers; third, extending eval_expand_tensorproduct for power handling; and fourth, adding an_eval_power method in the TensorProduct class. The edit de rived from this plan introduces a recursive rule that distributes the exponent over the arguments of a TensorProduct, enabling cor rect evaluation of symbolic powers. Notably, all modified functions were covered by the selected localization chain, demonstrating the accuracy and completeness of our code navigation strategy. This case highlights the strength of our method in both locating and resolving issues. The localization chain effectively surfaces the relevant symbolic manipulation points, while the structured planning and debate framework facilitates coordinated edits across multiple modules. Together, they contribute to generating a valid and verifiable patch that resolves the issue as expected.
* * *
## _6 DISCUSSION_
6.1 Strengths
* * *
SWE-Debate demonstrates three key advantages over existing ap proaches. First, the graph-based localization significantly improves fault localization accuracy. By building dependency graphs from code structure and generating multiple fault propagation traces, our method achieves 81.67% file-level accuracy. When comparing with
* * *
SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference’17, July 2017, Washington, DC, USA Issue:Evaluating powers of `TensorProduct`Powers of tensor product expressions are not possible to evaluate with either `expand(tensorproduct=True)` method nor the `tensor_product_simp`function....... ***stage 1***instruction: Modify the tensor_product_simp function to handle Pow expressions... context: physics/quantum/tensorproduct.py:tensor_product_simp(lines 348-393)***stage 2***instruction: Update tensor_product_simp_Mul to recognize and handle...context: physics/quantum/tensorproduct.py:tensor_product_simp_Mul (lines 271-345)***stage 3***instruction: Extend _eval_expand_tensorproduct to handle Pow expressions...context: physics/quantum/tensorproduct.py:TensorProduct._eval_expand_tensorproduct (lines 241-258)***stage 4***instruction: Add a _eval_power method to the TensorProduct class to handle power...context: physics/quantum/tensorproduct.py:TensorProduct class (lines 49-268) ---a/sympy/physics/quantum/tensorproduct.py+++ b/sympy/physics/quantum/tensorproduct.py@@ -142,6 +142,20 @@def _eval_adjoint(self):return TensorProduct(*[Dagger(i) for iin self.args])+ + def _eval_power(self, exponent):+ + return TensorProduct(*[arg**exponent for argin self.args])def _eval_rewrite(self, pattern, rule, **hints):sargs= self.args@@ -382,7 +396,10 @@if isinstance(e, Add):return Add(*[tensor_product_simp(arg) for argin e.args])elifisinstance(e, Pow):-return tensor_product_simp(e.base) ** e.exp+ base = tensor_product_simp(e.base)+ if isinstance(base, TensorProduct):+ return TensorProduct(*[arg**e.expfor argin base.args])+ return base ** e.expelifisinstance(e, Mul):PlanGenerate Localization Chain Chain1：['physics/paulialgebra.py:Pauli’, 'physics/paulialgebra.py:Pauli._eval_power', 'physics/quantum/gate.py:CGate._eval_power', 'physics/quantum/operator.py:HermitianOperator._eval_power', 'simplify/powsimp.py:_denest_pow’]Chain2：['physics/paulialgebra.py:evaluate_pauli_product', 'physics/quantum/tensorproduct.py:TensorProduct._eval_expand_tensorproduct', 'tensor/functions.py:TensorProduct', 'physics/quantum/tensorproduct.py:TensorProduct._eval_rewrite', 'diffgeom/diffgeom.py:TensorProduct.__new__']Chain3：['tensor/tensor.py', 'tensor/tensor.py:TensMul’, 'tensor/tensor.py:TensMul._flatten’, 'tensor/tensor.py:TensMul.__new__', 'physics/quantum/tensorproduct.py:TensorProduct.__new__', 'tensor/functions.py:TensorProduct.__new__’]Chain4：['physics/quantum/tensorproduct.py:TensorProduct', 'physics/quantum/tensorproduct.py:tensor_product_simp_Mul', 'physics/quantum/tensorproduct.py:tensor_product_simp', 'physics/quantum/tensorproduct.py:TensorProduct._eval_expand_tensorproduct ', 'matrices/expressions/matmul.py:MatMul.doit’, ]Chain5：['physics/quantum/tensorproduct.py:TensorProduct', 'physics/quantum/tensorproduct.py:tensor_product_simp_Mul', 'core/mul.py:Mul._eval_power’, 'core/power.py:Pow._eval_expand_power_base', 'core/power.py:Pow._eval_expand_power_base.pred'] Debate Edit locationlocation locationlocation locationIssue:Evaluating powers of `TensorProduct`Powers of tensor product expressions are not possible to evaluate with either `expand(tensorproduct=True)` method nor the `tensor_product_simp`function....... ***stage 1***instruction: Modify the tensor_product_simp function to handle Pow expressions... context: physics/quantum/tensorproduct.py:tensor_product_simp(lines 348-393)***stage 2***instruction: Update tensor_product_simp_Mul to recognize and handle...context: physics/quantum/tensorproduct.py:tensor_product_simp_Mul (lines 271-345)***stage 3***instruction: Extend _eval_expand_tensorproduct to handle Pow expressions...context: physics/quantum/tensorproduct.py:TensorProduct._eval_expand_tensorproduct (lines 241-258)***stage 4***instruction: Add a _eval_power method to the TensorProduct class to handle power...context: physics/quantum/tensorproduct.py:TensorProduct class (lines 49-268) ---a/sympy/physics/quantum/tensorproduct.py+++ b/sympy/physics/quantum/tensorproduct.py@@ -142,6 +142,20 @@def _eval_adjoint(self):return TensorProduct(*[Dagger(i) for iin self.args])+ + def _eval_power(self, exponent):+ + return TensorProduct(*[arg**exponent for argin self.args])def _eval_rewrite(self, pattern, rule, **hints):sargs= self.args@@ -382,7 +396,10 @@if isinstance(e, Add):return Add(*[tensor_product_simp(arg) for argin e.args])elifisinstance(e, Pow):-return tensor_product_simp(e.base) ** e.exp+ base = tensor_product_simp(e.base)+ if isinstance(base, TensorProduct):+ return TensorProduct(*[arg**e.expfor argin base.args])+ return base ** e.expelifisinstance(e, Mul):PlanGenerate Localization Chain Chain1：['physics/paulialgebra.py:Pauli’, 'physics/paulialgebra.py:Pauli._eval_power', 'physics/quantum/gate.py:CGate._eval_power', 'physics/quantum/operator.py:HermitianOperator._eval_power', 'simplify/powsimp.py:_denest_pow’]Chain2：['physics/paulialgebra.py:evaluate_pauli_product', 'physics/quantum/tensorproduct.py:TensorProduct._eval_expand_tensorproduct', 'tensor/functions.py:TensorProduct', 'physics/quantum/tensorproduct.py:TensorProduct._eval_rewrite', 'diffgeom/diffgeom.py:TensorProduct.__new__']Chain3：['tensor/tensor.py', 'tensor/tensor.py:TensMul’, 'tensor/tensor.py:TensMul._flatten’, 'tensor/tensor.py:TensMul.__new__', 'physics/quantum/tensorproduct.py:TensorProduct.__new__', 'tensor/functions.py:TensorProduct.__new__’]Chain4：['physics/quantum/tensorproduct.py:TensorProduct', 'physics/quantum/tensorproduct.py:tensor_product_simp_Mul', 'physics/quantum/tensorproduct.py:tensor_product_simp', 'physics/quantum/tensorproduct.py:TensorProduct._eval_expand_tensorproduct ', 'matrices/expressions/matmul.py:MatMul.doit’, ]Chain5：['physics/quantum/tensorproduct.py:TensorProduct', 'physics/quantum/tensorproduct.py:tensor_product_simp_Mul', 'core/mul.py:Mul._eval_power’, 'core/power.py:Pow._eval_expand_power_base', 'core/power.py:Pow._eval_expand_power_base.pred'] Debate Edit locationlocation locationlocation location
* * *
## _Figure 4: Case study of SWE-Debate with instance sympy-13974._
methods using identical models, SWE-Debate demonstrates substan
* * *
tial improvements: 14.67% over SWE-Agent with DeepSeek-V3-0324 and 8.31% over SWE-Search with GPT-4o, representing an 3.93 percentage point improvement over the strongest baseline across all configurations. The graph traversal strategy effectively captures structural relationships that single-agent individual exploration often misses, particularly for multi-file issues where traditional methods frequently fail due to insufficient architectural understand ing. This superior localization performance directly translates to better issue resolution, with SWE-Debate achieving 41.4% Pass@1 compared to baseline methods ranging from 23.0% to 38.8%. Second, the competitive multi-agent debate resolves modification ambiguity more effectively than individual agent reasoning. Our ablation study shows that removing the debate component causes a 4.2 percentage point drop in resolution rate. The three-round debate process—independent analysis, competitive refinement, and final selection—enables agents with different specializations to sys tematically evaluate competing fix strategies. This structured com petitive approach achieves a 2.6 percentage point improvement over the strongest baseline using the same model, demonstrating that architectural innovations can transcend model capabilities when multiple code locations appear relevant but require different architectural considerations for correct resolution. Third, the framework integrates seamlessly with existing issue resolution systems without requiring major modifications. Our approach can improve the localization modules in frameworks like SWE-Search and Agentless, providing better starting pointsfor downstream repair while maintaining compatibility with their existing architectures. This plug-and-play design enables practical adoption in real software engineering workflows. 6.2 Limitations and Future Work Despite the promising results, SWE-Debate has several limitations that suggest directions for future work. The graph construction process can be computationally expensive for large codebases, lim iting scalability. Future work chould explore more efficient graph construction algorithms and incremental analysis techniques to handle enterprise-scale repositories. Additionally, the current static analysis approach may miss dynamic relationships and runtime behaviors that could improve localization accuracy for certain types of issues. Our multi-agent debate currently relies on a single model with different prompts to simulate diverse reasoning perspectives, which may not fully capture the breadth of real-world developer reasoning styles. While our specialized prompts enforce distinct analytical viewpoints and our ablation study confirms significant performance gains from the debate mechanism, integrating multi ple heterogeneous models or incorporating domain-specific knowl edge bases could further enhance the diversity and quality of the debate process. Future work could explore how different founda tion models with varying reasoning capabilities can be orchestrated within the competitive debate framework to achieve even greater analytical diversity. The current batch processing approach limits integration with real-time development workflows. Future work could investigate lightweight continuous analysis modes and tighter
* * *
Conference’17, July 2017, Washington, DC, USA Li et al. integration with development environments to provide immediate issue resolution assistance during coding.
* * *
## _7 THREATS TO VALIDITY_
Internal. The primary internal threat stems from potential data
* * *
contamination, as the pre-training corpus of DeepSeek-V3-0324 may contain repositories from SWE-Bench. To mitigate this con cern, we emphasize that our evaluation focuses on reasoning pro cesses rather than memorized solutions. Our method generates fault propagation traces through systematic graph traversal and struc tured multi-agent debate, relying on analytical reasoning rather than direct code recall. The substantial improvements over base line methods using identical models provide evidence that perfor mance gains derive from enhanced reasoning capabilities rather than memorization effects. Future work will include evaluation on contamination-free datasets to further validate these findings. A second internal threat arises from experimental scope limita tions imposed by time and budget constraints. Our evaluation is restricted to the open-source DeepSeek-V3-0324 model and a sub set of the SWE-Bench-Verified dataset. While our results demon strate that SWE-Debate outperforms numerous methods across both identical and different foundation models, this constraint lim its our ability to comprehensively validate the generalizability of our competitive debate framework across diverse language model architectures. Future work will expand evaluation to encompass a broader range of foundation models and larger datasets to establish more comprehensive performance benchmarks. External. The main external threat comes from evaluation on a single dataset SWE-Bench-Verified limited to Python repositories, which may not generalize to other programming languages or soft ware domains. To address this, our key components—dependency graph construction, semantic matching, and debate frameworks—are designed to be language-agnostic, focusing on structural reasoning rather than language-specific patterns. And we will evaluate our method on more diverse datasets like Multi-SWE-Bench [ 49] in the future.
* * *
## _8 RELATED WORK_
8.1 Fault Localization
* * *
Traditional fault localization techniques, including spectrum-based fault localization (SBFL) [ 16], mutation-based fault localization (MBFL) [ 28], and learning-based methods [ 17,24,36], mainly relied on test execution data and program analysis to identify buggy code regions. However, these techniques face fundamental limitations when applied to repository-level issue resolution: they require com prehensive test suites to trigger fault patterns [ 25,37], struggle with complex dependency relationships across multiple files, and cannot effectively bridge the semantic gap between natural language issue descriptions and code structures. Recent LLM-based approaches have advanced fault localization through sophisticated code understanding and repository naviga tion capabilities. Methods like RCAgent [ 40] integrate multiple analysis tools for decision support, and AgentFL [ 32] scales fault localization through multi-agent collaboration with static analysistools [ 32,40,42,44]. LocAgent [ 6] leverages graph-based repre sentations to enable multi-hop reasoning across code dependen cies, while OrcaLoca [ 48] improves localization accuracy through priority-based scheduling and distance-aware context pruning. And CoSIL [ 14] reduces search space using module call graphs with iterative context-aware exploration. However, these methods re main fundamentally limited by single-agent reasoning perspectives and struggle with modification disambiguation scenarios where multiple locations match issue descriptions but require different architectural viewpoints for correct evaluation. Our work addresses this limitation by introducing competitive multi-agent debate that systematically evaluates competing localization hypotheses. 8.2 Repository-Level Issue Resolution Automated repository issue resolution has evolved through two main paradigms-agent-based and pipeline-based. Agent-based sys tems model software engineering as sequential decision-making, where language models interact with code environments through structured action spaces. SWE-Agent [ 46] established foundational principles for agent-environment interaction, AutoCodeRover [ 52] focused on search-based localization, SWE-Search [ 1] introduced Monte Carlo Tree Search for systematic exploration, and CodeR [ 3] explored collaborative multi-agent architectures with pre-defined task graphs. Pipeline-based approaches break down issue reso lution into specialized computational workflows. Agentless [ 43] pioneered this paradigm by separating localization, repair, and vali dation into targeted stages, while CodeMonkeys [10] investigated iterative refinement through test-time computation scaling. Recent advances include long-context models with appropriate prompt ing [ 13], training-based approaches for specialized model fine tuning [ 27,29,47], and RepoUnderstander [ 23] which constructs repository knowledge graphs for enhanced whole-repository un derstanding. However, existing methodologies face a fundamental limitation stemming from limited observation scope. They often get stuck in local solutions and fail to resolve ambiguities when multiple code locations appear plausible, as they lack the diverse analytical perspectives needed to systematically evaluate competing mod ification plans [ 1,7,51]. Our approach targets this localization bottleneck by providing more accurate fault localization through competitive multi-agent analysis, enabling seamless integration with current issue resolution systems while improving their overall issue resolution rates. 8.3 LLM Multi-Agent Systems Multi-agent systems have emerged as a promising approach for complex problem-solving by leveraging diverse specialized per spectives and collaborative reasoning. In software engineering contexts, these systems have shown success across various tasks including code generation [ 12,41], automated testing and debug ging [ 11,34]. Current multi-agent architectures mainly use col laborative paradigms that emphasize consensus-building and in formation sharing, with scaling achieved through either cognitive enhancement of individual agents or population scaling through large agent collectives [30, 53].
* * *
SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference’17, July 2017, Washington, DC, USA Multi-agent debate systems represent a particularly relevant approach for decision-making scenarios requiring systematic eval uation of competing alternatives. Existing debate frameworks typ ically follow collaborative models where agents engage in struc tured argumentation to reach consensus through iterative refine ment [ 2,9]. However, these collaborative approaches face crit ical limitations in technical domains: agents often suffer from thought degeneration and resist modification despite potentially incorrect stances. Recent work has attempted to address these is sues through role assignment strategies and agreement modulation techniques [ 19,50], but these approaches still maintain collabora tive consensus-seeking paradigms that may not generate sufficient analytical pressure for complex architectural decision-making. In contrast, our work introduces a competitive debate framework for software fault localization that creates analytical tension by forcing agents to rigorously defend their localization hypotheses against competing proposals. Our structured, multi-round debate, com bined with graph-based dependency analysis, is designed to excel at tasks requiring precise disambiguation and strategic architec tural reasoning, overcoming the limitations of purely collaborative systems.
* * *
## _9 CONCLUSION_
In this paper, we presented SWE-Debate, a competitive multi-agent
* * *
debate framework that addresses the agents’ limited observation scope problem in repository-level issue resolution. Our approach combines graph-based fault propagation trace generation with structured multi-agent debates to systematically evaluate compet ing localization hypotheses, overcoming the perspective limitations of single-agent methods. Experimental evaluation on SWE-Bench Verified shows that SWE-Debate achieves 6.7% improvement in issue resolution rate over state-of-the-art baselines. The framework also demonstrates 5.1% improvement in fault localization accuracy, with potential to enhance the fault localization phases of other automated issue resolution methods. ACKNOWLEDGMENT This research is funded by the National Key Research and Devel opment Program of China (Grant No. 2023YFB4503802) and the Natural Science Foundation of Shanghai (Grant No. 25ZR1401175). REFERENCES [1]Antonis Antoniades, Albert Örwall, Kexun Zhang, Yuxi Xie, Anirudh Goyal, and William Wang. 2024. SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement. arXiv:2410.20285 [2]Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate. In The Twelfth International Conference on Learning Representations. [3]Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang, Anton Cheshkov, Jun Sun, Hao Yu, Guoliang Dong, Artem Aliev, Jie Wang, Xiao Cheng, Guangtai Liang, Yuchi Ma, Pan Bian, Tao Xie, and Qianxiang Wang. 2024. CodeR: Issue Resolving with Multi-Agent and Task Graphs. arXiv:2406.01304 [cs] [4]Yuxiao Chen, Jingzheng Wu, Xiang Ling, Changjiang Li, Zhiqing Rui, Tianyue Luo, and Yanjun Wu. 2024. When Large Language Models Confront Repository Level Automatic Program Repair: How Well They Done? [5]Zhi Chen, Wei Ma, and Lingxiao Jiang. 2025. Unveiling Pitfalls: Un derstanding Why AI-driven Code Agents Fail at GitHub Issue Resolution. arXiv:2503.12374 [cs] [6]Zhaoling Chen, Xiangru Tang, Gangda Deng, Fang Wu, Jialong Wu, Zhiwei Jiang, Viktor Prasanna, Arman Cohan, and Xingyao Wang. 2025. LocAgent:Graph-Guided LLM Agents for Code Localization. arXiv:2503.09089 [cs] [7]Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, and Joseph E. Gonzalez. 2025. The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks. [8] DeepSeek-AI. 2025. DeepSeek-V3 Technical Report. arXiv:2412.19437 [cs] [9]Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mor datch. 2024. Improving Factuality and Reasoning in Language Models through Multiagent Debate. In Proceedings of the 41st International Conference on Machine Learning (ICML’24, Vol. 235). JMLR.org, Vienna, Austria, 11733–11763. [10] Ryan Ehrlich, Bradley Brown, Jordan Juravsky, Ronald Clark, Christopher Ré, and Azalia Mirhoseini. 2025. CodeMonkeys: Scaling Test-Time Compute for Software Engineering. [11] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. 2023. MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. arXiv:2308.00352 [12] Dong Huang, Qingwen Bu, Jie M. Zhang, Michael Luck, and Heming Cui. 2023. AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation. arXiv:2312.13010 [13] Mingjian Jiang, Yangjun Ruan, Luis Lastras, Pavan Kapanipathi, and Tatsunori Hashimoto. 2025. Putting It All into Context: Simplifying Agents with LCLMs. arXiv:2505.08120 [cs] [14] Zhonghao Jiang, Xiaoxue Ren, Meng Yan, Wei Jiang, Yong Li, and Zhongxin Liu. 2025. CoSIL: Software Issue Localization via LLM-Driven Code Repository Graph Searching. arXiv:2503.22424 [cs] [15] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R. Narasimhan. 2024. SWE-bench: Can Language Models Resolve Real-world Github Issues?. In ICLR. [16] James A. Jones and Mary Jean Harrold. 2005. Empirical Evaluation of the Taran tula Automatic Fault-Localization Technique. In Proceedings of the 20th IEEE/ACM International Conference on Automated Software Engineering. ACM, Long Beach CA USA, 273–282. [17] Xia Li, Wei Li, Yuqun Zhang, and Lingming Zhang. 2019. DeepFL: Integrating Multiple Fault Diagnosis Dimensions for Deep Fault Localization. In Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis. ACM, Beijing China, 169–180. [18] Yizhou Liu, Pengfei Gao, Xinchen Wang, Jie Liu, Yexuan Shi, Zhao Zhang, and Chao Peng. 2024. MarsCode Agent: AI-native Automated Bug Fixing. arXiv:2409.00899 [cs] [19] Yuhan Liu, Yuxuan Liu, Xiaoqing Zhang, Xiuying Chen, and Rui Yan. 2025. The Truth Becomes Clearer Through Debate! Multi-Agent Systems with Large Language Models Unmask Fake News. arXiv:2505.08532 [cs] [20] Weijie Lv, Xuan Xia, and Sheng-Jun Huang. 2024. CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs. arXiv:2408.02193 [cs] [21] Yingwei Ma, Rongyu Cao, Yongchang Cao, Yue Zhang, Jue Chen, Yibo Liu, Yuchen Liu, Binhua Li, Fei Huang, and Yongbin Li. 2024. Lingma SWE-GPT: An Open Development-Process-Centric Language Model for Automated Software Improve ment. arXiv:2411.00622 [cs] [22] Yingwei Ma and Yue Liu. 2025. Improving Automated Issue Resolution via Comprehensive Repository Exploration. In ICLR 2025 Third Workshop on Deep Learning for Code. [23] Yingwei Ma, Qingping Yang, Rongyu Cao, Binhua Li, Fei Huang, and Yongbin Li. 2024. How to Understand Whole Software Repository? arXiv:2406.01422 [cs] [24] Xiangxin Meng, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2022. Improving Fault Localization and Program Repair with Deep Semantic Features and Transferred Knowledge. In Proceedings of the 44th International Conference on Software Engineering. ACM, Pittsburgh Pennsylvania, 1169–1180. [25] Niels Mündler, Mark Niklas Mueller, Jingxuan He, and Martin Vechev. 2024. SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. [26] OpenAI. 2024. Introducing SWE-bench Verified. https://openai.com/index/introducing-swe-bench-verified/. [27] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. 2024. Training Software Engineering Agents and Verifiers with SWE-Gym. [28] Mike Papadakis and Yves Le Traon. 2015. Metallaxis-FL: Mutation-based Fault Localization. Software Testing, Verification and Reliability 25, 5-7 (Aug. 2015), 605–628. [29] Minh V. T. Pham, Huy N. Phan, Hoang N. Phan, Cuong Le Chi, Tien N. Nguyen, and Nghi D. Q. Bui. 2025. SWE-Synth: Synthesizing Verifiable Bug-Fix Data to En able Large Language Models in Resolving Real-World Bugs. arXiv:2504.14757 [cs] [30] Chen Qian, Zihao Xie, YiFei Wang, Wei Liu, Kunlun Zhu, Hanchen Xia, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. 2024. Scaling Large Language Model-based Multi-Agent Collaboration. In The Thirteenth International Conference on Learning Representations.
* * *
Conference’17, July 2017, Washington, DC, USA Li et al. [31] Chen Qian, Zihao Xie, YiFei Wang, Wei Liu, Kunlun Zhu, Hanchen Xia, Yu fan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. 2025. Scaling Large Language Model-based Multi-Agent Collaboration. arXiv:2406.07155 [cs] [32] Yihao Qin, Shangwen Wang, Yiling Lou, Jinhao Dong, Kaixin Wang, Xiaoling Li, and Xiaoguang Mao. 2025. AgentFL: Scaling LLM-based Fault Localization to Project-Level Context. arXiv:2403.16362 [cs] [33] Yuchen Shao, Yuheng Huang, Jiawei Shen, Lei Ma, Ting Su, and Chengcheng Wan. 2025. Are LLMs Correctly Integrated into Software Systems?. In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE). IEEE, 1178–1190. [34] Yuling Shi, Songsong Wang, Chengcheng Wan, and Xiaodong Gu. 2024. From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging. arXiv:2410.01215 [cs] [35] Yuling Shi, Hongyu Zhang, Chengcheng Wan, and Xiaodong Gu. 2024. Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Program mers. In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE). IEEE Computer Society, 51–62. [36] Jeongju Sohn and Shin Yoo. 2017. FLUCCS: Using Code and Change Metrics to Improve Fault Localization. In Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis. ACM, Santa Barbara CA USA, 273–283. [37] Xinchen Wang, Pengfei Gao, Xiangxin Meng, Chao Peng, Ruida Hu, Yun Lin, and Cuiyun Gao. 2024. AEGIS: An Agent-based Framework for General Bug Reproduction from Issue Descriptions. arXiv:2411.18015 [cs] [38] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. 2024. OpenHands: An Open Platform for AI Software Developers as Generalist Agents. arXiv:2407.16741 [39] You Wang, Michael Pradel, and Zhongxin Liu. 2025. Are "Solved Issues" in SWE-bench Really Solved Correctly? An Empirical Study. [40] Zefan Wang, Zichuan Liu, Yingying Zhang, Aoxiao Zhong, Jihong Wang, Fengbin Yin, Lunting Fan, Lingfei Wu, and Qingsong Wen. 2024. RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (CIKM ’24). Association for Computing Machinery, New York, NY, USA, 4966–4974. [41] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W. White, Doug Burger, and Chi Wang. 2023. AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation. arXiv:2308.08155 [cs] [42] Yonghao Wu, Zheng Li, Jie M. Zhang, Mike Papadakis, Mark Harman, and Yong Liu. 2023. Large Language Models in Fault Localisation. arXiv:2308.15276 [cs] [43] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. 2024. Agent less: Demystifying LLM-based Software Engineering Agents. arXiv:2407.01489 [44] Aidan Z. H. Yang, Claire Le Goues, Ruben Martins, and Vincent Hellendoorn. 2024. Large Language Models for Test-Free Fault Localization. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering (ICSE ’24). Association for Computing Machinery, New York, NY, USA, 1–12. [45] Boyang Yang, Haoye Tian, Jiadong Ren, Shunfu Jin, Yang Liu, Feng Liu, and Bach Le. 2025. Enhancing Repository-Level Software Repair via Repository-Aware Knowledge Graphs. arXiv preprint arXiv:2503.21710 (2025). [46] John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik R. Narasimhan, and Ofir Press. 2024. SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. [47] John Yang, Kilian Leret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. 2025. SWE-smith: Scaling Data for Software Engineering Agents. [48] Zhongming Yu, Hejia Zhang, Yujie Zhao, Hanxian Huang, Matrix Yao, Ke Ding, and Jishen Zhao. 2025. OrcaLoca: An LLM Agent Framework for Software Issue Localization. arXiv:2502.00350 [cs] [49] Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, Siyao Liu, Yongsheng Xiao, Liangqiang Chen, Yuyu Zhang, Jing Su, Tianyu Liu, Rui Long, Kai Shen, and Liang Xiang. 2025. Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving. arXiv:2504.02605 [cs] [50] Guibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang, Tianlong Chen, and Dawei Cheng. 2025. G-Designer: Archi tecting Multi-agent Communication Topologies via Graph Neural Networks. In ICLR 2025 Workshop on Foundation Models in the Wild. [51] Linghao Zhang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Chengxing Xie, Junhao Wang, Maoquan Wang, Yufan Huang, Shengyu Fu, Elsie Nallipogu, Qingwei Lin, Yingnong Dang, Saravan Rajmohan, and Dongmei Zhang. 2025. SWE-bench Goes Live! arXiv:2505.23419 [cs] [52] Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. 2024. Au toCodeRover: Autonomous Program Improvement. In Proceedings of the 33rdACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2024). Association for Computing Machinery, New York, NY, USA, 1592–1604. [53] Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jürgen Schmidhuber. 2024. Gptswarm: Language Agents as Optimizable Graphs. In Forty-First International Conference on Machine Learning. A SWE-BENCH-VERIFIED-S SWE-Bench-verified-mini4is a subset of SWE-Bench-Verified, con taining 50 instead of 500 datapoints, requiring 5GB instead of 130GB of storage, while maintaining a similar distribution of performance, test pass rates, and task difficulty as the original dataset. Building on SWE-Bench-verified-mini, we augment it with 25 additional instances to better approximate the distribution and performance characteristics of the full dataset, resulting in our constructed bench mark, SWE-Bench-Verified-S.
* * *
## _Table 4: Instance Id in SWE-Bench-Verified-S_
django__django-11790 django__django-11815
* * *
django__django-11848 django__django-11880 django__django-11885 django__django-11951 django__django-11964 django__django-11999 django__django-12039 django__django-12050 django__django-12143 django__django-12155 django__django-12193 django__django-12209 django__django-12262 django__django-12273 django__django-12276 django__django-12304 django__django-12308 django__django-12325 django__django-12406 django__django-12708 django__django-12713 django__django-12774 django__django-9296 sympy__sympy-13852 sympy__sympy-12481 sympy__sympy-17318 sympy__sympy-16766 sympy__sympy-15976 sympy__sympy-13974 sympy__sympy-13798 sympy__sympy-13647 sympy__sympy-20916 sympy__sympy-12489 sympy__sympy-24562 sympy__sympy-23824 sympy__sympy-23950 sympy__sympy-24661 sympy__sympy-16792 sympy__sympy-18189 sympy__sympy-12096 sympy__sympy-24539 sympy__sympy-13757 sympy__sympy-19495 sympy__sympy-18698 sympy__sympy-19346 sympy__sympy-17139 sympy__sympy-15809 sympy__sympy-22456 sphinx-doc__sphinx-10323 sphinx-doc__sphinx-10435 sphinx-doc__sphinx-10466 sphinx-doc__sphinx-10673 sphinx-doc__sphinx-11510 sphinx-doc__sphinx-7590 sphinx-doc__sphinx-7748 sphinx-doc__sphinx-7757 sphinx-doc__sphinx-7985 sphinx-doc__sphinx-8035 sphinx-doc__sphinx-8056 sphinx-doc__sphinx-8265 sphinx-doc__sphinx-8269 sphinx-doc__sphinx-8475 sphinx-doc__sphinx-8548 sphinx-doc__sphinx-8551 sphinx-doc__sphinx-8638 sphinx-doc__sphinx-8721 sphinx-doc__sphinx-9229 sphinx-doc__sphinx-9230 sphinx-doc__sphinx-9281 sphinx-doc__sphinx-9320 sphinx-doc__sphinx-9367 sphinx-doc__sphinx-9461 sphinx-doc__sphinx-9698 4https://huggingface.co/datasets/MariusHobbhahn/swe-bench-verified-mini
* * *
SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference’17, July 2017, Washington, DC, USA
* * *
## _Table 5: MCTS Hyperparameters_
Hyperparameter Description Default
* * *
`{ issue_description }`
Main Search Parameters c_param UCT exploration parameter 1.41 max_expansions Max children per node 2 max_iterations Max MCTS iterations 20 provide_feedback Enable feedback True best_first Use best-first strategy True value_function_temperature Value function temperature 0.2 max_depth Max tree depth 20 UCT Score Calculation Parameters exploration_weight UCT exploration weight 1.0 depth_weight Depth penalty weight 0.8 depth_bonus_factor Depth bonus factor 200.0 high_value_threshold High-value node threshold 55.0 low_value_threshold Low-value node threshold 50.0 very_high_value_threshold Very high-value threshold 75.0 high_value_leaf_bonus_constant High-value leaf bonus 20.0 high_value_bad_children_bonus_constant High-value bad children bonus 20.0 high_value_child_penalty_constant High-value child penalty 5.0 Action Model Parameters action_model_temperature Action model temperature 0.7 Discriminator Parameters number_of_agents Number of Discriminator Agents 5 number_of_round Number of debate rounds 3 discriminator_temperature Discriminator temperature 1 B HYPERPARAMETERS OF MCTS The Monte Carlo Tree Search (MCTS) algorithm used in this study employs several hyperparameters as following [15]: C PROMPT TEMPLATES In the following section, we enumerate all the prompts used through out our entire workflow, from the initial entity extraction to the final plan generation. Prompt 1: INITIAL ENTITY EXTRACTION PROMPT You are a code analysis expert. Given an issue description, your task is to identify the most relevant code entities ( classes, methods, functions, variables) that are likely involved in the issue. Important : Only extract entities that are explicitly mentioned or strongly implied by the issue description. Do not invent names that are not referenced in the text. ** Issue Description :** ** Instructions :** 1. Analyze the issue description to identify :** Classes **: e.g., `UserAuthenticator `,` PaymentProcessor `** Methods / Functions **: e.g., ` validate_credentials () `,`process_payment ()`** Variables / Parameters **: e.g., `user_id `,` transaction_amount `** Error Types / Exceptions **: e.g., ` RateLimitExceededError `,` DatabaseConnectionError `2. ** Focus on direct mentions **: Only include entities that are clearly referenced in the issue. 3. ** Avoid redundancy **: If multiple terms refer to the same entity (e.g., " the payment handler " and `PaymentProcessor `), pick the most precise name. 4. ** Prioritize key components **: Rank entities by how central they are to the issue. 5. ** Return only names **: Do not include paths, modules, or extra descriptions. 6. ** Limit to { max_entities } entities **: Select only the { max_entities } most relevant and important entities for this issue. ** Output Format :** Return a JSON list of exactly { max_entities } entity names in order of relevance ( most relevant first): [" entity_name1 ", " entity_name2 ", " entity_name3 ",...] ** Examples :** 1. ** Issue Description :** Query syntax error with condition and distinct combination Description : A Count annotation containing both a Case condition and a distinct = True param produces a query error on Django 2.2 ( whatever the db backend). A space is missing at least (... COUNT ( DISTINCTCASE WHEN...). ** Output (if max_entities =3) :** [" Count ", " DISTINCTCASE ", " distinct "] 2. ** Issue Description :**
* * *
> " After upgrading to v2.0, the `UserSession `
class sometimes fails to store session data in Redis, causing login loops." ** Output (if max_entities =2) :** [" UserSession ", " Redis "] 3. ** Issue Description :**
* * *
> " The `calculate_discount () `function applies
incorrect discounts for bulk orders when ` customer_type = 'wholesale '`." ** Output (if max_entities =3) :** [" calculate_discount ", " customer_type ", " wholesale "] Note : Return only the simple names like " __iter__
* * *
> ", " page_range ", " MyClass ", " my_function ",
etc. Do not include file paths or full qualified names. Return exactly { max_entities } entities, prioritizing the most important ones if there are more candidates.
* * *
`{ problem_statement }`
`{ code_snippets }`
Conference’17, July 2017, Washington, DC, USA Li et al. Prompt 2: CODE SNIPPET ENTITY EXTRACTION PROMPT Based on the following code snippets and problem statement, identify the 4 most relevant entities (files, classes, or functions) that are likely involved in solving this issue. ** Problem Statement :** ** Code Snippets :** ** Instructions :** 1. Analyze the problem statement to understand what needs to be fixed / implemented 2. Review the code snippets to identify relevant entities 3. ** PRIORITIZE DIVERSITY **: Select entities from different files whenever possible to ensure comprehensive coverage 4. ** BALANCE RELEVANCE AND DIVERSITY **: Choose entities that are both highly relevant to the issue AND come from different modules / files 5. Avoid selecting multiple entities from the same file unless absolutely necessary 6. Select exactly 4 entities that collectively provide the best coverage for solving the issue 7. For each entity, provide the exact entity ID in the format expected by the codebase ** Selection Strategy :**First priority : High relevance to the problem + Different file locationsSecond priority : High relevance to the problem ( even if some files overlap)Ensure the selected entities represent different aspects or layers of the solution ** Output Format :** Return a JSON list containing exactly 4 entities, each with the following format : ```json [ {{
* * *
> " entity_id ": " file_path : QualifiedName or
just file_path ",
* * *
> " entity_type ": " file | class | function ",
> " relevance_reason ": " Brief explanation of
why this entity is relevant to the issue ",
* * *
> " diversity_value ": " How this entity adds
diversity (e.g., 'different file ',' different layer ','different functionality ')" }} ] ``` ** Example :** ```json [ {{
* * *
> " entity_id ": " src / models.py: UserModel ",
> " entity_type ": " class "," relevance_reason ": " Contains userrelated
functionality mentioned in the issue
* * *
> ",
> " diversity_value ": " Model layer from
different file " }}, {{
* * *
> " entity_id ": " src / views.py: UserView ",
> " entity_type ": " class ",
> " relevance_reason ": " Handles user
interface logic that may need modification ",
* * *
> " diversity_value ": " View layer from
different file " }}, {{
* * *
> " entity_id ": " src / utils / validators.py:
validate_user_input ",
* * *
> " entity_type ": " function ",
> " relevance_reason ": " Input validation
logic relevant to the user issue ",
* * *
> " diversity_value ": " Utility function from
different module " }}, {{
* * *
> " entity_id ": " src / config.py",
> " entity_type ": " file ",
> " relevance_reason ": " Configuration
settings that may affect user behavior ",
* * *
> " diversity_value ": " Configuration file
`{ issue_description }`
`{ neighbor_list }`
from different location " }} ] ``` ** Remember **: Maximize both relevance to the issue AND diversity across different files / modules to ensure comprehensive localization chain generation. Prompt 3: NEIGHBOR PREFILTERING PROMPT You are a code analysis expert helping to select the most relevant and diverse neighbors for exploring a dependency graph to solve a specific issue. ** Issue Description :** ** Current Entity :** { current_entity } ** Current Entity Type :** { current_entity_type } ** Traversal Depth :** { depth } ** Available Neighbor Entities ({ total_count } total) :** ** Your Task :** From the { total_count } available neighbors, select up to { max_selection } most relevant and diverse entities that would be most promising to explore next.
* * *
SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference’17, July 2017, Washington, DC, USA ** Selection Criteria :** 1. ** Relevance to Issue **: How likely is this neighbor to contain code related to solving the issue? 2. ** Diversity **: Avoid selecting too many entities from the same file or with similar names 3. ** Strategic Value **: Prioritize entities that could lead to discovering the root cause or solution 4. ** Entity Type Variety **: Balance between files, classes, and functions when possible ** Instructions :** 1. Analyze each neighbor entity ID to understand what it likely represents 2. Consider file paths, entity names, and types to assess relevance 3. Ensure diversity by avoiding redundant selections from the same file / module 4. Select entities that complement each other in exploring different aspects of the issue 5. Return exactly the entity IDs that should be explored further (up to { max_selection }) ** Output Format :** Return a JSON object with your selection : ```json {{
* * *
> " selected_neighbors ": [
> " neighbor_entity_id_1 ",
> " neighbor_entity_id_2 ",
... ],
* * *
> " selection_reasoning ": " Brief explanation of
your selection strategy and why these neighbors were chosen ",
* * *
> " diversity_considerations ": " How you ensured
`{ issue_description }`
`{ neighbor_info }`
diversity in your selection " }} ``` Focus on strategic exploration that maximizes the chance of finding issuerelevant code while maintaining diversity. Prompt 4: NODE SELECTION PROMPT You are a code analysis expert helping to navigate a dependency graph to solve a specific issue. Given the current context and available neighboring nodes, determine which node would be most promising to explore next. ** Issue Description :** ** Current Entity :** { current_entity } ** Current Entity Type :** { current_entity_type } ** Traversal Depth :** { depth } ** Available Neighbor Nodes :** ** Context :**- We are performing graph traversal to find code locations relevant to solving this issueEach neighbor represents a related code entity ( file, class, or function)We need to select the most promising node to continue exploration ** Instructions :** 1. Analyze how each neighbor might relate to solving the issue 2. Consider the traversal depth and whether we should continue or stop 3. Evaluate which neighbor is most likely to contain relevant code for the solution 4. Return your decision on whether to continue exploration and which neighbor to select ** Output Format :** Return a JSON object with your decision : ```json {{
* * *
> " should_continue ": true /false,
> " selected_neighbor ": " neighbor_entity_id or
null ",
* * *
> " reasoning ": " Explanation of your decision ",
> " confidence ": 0 -100
`{ issue_description }`
`{ chains_info }`
}} ``` If should_continue is false, set selected_neighbor to null. If should_continue is true, select the most promising neighbor_entity_id. Prompt 5: CHAIN VOTING PROMPT You are an expert software engineer tasked with identifying the optimal modification location for solving a specific software issue. ** Issue Description :** ** Available Localization Chains :** ** Your Task :** Analyze each localization chain as a potential modification target and vote for the ONE chain where making changes would most likely resolve the issue described above. ** Evaluation Criteria :** 1. ** Problem Location Accuracy **: Does this chain contain the actual location where the bug / issue manifests? 2. ** Modification Impact **: How directly would changes to this code path affect the described problem? 3. ** Code Modifiability **: Is the code in this chain wellstructured and safe to modify? 4. ** Solution Completeness **: Would fixing this chain likely resolve the entire issue, not just symptoms?
* * *
Conference’17, July 2017, Washington, DC, USA Li et al. 5. ** Risk Assessment **: What are the risks of modifying this particular code path? ** Key Questions to Consider :**Which chain contains the root cause rather than just related functionality?Where would a developer most likely need to make changes to fix this specific issue?Which code path, when modified, would have the most direct impact on resolving the problem?Which chain provides the clearest entry point for implementing a fix? ** Instructions :** 1. For each chain, analyze whether modifying its code would directly address the issue 2. Consider the logical flow : which chain is most likely to contain the problematic code? 3. Evaluate implementation feasibility : which chain would be safest and most effective to modify? 4. Vote for exactly ONE chain that represents the best modification target 5. Focus on where to make changes, not just what 's related to the issue ** Output Format :** Return a JSON object with your vote : ```json {{
* * *
> " voted_chain_id ": " chain_X ",
> " confidence ": 85,
> " reasoning ": " Detailed explanation of why this
chain is the best modification target for solving the issue ",
* * *
> " modification_strategy ": " Brief description of
what type of changes would be needed in this chain ",
* * *
> " chain_analysis ": {{
> " chain_1 ": " Assessment of this chain as a
modification target ",
* * *
> " chain_2 ": " Assessment of this chain as a
modification target ",... }} }} ``` ** Example :** ```json {{
* * *
> " voted_chain_id ": " chain_2 ",
> " confidence ": 88,
> " reasoning ": " Chain 2 contains the pagination
iterator __iter__ method which is where the infinite loop issue described in the problem statement actually occurs. Modifying the logic in this method to properly handle the iteration termination would directly solve the reported bug.",
* * *
> " modification_strategy ": " Add proper boundary
checking and iteration termination logic in the __iter__ method ",
* * *
> " chain_analysis ": {{
> " chain_1 ": " Contains utility functions but
modifications here would not address the core iteration logic issue "," chain_2 ": " Contains the actual iterator implementation where the bug manifestsideal modification target
* * *
> ",
> " chain_3 ": " Related display logic but
`{ issue_description }`
`{ chain_info }`
changes here would not fix the underlying iteration problem " }} }} Prompt 6: ROUND 1 MODIFICATION LOCATION PROMPT You are an expert software engineer tasked with identifying specific code locations that need to be modified to solve a given issue. ** Issue Description :** ** Selected Localization Chain :** ** Your Task :** Analyze the localization chain and identify the specific locations within this chain that need to be modified to solve the issue. Focus on pinpointing the exact functions, methods, or code blocks that require changes. ** CRITICAL REQUIREMENT FOR INSTRUCTIONS :**Each suggested_approach must be a DETAILED, STEP -BYSTEP instructionInclude specific code examples, parameter names, and implementation detailsSpecify exact lines to modify, functions to add, and variables to changeProvide concrete implementation guidance that a developer can directly followInclude error handling, edge cases, and validation requirementsMention specific imports, dependencies, or setup needed ** Instructions :** 1. Examine each entity in the localization chain and its code 2. Identify which specific parts of the code are causing the issue or need enhancement 3. Determine the precise locations where modifications should be made 4. Explain why each location needs modification and what type of change is required 5. Prioritize the modifications by importance ( most critical first) 6. For each modification, provide DETAILED implementation instructions with specific code examples ** Output Format :** Return a JSON object with your analysis : ```json {{
* * *
> " modification_locations ": [
{{
* * *
SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference’17, July 2017, Washington, DC, USA
* * *
> " entity_id ": " specific_entity_id ",
> " location_description ": " Specific
function / method / lines that need modification ",
* * *
> " modification_type ": " fix_bug |
add_feature | refactor | optimize ",
* * *
> " priority ": " high | medium | low ",
> " reasoning ": " Detailed explanation of
why this location needs modification ",
* * *
> " suggested_approach ": " DETAILED step -
bystep implementation instructions with specific code examples, parameter names, exact function signatures, error handling, and complete implementation guidance that can be directly executed by a developer " }} ],
* * *
> " overall_strategy ": " Overall approach to
solving the issue using these modifications ",
* * *
> " confidence ": 85
}} ``` ** Example of DETAILED suggested_approach :** Instead of: " Add proper termination condition " Provide : " Modify the __iter__ method in the Paginator class by adding a counter variable 'current_page = 1 'at the beginning. Then add a while loop condition 'while current_page <= self. num_pages : 'to replace the infinite loop. Inside the loop, yield 'self. page ( current_page) 'and increment 'current_page += 1'. Add trycatch block to handle PageNotAnInteger and EmptyPage exceptions by catching them and breaking the loop. Import the exceptions 'from django. core. paginator import PageNotAnInteger, EmptyPage 'at the top of the file." Prompt 7: ROUND 2 COMPREHENSIVE MODIFICA TION PROMPT
* * *
> """
`{ issue_description }`
`{ chain_info }`
`{ your_initial_analysis }`
`{ other_agents_analyses }** Your Task :**`
You are an expert software engineer participating in a collaborative code review process to determine the best approach for solving a software issue. ** Issue Description :** ** Selected Localization Chain :** ** Your Initial Analysis :** ** Other Agents 'Analyses :** Based on the issue, the localization chain, your initial analysis, and insights from other agents, provide a refined and comprehensive analysis of where and how the code should be modified. ** CRITICAL REQUIREMENT FOR REFINED INSTRUCTIONS :**Each suggested_approach must be EXTREMELY DETAILED with complete implementation guidanceInclude specific code snippets, exact function signatures, and parameter detailsProvide line -byline modification instructions where applicableSpecify all necessary imports, dependencies, and setup requirementsInclude comprehensive error handling and edge case considerationsMention testing requirements and validation stepsProvide specific examples of input / output or before / after code states ** Instructions :** 1. Review your initial analysis and the analyses from other agents 2. Identify common patterns and disagreements in the proposed modifications 3. Synthesize the best insights from all analyses 4. Refine your modification recommendations based on collective wisdom 5. Provide a more comprehensive and wellreasoned final recommendation 6. Ensure each suggested_approach contains exhaustive implementation details ** Output Format :** Return a JSON object with your refined analysis : ```json {{
* * *
> " refined_modification_locations ": [
{{
* * *
> " entity_id ": " specific_entity_id ",
> " location_description ": " Specific
function / method / lines that need modification ",
* * *
> " modification_type ": " fix_bug |
add_feature | refactor | optimize ",
* * *
> " priority ": " high | medium | low ",
> " reasoning ": " Enhanced reasoning
incorporating insights from other agents ",
* * *
> " suggested_approach ": " EXHAUSTIVE step
-bystep implementation guide including : exact code snippets to add / modify / remove, complete function signatures, all required imports, parameter validation, error handling, edge cases, testing considerations, and specific examples of before / after states ",
* * *
> " supporting_evidence ": " References to
other agents 'insights that support this decision " }}
* * *
Conference’17, July 2017, Washington, DC, USA Li et al. ],
* * *
> " overall_strategy ": " Comprehensive strategy
refined through collaborative analysis ",
* * *
> " confidence ": 90,
> " key_insights_learned ": " What you learned from
other agents 'analyses ",
* * *
> " potential_risks ": " Potential risks or
`{ issue_description }`
`{ chain_info }`
`{ all_agents_analyses }`
challenges identified through collaborative review " }} ``` Remember : Each suggested_approach should be so detailed that a developer can implement it without additional research or clarification. Prompt 8: FINAL DISCRIMINATOR PROMPT You are the lead software architect making the final decision on a code modification plan. Multiple expert engineers have provided their analyses for solving a software issue. ** Issue Description :** ** Selected Localization Chain :** ** All Agents 'Final Analyses :** ** Your Task :** Synthesize all the expert analyses and create a definitive, actionable modification plan that will solve the issue effectively and safely. ** CRITICAL REQUIREMENTS FOR INSTRUCTIONS :**Every instruction MUST be a concrete modification action (Add, Remove, Modify, Replace, Insert, etc.)NO verification, checking, or validation instructions ( avoid " Verify ", " Ensure ", " Check ", " Maintain ", etc.)Each instruction should specify exactly WHAT to change and HOW to change itFocus on direct code modifications that implement the solution ** Instructions :** 1. Analyze all the expert recommendations and identify the most reliable and consistent suggestions 2. Resolve any conflicts between different expert opinions using technical merit 3. Create a prioritized, step -bystep modification plan with ONLY concrete modification actions 4. Ensure the plan is practical, safe, and addresses the root cause of the issue 5. Include specific instructions for each modification 6. The output context should be as detailed as possible7. Use action verbs like : " Add ", " Modify ", " Replace ", " Insert ", " Update ", " Change ", " Remove ", " Implement " ** Output Format :** Return a comprehensive modification plan : ```json {{
* * *
> " final_plan ": {{
> " summary ": "Highlevel summary of the
modification approach ",
* * *
> " modifications ": [
{{
* * *
> " step ": 1,
> " instruction ": " Concrete
modification instruction using action verbs ( Add / Modify / Replace / etc.)",
* * *
> " context ": " File path and specific
location (e.g., function, method, line range)",
* * *
> " type ": " fix_bug | add_feature |
refactor | optimize ",
* * *
> " priority ": " critical | high | medium |
low ",
* * *
> " rationale ": " Why this
modification is necessary and how it contributes to solving the issue ",
* * *
> " implementation_notes ": " Specific
technical details for implementation " }} ],
* * *
> " execution_order ": " The recommended order
for implementing these modifications
* * *
> ",
> " testing_recommendations ": " Suggested
testing approach for validating the modifications ",
* * *
> " risk_assessment ": " Potential risks and
mitigation strategies " }},
* * *
> " confidence ": 95,
> " expert_consensus ": " Summary of areas where
experts agreed ",
* * *
> " resolved_conflicts ": " How conflicting expert
opinions were resolved " }} ``` ** Examples of GOOD instructions :**" Add maxlength attribute to the widget configuration "" Modify the widget_attrs method to include max_length parameter "" Replace the current field initialization with max_length support "" Insert validation logic for maximum length " ** Examples of BAD instructions (DO NOT USE) :**" Verify the max_length setting "" Ensure proper validation "" Check if the field is configured correctly "" Maintain the existing functionality "
* * *
SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference’17, July 2017, Washington, DC, USA Focus on creating a plan that can be directly executed by a modification agent with clear, actionable steps.
* * *
