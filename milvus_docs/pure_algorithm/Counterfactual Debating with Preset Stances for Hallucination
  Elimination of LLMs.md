## Counterfactual Debating with Preset Stances for Hallucination
`{wenjiewang96,fulifeng93}@gmail.com,linhui@whu.edu.cn`
Elimination of LLMs Yi Fang1, Moxin Li2, Wenjie Wang2*, Hui Lin3, Fuli Feng1* 1University of Science and Technology of China,2National University of Singapore, 3Electronic Science Research Institute of China Electronics peterfang@mail.ustc.edu.cn,limoxin@u.nus.edu,
* * *
## _Abstract_
Large Language Models (LLMs) excel in vari
* * *
ous natural language processing tasks but strug gle with hallucination issues. Existing solu tions have considered utilizing LLMs’ inher ent reasoning abilities to alleviate hallucina tion, such as self-correction and diverse sam pling methods. However, these methods of ten overtrust LLMs’ initial answers due to in herent biases. The key to alleviating this is sue lies in overriding LLMs’ inherent biases for answer inspection. To this end, we pro pose a Counter Factual Multi-Agent Debate (CFMAD) framework. CFMAD presets the stances of LLMs to override their inherent bi ases by compelling LLMs to generate justifi cations for a predetermined answer’s correct ness. The LLMs with different predetermined stances are engaged with a skeptical critic for counterfactual debate on the rationality of gen erated justifications. Finally, the debate process is evaluated by a third-party judge to determine the final answer. Extensive experiments on four datasets of three tasks demonstrate the superi ority of CFMAD over existing methods.
* * *
## _1 Introduction_
Large Language Models, especially closed-source
* * *
ones such as GPT-4 (Achiam et al., 2023) and Gem ini (Team et al., 2023), have demonstrated state of-the-art performance across various natural lan guage processing tasks (Bubeck et al., 2023; Zhao et al., 2023). However, LLMs still struggle with the hallucination problem, i.e.,occasionally generating unfaithful content (Zhang et al., 2023; Bang et al., 2023; Zheng et al., 2023). Due to the black-box na ture of closed-source LLMs, it is difficult for users to directly intervene in or optimize their internal mechanisms to address the hallucination problems. Currently, extensive research is investigating how to use LLMs’ inherent reasoning abilities to al *Corresponding author. Counterfactual DebatingDiverse Sampling Question: Crystals can heal all illnesses. Is this claim true or false? It is true because …The previous answer is right! It is true because … The claim is true because… I think is true because…No need for comparison or deliberation!The claim is true. It is false because…The claim is false. Assistant and critic try to persuade each other Overconfidence!The claim is true. Overconfidence! Correct! Self-correction All answers are same! It is true because … Why the claim is true? Why the claim is false? Initial Reply Self-reflection Judgement Sampling Replies Contrast/Deliberate Judgement Counterfactual generationCounterfactual DebateJudgement You are wrong! You are wrong! I am right!I am right!Counterfactual DebatingDiverse Sampling Question: Crystals can heal all illnesses. Is this claim true or false? It is true because …The previous answer is right! It is true because … The claim is true because… I think is true because…No need for comparison or deliberation!The claim is true. It is false because…The claim is false. Assistant and critic try to persuade each other Overconfidence!The claim is true. Overconfidence! Correct! Self-correction All answers are same! It is true because … Why the claim is true? Why the claim is false? Initial Reply Self-reflection Judgement Sampling Replies Contrast/Deliberate Judgement Counterfactual generationCounterfactual DebateJudgement You are wrong! You are wrong! I am right!I am right!Figure 1: Comparison of CFMAD with self-correction and diverse sampling methods. CFMAD presets stances for LLMs to override their inherent biases. leviate hallucinations without model intervention (Shinn et al., 2024; Liang et al., 2023). Related work of using LLMs’ own abilities for hallucination elimination can be categorized into self-correction and diverse sampling methods, which imitate human deep reasoning and broad rea soning to enhance LLMs’ reasoning capabilities, respectively (Zhang et al., 2024b). Self-correction methods (Shinn et al., 2024; Madaan et al., 2024) guide LLMs to reflect on and refine their previ ous answers iteratively. Diverse sampling methods (Zhang et al., 2024b; Du et al., 2023; Wang et al., 2023; Mielke et al., 2022; Xiong et al., 2023) first sample multiple initial answers, and then compare or deliberate on the differences among these an swers to reach a consistent answer. While self-correction and diverse sampling meth ods show potential for improving the output re liability of LLMs, they still have the overconfi dence issue (Mielke et al., 2022; Xiong et al., 2023)arXiv:2406.11514v2 [cs.CL] 15 Jan 2025
* * *
## as illustrated in Figure 1. Self-correction meth-
ods may overtrust LLMs’ initially generated an swers, making it difficult to effectively recognize errors (Huang et al., 2024b; Stechly et al., 2023; Valmeekam et al., 2023). By contrast, diverse sam pling methods may repeatedly generate the same incorrect answers due to LLMs’ inherent biases and beliefs (Wang et al., 2024b), limiting LLMs to contrast and deliberate on other possible answers. We believe that a key reason for the above overcon fidence issue is that these methods do not intervene in the LLMs’ answer-generation process, allowing LLMs to refine or sample diverse answers accord ing to their own biases and beliefs. The main challenge in addressing the overcon fidence issue is to override LLMs’ inherent biases and beliefs, compelling them to inspect answers they would not normally consider. To achieve this, we consider presetting different stances for LLMs, allowing LLMs to imagine each answer as correct in each round of reasoning, and then generate the reasons why the answer is valid. By overriding the LLM’s original beliefs with this new mindset, we can regulate LLMs to assess the possibility of each answer being correct. Thereafter, we can eliminate the incorrect answers by reflecting the generated reasons for all answers. To this end, we propose a Counter Factual Multi Agent Debate (CFMAD) framework comprising two key stages: abduction generation and counter factual debate. In the abduction generation stage, LLMs are tasked with producing potential cor rect reasons for a predetermined answer. Sub sequently, in the counterfactual debate stage, a structured debate method is employed to assess these abductions and ascertain the sole correct re sponse. Specifically, we introduce a critic who questions the validity of each generated abduction, and prompt the LLM to defend its position in a debate with the critic. The deliberation is then presented to an impartial third-party judge for fi nal adjudication. Extensive experiments spanning fact-checking, reading comprehension, and com monsense reasoning tasks validate the effective ness of CFMAD over existing benchmarks across four datasets. We release our code and data at https://github.com/Peter-Fy/CFMAD/. The contributions of this work are threefold: •We propose to preset various stances for LLMs, overriding their inherent biases and beliefs to address the overconfidence issue of LLMs.•We propose a CFMAD framework, which in structs LLMs to generate abduction with preset stances and then conduct counterfactual debate to eliminate incorrect answers. •We conduct extensive experiments on three gen erative tasks with four datasets, validating the effectiveness of CFMAD.
* * *
## _2 Preliminary Experiments_
We formulate methods for self-correction and di
* * *
verse sampling, and subsequently conduct a quan titative experiment to expose the overconfidence issue prevalent in both approaches. 2.1 Problem Definition Self-correction. Self-correction methods involve two steps: reflection and refinement (Shinn et al., 2024). Given a question qandR0=LLM (q) representing the initial response of an LLM, self correction methods further instruct the LLM to reflect on the initial response R0and generate feed back by F=LLM (q, R 0). Given R0andF, the LLM then generates a revised answer in the refine ment stage, denoted as R1=LLM (q, R 0, F). Diverse Sampling. Diverse Sampling methods usually involve three steps: sampling, deliberation, and judging (Zhang et al., 2024b). First, Ninitial responses are sampled by: Ri 0=LLM (q, θi), i∈ [1, N]. Here θirepresents settings such as improv ing temperature or using different prompts, which are widely used in diverse sampling to enhance the diversity of responses. In the following delibera tion stage, each response is refined by contrasting with other responses, thereby improving the initial responses: Ri 1=LLM (q, Ri 0,{Rj ̸=i 0}). Finally, a judging process is employed to determine the final answer Rf=judge (R1 1, R2 1,..., RN 1). However, the LLM with self-correction or di verse sampling face the issue of overconfidence. Formally, the LLM with self-correction tends to overtrust the initial response R0, resulting in R1 having the same error as R0(Zhang et al., 2024b). Meanwhile, for diverse sampling, the incorrect an swer might repeat in {R1 0, R2 0,..., RN 0}, resulting in the deliberation and judging stages potentially accepting such an incorrect answer. 2.2 Investigation of Overconfidence To investigate the overconfidence issue, we conduct some preliminary experiments on the representative self-correction and diverse sampling methods.
* * *
## Testing Methods. We evaluate four representa-
tive methods and count the number of testing sam ples exhibiting the overconfidence issue as follows: •Self-reflection (Shinn et al., 2024): This method instructs the LLMs to reflect on an initial answer and subsequently provide feedback, asking the LLM to refine and generate a revised response based on this feedback. If the revised answer for a testing sample remains the same as the initial incorrect response, we treat it as a sample with the overconfidence issue. •Self-consistency (Wang et al., 2023): This ap proach samples multiple initial answers using the same prompts, followed by a voting process to determine the final answer. We implement it by sampling seven initial answers and consider a test sample as an overconfidence sample if six out of the seven responses are identically incorrect. •Self-contrast (Zhang et al., 2024b): In this method, three initial answers are generated by the LLMs using self-generated, varying prompts. These answers are then contrasted to derive the final answer. If all three initial responses are the same incorrect answers for a given testing sam ple, it is regarded as an overconfidence sample. •MAD (Du et al., 2023): This strategy involves sampling multiple initial answers from different agents and using a debate process to decide on the final answer. Similarly, an overconfidence sample is defined as that three initial responses are the same and incorrect. Results. We assess the overconfidence issue by applying these methods to a representative LLM, GPT-3.5-turbo (Ouyang et al., 2022), on the Com monsenseQA (Talmor et al., 2019) and Hover (Jiang et al., 2020) datasets. We calculate the pro portion of incorrect answers attributable to over confidence among all incorrect cases. As shown in Figure 2, for self-reflection, MAD, and Self Contrast, more than half of the errors are caused by overconfidence. For Self-consistency, although the overconfidence issue is alleviated due to the increase in sample number and temperature, ap proximately 40% of the errors are still caused by the overconfidence of LLMs. This validates the severity of the overconfidence issue in existing self correction and diverse sampling methods. A key reason for the overconfidence issue of LLMs might be that self-correction and diverse ComenseseQA Hover0.00.20.40.60.8Proportion of Overconfidence0.8350.886 0.5440.765 0.648 0.644 0.3970.523 0.5Self-Reflection MAD Self-Contrast Self-ConsistencyComenseseQA Hover0.00.20.40.60.8Proportion of Overconfidence0.8350.886 0.5440.765 0.648 0.644 0.3970.523 0.5Self-Reflection MAD Self-Contrast Self-ConsistencyFigure 2: Proportion of the overconfident answers among all incorrect answers. sampling methods do not intervene in the LLM’s answer-generation process, permitting LLMs to refine and sample diverse answers based on their inherent biases and beliefs. Consequently, LLMs tend to trust the initial incorrect answer, hindering the consideration of alternative potential answers.
* * *
## _3 Counterfactual Multi-agent Debate_
To address the overconfidence issue, the key lies in
* * *
overriding the inherent biases and beliefs of LLMs for answer generation. To achieve this, we con sider initially configuring the LLMs with various stances, allowing them to hypothesize the correct ness of each possible answer and uncover the un derlying rationale of each answer. This approach compels LLMs to inspect all potential answers, re ducing their reliance on inherent biases and beliefs. Subsequently, we can critically assess the potential rationales to identify the correct answer. To this end, we propose a CFMAD framework comprising two sequential stages: abduction gen eration and counterfactual debate, depicted in Fig ure 3. In the abduction generation phase, we initial ize multiple LLM agents and configure each one to adopt a predetermined stance, assuming a speci fied answer is correct. Subsequently, these agents are instructed to generate abductions, i.e.,potential correct reasons for the given answer. In the coun terfactual debate phase, we create an adversarial debate scenario. Each abducting agent, adopting a predetermined answer as correct, faces a criti cal evaluator tasked with challenging the validity of the abductions generated by the agent. Mean while, the abducting agent is directed to defend its position on the abduction correctness. Eventually,
* * *
Question: Where does one store fabric in their own home? A. sewing room B. clothing store C. tailor shop D. clothes store E. cotton mill Preset positions 1: Why the answer is option A? Your answer should look like this: The answer is A because….Preset positions N: Why the answer is option E? Your answer should look like this: The answer is C because…. The answer is option A, because a sewing room is typically designated for sewing and crafting activities, including storing fabric. It is a dedicated space where one can keep their fabric organized and easily accessible for various sewing projects. Additionally, …... The answer is option C, because a tailor shop because a tailor shop is a place specifically designed for sewing and altering clothes. It is common for tailors to have a wide variety of fabrics in stock to …… Counterfactual Debate Preset positions for critic: The agent's answer may be wrong. Please persuade the agent that the answer is incorrect. The agent's answer may indeed be incorrect. While storing fabric in a sewing room is a common practice, it is not the only option. Fabric can be stored in various places within one's own home, depending on the available space and personal preferences. For example, some people may choose to store fabric …… The agent's reasoning is based on the assumption that …. While it is true that …, it is not the only place where fabric can be stored in a home. Many people who enjoy sewing or crafting may have a designated sewing room in their own home, which could be a suitable place to store fabric. Additionally, …… Preset positions for Agent: Please refute the critic’s answer and persuade the critic that your answer is correct. I understand the critic's perspective and acknowledge that …. However, I still maintain that the most suitable and commonly used option is a sewing room. While it is true that not everyone has a dedicated sewing room, …. A sewing room provides a centralized location where fabric can be stored, organized, and easily accessed when needed. Furthermore …… I apologize for any confusion caused. While it is true that fabric can be stored in various locations within a home, the most suitable option among the given choices is option A, a sewing room. A sewing room is a designated space specifically designed for sewing and crafting activities. It typically …… Based on the evidence provided and the arguments presented, the answer is option A.…… ……Abduction Generation After seeing the debate process above, do you think which option is the most appropriate answer for the question? Question: Where does one store fabric in their own home? A. sewing room B. clothing store C. tailor shop D. clothes store E. cotton mill Preset positions 1: Why the answer is option A? Your answer should look like this: The answer is A because….Preset positions N: Why the answer is option E? Your answer should look like this: The answer is C because…. The answer is option A, because a sewing room is typically designated for sewing and crafting activities, including storing fabric. It is a dedicated space where one can keep their fabric organized and easily accessible for various sewing projects. Additionally, …... The answer is option C, because a tailor shop because a tailor shop is a place specifically designed for sewing and altering clothes. It is common for tailors to have a wide variety of fabrics in stock to …… Counterfactual Debate Preset positions for critic: The agent's answer may be wrong. Please persuade the agent that the answer is incorrect. The agent's answer may indeed be incorrect. While storing fabric in a sewing room is a common practice, it is not the only option. Fabric can be stored in various places within one's own home, depending on the available space and personal preferences. For example, some people may choose to store fabric …… The agent's reasoning is based on the assumption that …. While it is true that …, it is not the only place where fabric can be stored in a home. Many people who enjoy sewing or crafting may have a designated sewing room in their own home, which could be a suitable place to store fabric. Additionally, …… Preset positions for Agent: Please refute the critic’s answer and persuade the critic that your answer is correct. I understand the critic's perspective and acknowledge that …. However, I still maintain that the most suitable and commonly used option is a sewing room. While it is true that not everyone has a dedicated sewing room, …. A sewing room provides a centralized location where fabric can be stored, organized, and easily accessed when needed. Furthermore …… I apologize for any confusion caused. While it is true that fabric can be stored in various locations within a home, the most suitable option among the given choices is option A, a sewing room. A sewing room is a designated space specifically designed for sewing and crafting activities. It typically …… Based on the evidence provided and the arguments presented, the answer is option A.…… ……Abduction Generation After seeing the debate process above, do you think which option is the most appropriate answer for the question?
* * *
## _Figure 3: Illustration of CFMAD framework with two stages. In the abduction generation stage, we initialize_
multiple LLM agents, each configured to assume a specific answer is correct and to generate supporting abductions. In the subsequent counterfactual debate stage, each agent is challenged by a critical evaluator for debating. The
* * *
debating processes are assessed by a third-party judge for final adjudication. the deliberations between each agent-critic pair are presented to a third-party judge to deliver the final adjudication. 3.1 Abduction Generation Prior studies have illustrated that LLMs exhibit pro ficiency in counterfactual reasoning (Nguyen et al., 2024; Bhattacharjee et al., 2024), thereby allowing them to engage in reasoning with predetermined stances. Specifically, given a possible answer ai, we preset the LLMs’ stance with aiby the follow ing prompt:Why is aithe correct answer? Your answer should look like this: The answer is aibecause... Even if aiis incorrect, the LLM can still follow our instructions to perform counterfactual reasoning to generate plausible justifications. Drawing from this insight, CFMAD assigns the LLM agents the task of generating abductions for each potential answer. Concretely, as depicted in
* * *
## _Figure 3, when presented with a set of possible_
answers {a1, a2,..., a M}, we activate multiple ab
* * *
ducting agents. Each agent is tasked with assuming that a specific answer aiis correct and then gener ating the corresponding abduction ri.
* * *
## 3.2 Counterfactual Debate
Among the generated abductions {r1, r2,..., r M}, only one is factual, while the remainder are incor rect justifications. Hence, we introduce a counter factual debate mechanism to discern the correct answer from the pool of abductions. Specifically, for each abducting agent gi, who is preset with the position that aiis correct, we introduce a critic evaluator to challenge the correctness of ai. By showing the agent’s abduction rito critic ciand instructing the critic with a prompt like: The agent’s answer may be wrong. Please per suade the agent that the answer is incorrect. Simultaneously, we preset the stance of gi, ensur ing it firmly believes in the correctness of its an swer and addresses challenges from the critic. For instance, we provide giwith a prompt such as: Please refute the critic’s answer and persuade the critic that your answer is correct. With the aforementioned configuration, we orches trate an adversarial debate scenario for each agent critic pair. The abduction rifor an incorrect answer aiin evitably incorporates numerous fabricated reason ing processes and factually incorrect elements. The adversarial debate process will help to unveil the errors or unreasonable justifications in ri. After multi-round debating, we present the de bate process of all agent-critic pairs to a third-party judge, implemented as another LLM. This enables the judge to meticulously analyze and juxtapose the varied debate trajectories, thereby discerning the fi nal answer. An example prompt for the third-party judge can be found in the Appendix D.
* * *
## _4 Experiments_
In this section, we conduct extensive experiments
* * *
on the widely studied fact-checking, reading com prehension, and commonsense reasoning tasks. 4.1 Experimental Setup Datasets. We conduct experiments on four datasets: Hover (Jiang et al., 2020), BoolQ (Clark et al., 2019), CosmosQA (Huang et al., 2019), and CommonsenseQA (Talmor et al., 2019). Hover and BoolQ are binary prediction tasks with only true or false answers. CosmosQA and CommenseQA are multi-choice tasks with 4 and 5 options, re spectively. Note that we split Hover into two sub sets named Hover 3-hop and Hover 4-hop withquestions requiring 3 and 4 steps of reasoning, re spectively. Comparison between Hover 3-hop and Hover 4-hop might reveal the influence of prob lem difficulty on method effectiveness since more complex questions typically require more reason ing hops. More details about these datasets can be found in the Appendix B.1. Baselines. As introduced in Sections 2.2, we compare CFMAD with the four baselines: Chain-of-thought (CoT) prompting (Wei et al., 2022), Self-Reflection (Shinn et al., 2024), Self Consistency (Wang et al., 2023), Self-Contrast (Zhang et al., 2024b), MAD (Du et al., 2023). Implementation Details. The implementation detail involves three key factors: backbone, prompt, and inference temperature. For all compared meth ods, we use GPT-3.5-turbo-06131as our backbone LLM and present their prompts in Appendix C and D. As to the inference temperature, we set it to 0.2 in most methods for the sake of fair comparison. The only exception is Self-Consistency, where we follow the original paper and set the temperature to 1 since the method requires high diversity of samples (Wang et al., 2023). Evaluation Metrics. For binary prediction datasets, i.e.,Hover and BoolQ, we follow the pre vious work (Wang and Shu, 2023) and adopt the macro-F1 score as the evaluation metric. As to multi-choice datasets, i.e.,CosmosQA and Com menseQA, we report accuracy following previous work (Wang and Zhao, 2023). 4.2 Performance Comparison
* * *
## _Table 1 shows the performance of the compared_
methods on all datasets. From the table, we have
* * *
the following observations: •In all cases, CFMAD outperforms all baselines, showing stronger reasoning capabilities. Such performance gain indicates the effectiveness of the abduction generation and counterfactual de bate mechanism. •Among all self-correction and diverse sampling methods, Self-Reflection performs the worst in all cases, sometimes even worse than CoT. Given that Self-Reflection encounters the most severe overconfidence issue (as shown in Figure 2), we postulate that such inferior performance is due to overconfidence. 1https://chatgpt.com/.
* * *
## Method Hover 3-hop Hover 4-hop BoolQ CosmosQA CommenseQA
CoT 0.6108 0.5886 0.7767 0.7833 0.7467 Self-Reflection 0.5986 0.5813 0.7728 0.7867 0.7567 Self-Consistency 0.6342 0.6044 0.8033 0.8067 0.7733 MAD 0.6476 0.6069 0.8020 0.7933 0.7700 Self-Contrast 0.6359 0.6178 0.8267 0.8133 0.7633 CFMAD (Ours) 0.6757 0.6361 0.8366 0.8267 0.7933
* * *
## _Table 1: Overall performance comparison on all experiment datasets. Bold font and underline indicate the best and_
second-best performance, respectively. 36.7%
* * *
20.2%Hover 3-hop 34.6% 23.5%Hover 4-hopValid Invalid36.7% 20.2%Hover 3-hop 34.6% 23.5%Hover 4-hopValid Invalid
* * *
## _Figure 4: Proportion of changes in initial stances._
> “Valid” means the stances changed from incorrect to
correct. “Invalid” represents the stances changed from
* * *
correct to incorrect. •While Self-Consistency exhibits lower levels of overconfidence than Self-Contrast by providing answers within a wider scope, it does not consis tently outperform Self-Contrast across all tasks. This suggests that incorporating diverse perspec tives alone does not guarantee superior reasoning outcomes; the effective utilization of these varied viewpoints is crucial for optimal performance. 4.3 In-depth Analysis We proceed to analyze the performance enhance ment of CFMAD. We posit that the efficacy of CFMAD stems from two key factors: 1) Agents instructed to generate abductions for incorrect an swers are more likely to waver and change their stance during the debate process due to the contra dictions with factual information. 2) Engaging in counterfactual debates aids judges in distinctly dis cerning between accurate and inaccurate answers. Subsequently, we undertake experimental investi gations to delve into these aspects. 4.3.1 Counterfactual Answers are More Prone to Change As to stance change, we first analyze whether the agents would change their stance even when in structed to maintain their original position. For simplicity, we conduct our analysis using the Hover dataset with binary answers. Specifically, we first 28.9%14.2%Hover 3-hop 26.9%15.0%Hover 4-hopValid Invalid28.9%14.2%Hover 3-hop 26.9%15.0%Hover 4-hopValid InvalidFigure 5: The final judgment on inconsistent stances.
* * *
> “Valid” means that the judge makes a correct judgment
while “Invalid” denotes making an incorrect judgment. ask two agents to generate abductions for both
* * *
> “True” and “False” answers, respectively. Given
that there are only two possible answers, one of these abductions is necessarily factual while the other is counterfactual. Given these abductions, we conduct a single round of counterfactual debate. For both agents with factual and counterfactual abductions, we instruct the critic to persuade the agent that their claim is actually incorrect. After that, we present the critic’s argument to the corre sponding agents and instruct them to maintain their original stance by pointing out the errors in the critic’s answer and reiterating your point. Finally, we observe whether these factual and counterfac tual agents would change their stance. The results on Hover 3-hop and Hover 4-hop are shown in Figure 4. From Figure 4, we find that over 50% of factual and counterfactual agents reached a consensus after one round of counterfactual debate. It means that a significant number of agents were persuaded by the critic, while we instruct these agents to maintain their original stance. Specifi cally, more than 34% of the stance changes came from counterfactual agents, which is 10% higher than the changes from factual agents. We believe this is because counterfactual answers inherently contradict the facts, making it easier for the critic to point out issues and for the agents to realize the problems and subsequently change their stance.
* * *
## Method Hover 3-hop CosmosQA CommenseQA
CFMAD 0.6815 0.8267 0.7933 Direct Judge 0.6027 0.7633 0.7500 Repl. w/ SR 0.6063 0.7800 0.7600 Repl. w/ MAD 0.6224 0.6767 0.7200
* * *
## _Table 2: Ablation studies on the effectiveness of our_
counterfactual debate component. 4.3.2 Counterfactual Debates Contain
* * *
Additional Clues We first analyze the contribution of the counterfac tual debate by continuing the previous experiment. For those agents that do not reach a consensus, we present the entire debate process between the critic and the factual and counterfactual agents to a third party judge. The judge then makes a final decision on which stance is more factual. As shown in Fig ure 5, the number of correct judgments was twice that of incorrect judgments, indicating that even if a consensus is not ultimately reached, leveraging the judge to evaluate the counterfactual debate process can still significantly improve the accuracy of the final decision. To further investigate the effectiveness of the counterfactual debate, we evaluate several variants of CFMAD, including: •Direct Judge: Removing the counterfactual de bate and directly presenting the generated abduc tions to the judge for final decision. •Replace with Self-Reflection: Replacing the counterfactual debate with self-reflection, where the LLM reflects on each generated abduction. Both the original answer and the reflection pro cess were shown to the judge for final decision. •Replace with MAD: Replacing the counterfac tual debate with three rounds of MAD (Du et al., 2023), then presenting the MAD debate process to the judge for the final decision. We conduct the ablation experiments on three datasets. For CosmosQA and CommenseQA, we used the same 300 data as in Table 1. For Hover 3-hop, we randomly sampled 300 data points due to cost limitations. The results are shown in Ta ble 2. We can see that our proposed counterfactual debate component outperforms the other control group across all tasks. This demonstrates that the counterfactual debate component helps the judge more effectively determine the correct final answer. 2 3 4 5 Answers Numbers0.50.60.70.80.91.0Accuracy (A) Accuracy V.S. Answers Numbers CosmosQA CommsenseQA 1 2 3 4 5 Debate Rounds0.50.60.70.80.91.0 (B) Accuracy V.S. Debate Rounds CosmosQA CommsenseQA2 3 4 5 Answers Numbers0.50.60.70.80.91.0Accuracy (A) Accuracy V.S. Answers Numbers CosmosQA CommsenseQA 1 2 3 4 5 Debate Rounds0.50.60.70.80.91.0 (B) Accuracy V.S. Debate Rounds CosmosQA CommsenseQAFigure 6: Comparison of different numbers of (A) initial counterfactual answers and (B) debate rounds. 4.4 Impact of Hyperparameters We then investigate the influence of hyperparam eters on the effectiveness of CFMAD, including the number of initial counterfactual answers and debate rounds. Number of Initial Counterfactual Answers. Considering that datasets like CosmosQA and CommenseQA have multiple potential answers, we explore the influence of initial counterfactual an swers by increasing the number of sampled stances. Note that directly sampling a few stances from many options may fail to include the correct an swer when the initial number of stances is small (e.g., 2 out of 5 choices). We thus need to con duct the comparison under the condition that the correct answer is included. To this end, we use a CoT prompt to generate three answers and select the most frequently occurring answer as the most potential stance. We then randomly sample the remaining stances to complete the initial settings. Considering the expensive time and monetary costs, we randomly sampled 100 data from each dataset. The final result is shown in Figure 6(A), where the accuracy of the final judgment decreases as the number of initial counterfactual responses in creases. We believe this is because the presence of too many incorrect stances can confuse the LLMs. Notably, only two initial counterfactual answers are needed to achieve good results, which also saves time and cost. Number of Debate Rounds. We also test the im pact of conducting multiple rounds of counterfac tual debate. As shown in Figure 6(B), the accuracy decreases with the increase of debate rounds. We speculate that through multiple rounds of debate, LLM-based agents and critics may veer away from our predetermined stances to adhere to the biases in the LLM itself, thereby influencing the efficacy of the debate. As such, we conduct only one-round debate between the agent and critic by default.
* * *
## 5 Related Work
Prompting LLM for Better Reasoning. Re searchers have made significant progress in im proving the reasoning abilities of LLMs through designing better prompting methods. These meth ods often enhance the LLM’s reasoning capabil ities in either reasoning depth or breadth. CoT prompting (Wei et al., 2023) guides the model to generate intermediate reasoning steps before arriv ing at a final answer, thus improving the reason ing depth. Self-correction methods (Madaan et al., 2024; Shinn et al., 2024; Paul et al., 2023; Xi et al., 2024) are also typical examples of enhancing LLM reasoning depth. They leverage the LLM’s self correction ability, generating feedback by LLM it self to iteratively refine its answers, thereby enhanc ing its accuracy and reliability. Breadth reasoning approaches, on the other hand, involve sampling diverse responses with temperature larger than 0 (Wang et al., 2023; Yoran et al., 2023) or guid ing the LLM to generate responses from different perspectives (Huang et al., 2024a; Zhang et al., 2024b), gathering more diverse insights for the answer. This helps to derive the correct answer from the collection of a wider range of potential responses to improve the overall reliability. Multi-agent Debate. Recent research has ex plored how to engage multiple agents of the same model or different models in debates to jointly im prove decision-making and reasoning processes (Du et al., 2023; Liang et al., 2023; Wang et al., 2024b), which can be divided into two modes: col laborative and adversarial. In the collaborative mode, each agent provides its own answer to the same question and then refines its answer with ref erence to the responses of other agents (Du et al., 2023). This mode may encounter overconfidence issues that the initial responses of most agents ar rive at the same incorrect answer. In the adversarial mode, for a given answer, two agents are initial ized: one believing the answer is correct, and the other believing the answer is incorrect, and they are instructed to debate and challenge each other’s response to reach a more precise conclusion (Liang et al., 2023; Wang et al., 2024b). The difference between our counterfactual debate and the adver sarial debate lies in that they first have the LLM generate a single answer and then conduct a debate about that answer, while we first have the LLM explore multiple answers as thoroughly as possible, and then conduct debates for each of these answers.Additionally, existing work also leverages multi ple side rationales in LLM reasoning (Jung et al., 2022; Liu et al., 2023; Balepur et al., 2023) which is similar to our abduction, yet not all of them has shown promising results. We incorporate them in the counterfactual debate process and achieve en hanced reasoning. Confidence Calibration. Recently, confidence calibration for LLMs has gained significant atten tion (Lin et al., 2022; Kuhn et al., 2023; Huang et al., 2023; Tian et al., 2023). The goal of con fidence calibration is to obtain LLM’s confidence score on its own answer which aligns with the ac tual answer accuracy. However, some studies found that LLMs sometimes generate confidence scores that are poorly calibrated and often assign high confidence scores to incorrect answers (Shrivastava et al., 2023; Yang et al., 2024; Xiong et al., 2024). Some methods attempt to calibrate the confidence for LLMs through estimating response consistency across multiple perspectives (Zhang et al., 2024a; Wang et al., 2024a), and various prompting strate gies for LLM to self-estimate the confidence (Tian et al., 2023; Kadavath et al., 2022; Li et al., 2024), where some work also leverages explanation and rationales (Li et al., 2024; Feng et al., 2024). How ever, these works mainly aim at improving calibra tion errors or identifying incorrect answers instead of directly improving the answer accuracy.
* * *
## _6 Conclusion_
In this paper, we addressed the overconfidence is
* * *
sue presented in existing self-correction and di verse sampling methods for hallucination elimina tion in LLM reasoning. We revealed the overcon fidence issues of these two methods through ex periments, and pointed out that the overconfidence issue mainly stems from the LLM’s inherent biases towards overly favoring a particular answer while lacking sufficient exploration of other potential an swers. To address this, we proposed the CFMAD framework, which first presets the stance for the LLM, encouraging it to explore as many answers as possible, and then uses counterfactual debate to expose and correct the errors in the incorrect answers. Empirical results validate the superiority of CFMAD over baselines in mitigating hallucina tions. In this work, we mainly test CFMAD on binary and multiple-choice questions. In the future, we intend to extend CFMAD to more scenarios with open-ended questions.
* * *
Limitations Our work has the following limitations: First, we require the LLM to generate reasons for each pos sible answer and conduct debates for each answer, which results in additional computational overhead. Secondly, since it is necessary to preset the stance for the LLM, we must identify potential answers. We address this by initially using CoT prompts sam pling to generate three possible answers. However, it is worth exploring superior methods to improve the recall rate of correct answers. Ethics Statement Our ethical concerns include the following points. First, although we can mitigate LLM hallucinations using CFMAD, the LLM may still produce some inaccurate answers, which could potentially cause harm. Secondly, our experiments are conducted exclusively on English datasets, meaning the appli cability of our findings to other languages has not been comprehensively evaluated. Acknowledgments We thank the reviewers for their constructive feed back.
* * *
## _References_
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
* * *
Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Nishant Balepur, Shramay Palta, and Rachel Rudinger. 2023. It’s not easy being wrong: Evaluating process of elimination reasoning in large language models. arXiv preprint arXiv:2311.07532. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multi task, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023. Amrita Bhattacharjee, Raha Moraffah, Joshua Gar land, and Huan Liu. 2024. Zero-shot llm-guided counterfactual generation for text. arXiv preprint arXiv:2405.04793. Sébastien Bubeck, Varun Chandrasekaran, Ronen El dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund berg, et al. 2023. Sparks of artificial general intelli gence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenen baum, and Igor Mordatch. 2023. Improving factual ity and reasoning in language models through multia gent debate. arXiv preprint arXiv:2305.14325. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia Tsvetkov. 2024. Don’t hallucinate, abstain: Identifying llm knowl edge gaps via multi-llm collaboration. arXiv preprint arXiv:2402.00367. Jie He, Tao Wang, Deyi Xiong, and Qun Liu. 2020. The box is in the pen: Evaluating commonsense reasoning in neural machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3662–3672. Baizhou Huang, Shuai Lu, Weizhu Chen, Xiaojun Wan, and Nan Duan. 2024a. Enhancing large language models in coding through multi-perspective self consistency. Preprint, arXiv:2309.17272. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xiny ing Song, and Denny Zhou. 2024b. Large language models cannot self-correct reasoning yet. Preprint, arXiv:2310.01798. Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Cosmos qa: Machine reading com prehension with contextual commonsense reasoning. arXiv preprint arXiv:1909.00277. Yuheng Huang, Jiayang Song, Zhijie Wang, Huaming Chen, and Lei Ma. 2023. Look before you leap: An exploratory study of uncertainty measurement for large language models. Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020. Hover: A dataset for many-hop fact extraction and claim verification. In Findings of the Association for Computational Linguistics: EMNLP 2020. Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah man, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. 2022. Maieutic prompting: Logically consistent reasoning with recursive explanations. In
* * *
## Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing, pages 1266–1279. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for un certainty estimation in natural language generation. Moxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, and Tat-Seng Chua. 2024. Think twice before assure: Confidence estimation for large language models through reflection on multiple answers. arXiv preprint arXiv:2403.09972. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334. Ziyi Liu, Isabelle Lee, Yongkang Du, Soumya Sanyal, and Jieyu Zhao. 2023. Score: A framework for self contradictory reasoning evaluation. arXiv preprint arXiv:2311.09603. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2024. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Pro cessing Systems, 36. Sabrina J Mielke, Arthur Szlam, Emily Dinan, and Y Lan Boureau. 2022. Reducing conversational agents’ overconfidence through linguistic calibration. Trans actions of the Association for Computational Linguis tics, 10:857–872. Van Bach Nguyen, Paul Youssef, Jörg Schlötterer, and Christin Seifert. 2024. Llms for generating and evalu ating counterfactuals: A comprehensive study. arXiv preprint arXiv:2405.00722. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730–27744. Curran Associates, Inc.Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beat riz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2023. Refiner: Reasoning feedback on in termediate representations. Subhro Roy and Dan Roth. 2016. Solving gen eral arithmetic word problems. arXiv preprint arXiv:1608.01413. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Re flexion: Language agents with verbal reinforcement learning. Advances in Neural Information Process ing Systems, 36. Vaishnavi Shrivastava, Percy Liang, and Ananya Ku mar. 2023. Llamas know what gpts don’t show: Sur rogate models for confidence estimation. Preprint, arXiv:2311.08877. Kaya Stechly, Matthew Marquez, and Subbarao Kamb hampati. 2023. Gpt-4 doesn’t know it’s wrong: An analysis of iterative prompting for reasoning prob lems. arXiv preprint arXiv:2310.12397. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question answering challenge targeting commonsense knowl edge. Preprint, arXiv:1811.00937. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D. Manning. 2023. Just ask for cali bration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. Preprint, arXiv:2305.14975. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open founda tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. 2023. Can large language models really improve by self-critiquing their own plans? arXiv preprint arXiv:2310.08118. Haoran Wang and Kai Shu. 2023. Explainable claim verification via knowledge-grounded reason ing with large language models. arXiv preprint arXiv:2310.05253.
* * *
Pei Wang, Yejie Wang, Muxi Diao, Keqing He, Guant ing Dong, and Weiran Xu. 2024a. Multi-perspective consistency enhances confidence estimation in large language models. arXiv preprint arXiv:2402.11279. Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, and Yangqiu Song. 2024b. Rethinking the bounds of llm reasoning: Are multi-agent discussions the key? arXiv preprint arXiv:2402.18272. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. Preprint, arXiv:2203.11171. Yuqing Wang and Yun Zhao. 2023. Gemini in reason ing: Unveiling commonsense in multimodal large language models. Preprint, arXiv:2312.17661. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elic its reasoning in large language models. Preprint, arXiv:2201.11903. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits rea soning in large language models. Advances in neural information processing systems, 35:24824–24837. Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Tao Gui, Qi Zhang, and Xuan jing Huang. 2024. Self-polish: Enhance reasoning in large language models via problem refinement. Preprint, arXiv:2305.14497. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2023. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint arXiv:2306.13063. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2024. Can llms express their uncertainty? an empirical eval uation of confidence elicitation in llms. Preprint, arXiv:2306.13063. Ruixin Yang, Dheeraj Rajagopa, Shirley Anugrah Hay ati, Bin Hu, and Dongyeop Kang. 2024. Confidence calibration and rationalization for llms via multi agent deliberation. arXiv preprint arXiv:2404.09127. Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. 2023. Answering questions by meta-reasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007. Mozhi Zhang, Mianqiu Huang, Rundong Shi, Linsen Guo, Chong Peng, Peng Yan, Yaqian Zhou, and Xipeng Qiu. 2024a. Calibrating the confidence of large language models by eliciting fidelity. arXiv preprint arXiv:2404.02655.Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuy ing Peng, Jun Wang, Yueting Zhuang, and Weim ing Lu. 2024b. Self-contrast: Better reflection through inconsistent solving perspectives. Preprint, arXiv:2401.02009. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023. Siren’s song in the ai ocean: a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223. Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023. Why does chatgpt fall short in answering ques tions faithfully? arXiv preprint arXiv:2304.10513. A Complementary Experiments A.1 Token Efficiency Analysis We compared the token costs of our method to several baselines on the CommonsenseQA dataset, as shown in Table 3. Specifically, We computed the number of prompts used per data sample, the average number of tokens per prompt for a sin gle sample, and the total average token usage per sample. The results indicate that our method’s to ken costs are similar to those of other multi-agent methods, such as MAD and Self-Contrast. Similar results are observed on other datasets. Method Prompts/Sample Tokens/Prompt Tokens/Sample CoT 1 103.25 103.25 Self-Reflection 3 255.86 767.59 Self-Consistency 7 103.25 722.73 MAD 10 276.72 2767.18 Self-Contrast 6 349.80 2098.81 CFMAD (Ours) 10 246.82 2468.22
* * *
## _Table 3: Token Costs of different methods._
A.2 Results on Open-Ended Tasks
* * *
In addition to evaluating binary prediction and multi-choice QA tasks, where candidate an swers are provided, we also conducted eval uations on more open-ended tasks, including mathematical reasoning and machine transla tion. Specifically, we selected three mathemat ical reasoning datasets—SV AMP (Patel et al., 2021), GSM8K (Cobbe et al., 2021), and Multi_Arith (Roy and Roth, 2016)—and one ma chine translation dataset, CommonMT (He et al., 2020). These datasets do not offer predefined
* * *
## answer choices and require the language models
(LLMs) to generate open-ended responses. Since these tasks lack candidate answers, we first used CoT prompts to generate candidate answers from the open answer spaces by sampling multiple times. This allowed us to transform the problems into a multi-choice QA format, which could then be processed using our pipeline. For evaluation, we used precision as a metric for the mathematical reasoning tasks and BLEURT1 for the machine translation task. The results, as shown in the Table 4, demonstrate that CFMAD outperforms the other baselines, highlighting the effectiveness and generality of our approach. Method SV AMP GSM8K Multi_Arith CommonMT CoT 0.8067 0.7567 0.9575 0.7008 Self-Consistency 0.8500 0.8167 0.9804 / MAD 0.8567 0.8100 0.9575 0.7183 Self-Contrast 0.8467 0.8133 0.9771 0.7119 CFMAD 0.8700 0.8300 0.9837 0.7201
* * *
## _Table 4: Results on Open-Ended Tasks_
A.3 Results on Open-Sourced LLMs
* * *
In addition to GPT-3.5, we have also assessed our method on two other open-source LLMs: Llama2 7B (Touvron et al., 2023) and Llama3-8B (Dubey et al., 2024). The results on the CommonsenseQA dataset are presented in Table 5. Our approach, CFMAD, consistently outperforms other baselines across these models, highlighting the effectiveness of our method when applied to open-source LLMs. CommonsenseQA Llama2-7B Llama3-8B Self-Consistency 0.6233 0.7533 MAD 0.5900 0.7467 Self-Contrast 0.6233 0.7600 CFMAD 0.6367 0.7700
* * *
## _Table 5: Results on Open-Sourced Tasks_
B Experiments Details
* * *
B.1 Dataset Details We performed experiments using four datasets: Hover, BoolQ, CosmosQA, and CommonsenseQA. The details of these datasets are as follows: •Hover: Hover is a fact-checking task dataset. Each instance in the Hover dataset consists ofa claim and supporting evidence. The task re quires multi-hop reasoning based on the support ing evidence to determine whether the evidence supports the claim or not. •BoolQ: BoolQ is a reading comprehension task dataset that consist of questions that can be answered with a simple “yes” or “no”. And each question is paired with a paragraph from Wikipedia that contains the answer. •CosmosQA: CosmosQA is a dataset focused on reading comprehension and commonsense rea soning. Each instance consists of a context and a question with four answer options that require inference beyond the text, using commonsense knowledge to determine the correct answer. •CommonsenseQA : CommonsenseQA is a chal lenging dataset that tests a model’s ability to use commonsense knowledge to answer multiple choice questions. Each question has one correct answer and four distractors. In this work, we first tested all 3-hop and 4-hop instances in the validation set of Hover, with 1,835 instances for 3-hop and 1,039 instances for 4-hop to demonstrate our method’s effectiveness. Next, due to budget constraints, we randomly selected 300 instances from the validation set of each of the remaining three datasets to conduct our experi ments. B.2 Method Implementation Details For MAD, we initialized 3 agents and conducted 3 rounds of debate. For Self-Contrast, we had the LLM initially generate answers from 3 perspectives for subsequent contrast. For Self-Consistency, we initially generated 7 answers, voting for the final answer. For our CFMAD framework, we initially preset two predetermined answers to instruct the LLMs to generate abduction. For datasets like Cos mosQA and CommonsenseQA, which have mul tiple potential answers, we first use 3 rounds of CoT prompting to obtain one potentially correct answer as a predetermined answer. Then, we ran domly select another predetermined answer from the remaining options. C Basline Prompts C.1 CoT Prompt •Fact Check Task Evidence: {evidence}
* * *
`{Question and option here}`
`{Question Content Here}`
`{Question Content Here}`
`{Question Content Here}`
`{Question Content Here}`
`{Question Content Here}`
`{Question Content Here}`
`{Question Content Here}`
Claim: {claim} You are a fact checker. Please fully understand the evidence and claim, and answer is the claim true or false? Let us verify step by step. •Commonsense Resoning Play the role of a common sense reasoning ex pert. Choose the most appropriate answer for the question. You are expected to explain your reasoning process step-by-step before providing the final answer. Output format: Reasoning steps: [Your precise reasoning steps here] Judgement: The correct answer is Option [X]. C.2 Relfection Prompt •Reflection Prompt As a critic, review the assistant’s response. Iden tify any incorrect or missing information, and provide feedback. Assistant’s reply: {CoT_reply} Output format: Judgement: [Critically evaluate the assistant’s response.] Potential Improvements: [Suggest ways to en hance the accuracy or clarity of the assistant’s response.] •Revision Prompt Assistant’s reply: {CoT_reply} Feeback: {reflection_reply} Based on the feedback provided, revise your re sponse to the question. Output format: The correct answer is Option [X]. C.3 MAD Here we show the prompt for CommonsenseQA. The prompt structure is similar for other tasks, and the specific prompts for other tasks can be found in our code. •Initial Prompt 1 Play the role of a common sense reasoning ex pert. Choose the most appropriate answer for the question. You are expected to explain your reasoning process step-by-step before providing the final answer.•Initial Prompt 2 Which option is the most appropriate answer based on the common sense? •Initial Prompt 3 Let us think step by step and find the most appro priate answer based on the common sense. •Debate Prompt Let us think step by step and find the most appro priate answer based on the common sense. Assistant: {Your previous response} Other agent1: {Other agents’ previous re sponses1} Other agent2: {Other agents’ previous re sponses2} Using the judgements from other agents as ad ditional information, can you give an updated response. •Judge Prompt Let us think step by step and find the most appro priate answer based on the common sense. Agent1: {last response of agent 1} Agent2: {last response of agent 2} Agent3: {last response of agent 3} Three agents have given their answers. According to the majority of the answers, what is the most appropriate answer? Your answer should look like this: “The correct answer is Option [X]” C.4 Self-contrast Here we show the prompt for CommonsenseQA. The prompt structure is similar for other tasks, and the specific prompts for other tasks can be found in our code. •Self-Curate Prompt You are a commonsense reasoning specialist. You need to complete multiple choice questions re lated to commonsense reasoning. Given a ques tion, you need to carefully analyze the question and dynamically generate several useful prompt instructions. These prompt instructions should be diverse and also useful for commonsense rea soning. These prompt instructions are used to guide the language model to think in different ways, attention to different emphases, and rea son from different perspectives for more accurate
* * *
`{Question Content Here}`
`{Question Content Here}`
commonsense reasoning. For instance, you can adopt multi-faceted think ing (logical thinking, lateral thinking, analogical thinking, etc.), different reasoning perspectives( e.g., top-down, bottom-up, step-by-step), and dif ferent emphases of concern, (entity words, num bers, time, etc) for input question in prompt instruction. Here are some guidance rules for Prompt Gener ation: 1. Tone Requirement: Please generate prompt instructions in the third person. 2. Content Requirement: Each prompt instruc tion should adopt a different way of thinking, or focus on a different perspective, or different em phases to solve the question. 3. Number Requirement: Dynamically generate the most valuable 3 prompt instructions based on the input math question. 4. Format Requirement: Each prompt instruction should start with ### and end with @@@ 5. Others: Prompt instructions should focus on commonsense reasoning. So don’t ask any other irrelevant questions in the prompt. Here is an example : The question is: Who is the first president of the United States? Output: bottomup perspective : ### As a specialist in commonsense reasoning, you have to judge the given question from a bottom-up perspective. Breaking the question down into smaller compo nents or details. What specific pieces of infor mation are provided in the question, and how do they contribute to understanding the problem? @@@ The input question is: {question}. Please gener ate the most suitable three prompts: •Contrast Prompt You are a specialist in commonsense reasoning. Given some candidate judgements for a question, you should carefully compare the difference for each two judgements in their reasoning steps. When you compare, you need to consider the fol lowing questions: 1: Are the two judgements have different final judge and judge reasons? 2: Where are the differences in their reason steps and judge reasons? 3. Why are the answers of the two judgements different? After contrasting, you should generate a check-list based on these differences between candidate judgements. You should carefully consider each discrepancy and the reasons behind it, summariz ing them into a few checking instructions in the checklist. This checklist can guide others to re examine the input question and these candidate judgements to eliminate these discrepancies. Judgements: Judgement1: {reply1}, Judgement2: {reply2}, Judgement3: {reply3} Output Format: For Judgement1 and Judgement2 : [Give the dif ference between Judgement1 and Judgement2 here] For Judgement1 and Judgement3 : [Give the difference between Judgement1 and Judgement3 here] For Judgement2 and Judgement3 : [Give the difference between Judgement2 and Judgement3 here] Checklist : [Give the directives for checking here] •Reflection Prompt Given a question, multiple inconsistent judge ments, their differences in their reasoning pro cesses and a checklist. You should revise the inconsistent reasoning step for each judgements, eliminate the differences, and output a new judge ment. Guidance Rules for Reflection: 1. Please check carefully according to the re quirements on the checklist. It helps you to re solve conflicts between different judgements. 2. When you finish revising inconsistent judge ments, please ensure all revised judgements should have the same answer. If not, please revise again until all inconsistencies are removed, and all candidates are consistent. The candidate judgements and their discrepancy are as follows: {
* * *
> “Candidate”: {
> “Judgement”: “{reply1}”,
> “Judgement”: “{reply2}”,
> “Judgement3”: “{reply3}”
},
* * *
> “Discrepancy”: {
> “difference_1_2”: {
> “source”: “Judgement1”,
> “target”: “Judgement2”,
> “relation”: {difference_1_2}
},
* * *
> “difference_1_3”: {
> “source”: “Judgement1”,
> “target”: “Judgement3”,
> “relation”: {difference_1_3}
},
* * *
> “difference_2_3”: {
> “source”: “Judgement2”,
> “target”: “Judgement3”,
> “relation”: {difference_2_3}
`{Debate Process for each stance}`
`{Question Content Here}`
`{Question Content Here}`
`{Question Content Here}`
`{Question Content Here}`
`{Debate Process for each stance}`
} } } Checklist: {checklist} Please revise each inconsistent judgement and give your final judgement. Output Format: The answer is Option [X]. D Our Prompts D.1 Fact Check Task •Abduction Generation Evidence: {evidence} Claim: {claim} Please fully understand the evidence and claim, and answer why the claim is {true/false}? •Counterfactual Debate for Critic Evidence: {evidence} Claim: {claim} Assistant: {reply of assistant} The Assistant’s answer maybe wrong. Please persuade the assistant that the claim is actually incorrect based on the evidence. •Counterfactual Deabte for Assistant Evidence: {evidence} Claim: {claim} Please fully understand the evidence and claim, and answer why the claim is true? Fact checker: {reply of assistant} Critic: {reply of crtic} Play the role of fact checker. Please point out the errors in critic’s answer and reiterate your point. •Judge Evidence: {evidence} Claim: {claim} After hearing the positive and negative sides, do you think the claim is true or false? [True/False]D.2 Commonsense Reasoning •Abduction Generation Try to explain why the question’s answer might be option {predetermined answer}. Output Format: Judgement: The answer is option {predetermined answer}. Reasoning: [Your reasoning here] •Counterfactual Debate for Critic Assistant: {reply of assistant} The Assistant’s answer maybe wrong. Please persuade the assistant that his answer maybe wrong. •Counterfactual Deabte for Assistant Assistant: {reply of assistant} Critic: {reply of critic} As assistant, please refute the critic’s answer and persuade the critic that your answer is correct. •Judge Which option is the answer of the question? The results of the analysis for each of the possible options are as follows: After seeing the debate process above, do you think which option is the most appropriate an swer for the question? Please only give a correct answer and no other replies. Output format: Judgement: The correct answer is Option [X]. Reasoning steps: [Your precise reasoning steps here]
* * *
