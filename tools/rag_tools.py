from pymilvus import MilvusClient, DataType
from typing import List, Dict, Any, Optional
from pymilvus import model
# å‡è®¾è¿™äº›å¸¸é‡å·²åœ¨ä½ é¡¹ç›®ä¸­å®šä¹‰
MILVUS_URI = "http://localhost:19530"
INDEX_TYPE = "IVF_FLAT"
METRIC_TYPE = "L2"
NLIST = 128
import math

MAX_TEXT_LEN = 4000  # Milvus VARCHAR é™åˆ¶ï¼ˆå¯ä¸ ensure_collection ä¿æŒä¸€è‡´ï¼‰
BATCH_SIZE = 100



def ensure_collection(client: MilvusClient, dim, collection_name: str):
    """
    åˆ›å»ºç”¨äº RAG çš„ Collectionï¼Œå¸¦æ–‡ç« æ ‡é¢˜å­—æ®µã€‚
    Schema:
      - id (ä¸»é”®)
      - article_id
      - title
      - text
      - embedding
    """

    if client.has_collection(collection_name):
        print(f"âœ… Collection `{collection_name}` å·²å­˜åœ¨ï¼Œå¤ç”¨ã€‚")
        return

    # å¯ç”¨åŠ¨æ€å­—æ®µï¼Œæ–¹ä¾¿åç»­æ‰©å±•
    schema = client.create_schema(auto_id=False, enable_dynamic_fields=True)

    # å­—æ®µå®šä¹‰
    schema.add_field("id", DataType.INT64, is_primary=True, auto_id=True)
    schema.add_field("article_id", DataType.VARCHAR, max_length=128)
    schema.add_field("title", DataType.VARCHAR, max_length=512)
    schema.add_field("text", DataType.VARCHAR, max_length=4096)
    schema.add_field("embedding", DataType.FLOAT_VECTOR, dim=dim)

    # å‘é‡ç´¢å¼•é…ç½®
    index_params = client.prepare_index_params()
    index_params.add_index(
        field_name="embedding",
        index_type=INDEX_TYPE,
        metric_type=METRIC_TYPE,
        params={"nlist": NLIST} if "IVF" in INDEX_TYPE else {}
    )

    client.create_collection(
        collection_name=collection_name,
        schema=schema,
        index_params=index_params
    )

    print(f"âœ… Collection `{collection_name}` å·²åˆ›å»ºå®Œæˆï¼")




def split_long_text(text: str, max_len: int = 4096) -> list[str]:
    """
    é€’å½’å¯¹åŠåˆ‡åˆ†è¶…é•¿æ–‡æœ¬ï¼Œç¡®ä¿æ¯æ®µé•¿åº¦ <= max_lenã€‚
    ä¼˜å…ˆåœ¨å¥å·æˆ–ç©ºæ ¼å¤„åˆ†å‰²ï¼Œæœ€åå¼ºåˆ¶æˆªæ–­ã€‚
    """
    text = text.strip()

    # âœ… å¦‚æœæ–‡æœ¬å·²ç»è¶³å¤ŸçŸ­ï¼Œåˆ™ç›´æ¥è¿”å›ï¼ˆå¹¶ä¿è¯ä¸è¶…é™ï¼‰
    if len(text) <= max_len:
        return [text]

    mid = len(text) // 2

    # ä¼˜å…ˆæŒ‰å¥å·æˆ–ç©ºæ ¼åˆ‡åˆ†
    split_pos = text.rfind('.', 0, mid)
    if split_pos == -1:
        split_pos = text.rfind(' ', 0, mid)
    if split_pos == -1:
        split_pos = mid  # å®åœ¨æ‰¾ä¸åˆ°å°±ç¡¬åˆ‡

    left = text[:split_pos + 1].strip()
    right = text[split_pos + 1:].strip()

    # âœ… å¦‚æœå·¦/å³ä»ç„¶å¤ªé•¿ï¼Œé€’å½’åˆ‡åˆ†
    left_parts = split_long_text(left, max_len) if len(left) > max_len else [left]
    right_parts = split_long_text(right, max_len) if len(right) > max_len else [right]

    # âœ… æ‹¼æ¥åç»Ÿä¸€ã€Œç¡¬æˆªæ–­ã€ä»¥é˜² strip å¯¼è‡´è¶Šç•Œ
    result = []
    for chunk in (left_parts + right_parts):
        if len(chunk) > max_len:
            # å¯¹è¶…é•¿ chunk ç¡¬æˆªæ–­æˆå¤šä¸ªç­‰é•¿ç‰‡æ®µ
            for i in range(0, len(chunk), max_len):
                result.append(chunk[i:i + max_len])
        else:
            result.append(chunk)
    return result


def insert_chunks(
    client: MilvusClient,
    article_id: str,
    title: str,
    chunks: List[str],
    embedding_fn,
    collection_name: str,
):
    """
    å°†ä¸€ç¯‡æ–‡ç« çš„æ–‡æœ¬ç‰‡æ®µæ‰¹é‡æ’å…¥åˆ° Milvusã€‚
    è‡ªåŠ¨å¤„ç†è¶…é•¿æ–‡æœ¬ã€é€’å½’åˆ‡åˆ†ã€‚
    æ¯æ¡è®°å½•åŒ…å«ï¼š
      - idï¼ˆå¯é€‰ï¼‰: å”¯ä¸€é”® (article_id_index)
      - article_id: æ–‡ç« å”¯ä¸€æ ‡è¯†
      - title: æ–‡ç« æ ‡é¢˜
      - text: å†…å®¹ï¼ˆå¦‚è¶…é•¿è‡ªåŠ¨åˆ‡åˆ†ï¼‰
      - embedding: å‘é‡
    """

    if not chunks:
        print(f"âš ï¸ article_id={article_id} æ²¡æœ‰å¯æ’å…¥å†…å®¹ï¼Œè·³è¿‡ã€‚")
        return

    # âš™ï¸ Step 1: é¢„å¤„ç†æ‰€æœ‰ chunkï¼ˆè‡ªåŠ¨åˆ‡åˆ†è¶…é•¿æ–‡æœ¬ï¼‰
    safe_chunks = []
    for chunk in chunks:
        safe_chunks.extend(split_long_text(chunk, MAX_TEXT_LEN))

    print(f"ğŸ§© æ–‡ç«  {title} åŸ {len(chunks)} æ®µï¼Œç»åˆ‡åˆ†å {len(safe_chunks)} æ®µã€‚")

    # âš™ï¸ Step 2: æ‰¹é‡ç”Ÿæˆå‘é‡ï¼ˆSentenceTransformerEmbeddingFunctionï¼‰
    embeddings = embedding_fn.encode_documents(safe_chunks)

    # âš™ï¸ Step 3: ç»„è£…æ’å…¥è¡Œ
    rows = []
    for i, (text, emb) in enumerate(zip(safe_chunks, embeddings)):
        safe_text = text[:MAX_TEXT_LEN]
        rows.append({
            "article_id": article_id,
            "title": title,
            "text": safe_text,
            "embedding": emb.tolist(),
        })

    # âš™ï¸ Step 4: åˆ†æ‰¹å†™å…¥
    total_batches = math.ceil(len(rows) / BATCH_SIZE)
    for b in range(total_batches):
        batch = rows[b * BATCH_SIZE:(b + 1) * BATCH_SIZE]
        client.insert(collection_name=collection_name, data=batch)
        print(f"   âœ… æ‰¹æ¬¡ {b+1}/{total_batches} å·²æ’å…¥ {len(batch)} æ¡")

    print(f"âœ… æˆåŠŸæ’å…¥ {len(rows)} æ¡è®°å½•åˆ° `{collection_name}` (title={title})")

def delete_paper(client: MilvusClient, article_id: str, collection_name: str):
    """
    åˆ é™¤æŒ‡å®š article_id çš„æ‰€æœ‰æ–‡æ¡£æ•°æ®ã€‚
    å…¼å®¹ pymilvus 2.6+ï¼Œä½¿ç”¨ filter å‚æ•°ã€‚
    """
    filter_expr = f"article_id == '{article_id}'"
    client.delete(
        collection_name=collection_name,
        filter=filter_expr  # âœ… æ–°ç‰ˆå†™æ³•
    )
    print(f"ğŸ—‘ï¸ å·²åˆ é™¤ article_id={article_id} çš„è®°å½•ã€‚")

# -----------------------------
# æœç´¢å‡½æ•°
# -----------------------------
def search(
        query: str,
        embed_fn: Any,
        collection_name: str,
        article_id: Optional[str] = None,
        title: Optional[str] = None,
        topk: int = 5,
        output_fields: Optional[List[str]] = None,
    ) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨ SentenceTransformerEmbeddingFunction å‘é‡åŒ–æŸ¥è¯¢ï¼Œ
    åœ¨ Milvus ä¸­æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µã€‚
    """

    client = MilvusClient(uri=MILVUS_URI)

    # 1ï¸âƒ£ å°† query ç¼–ç ä¸ºå‘é‡
    qv = embed_fn.encode_queries([query])[0]

    # 2ï¸âƒ£ æ„å»ºè¿‡æ»¤è¡¨è¾¾å¼
    expr_parts = []
    if article_id:
        expr_parts.append(f"article_id == '{article_id}'")
    if title:
        expr_parts.append(f"title == '{title}'")
    expr = " and ".join(expr_parts) if expr_parts else None

    # 3ï¸âƒ£ æ£€ç´¢
    raw_res = client.search(
        collection_name=collection_name,
        data=[qv],
        anns_field="embedding",
        limit=topk,
        search_params={"metric_type": "L2", "params": {"nprobe": 10}},
        filter=expr,  # âœ… æ–°ç‰ˆå‚æ•°
        output_fields=output_fields or ["id", "article_id", "title", "text"]
    )

    # 4ï¸âƒ£ æ•´ç†ç»“æœ
    results = []
    for hits in raw_res:
        for hit in hits:
            results.append({
                "id": hit.id,
                "score": hit.score,
                "distance": hit.distance,
                "query": query,
                **hit.entity
            })

    return results


if __name__ == "__main__":
    pass